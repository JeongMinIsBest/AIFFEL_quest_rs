{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464bc300",
   "metadata": {},
   "source": [
    "# 프로젝트: KoChatGPT 업그레이드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431203e",
   "metadata": {},
   "source": [
    "## 서론\n",
    "이 프로젝트의 목표는 기존 KoChatGPT 모델을 기반으로 다양한 개선 전략을 적용하여 자신만의 Custom ChatGPT를 개발하는 것입니다.\n",
    "제시된 전략(데이터셋 정제, 신규 데이터셋 추가, 파운데이션 모델 교체 등) 중 하나 이상을 선택하여 모델을 업그레이드하고 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095ef02",
   "metadata": {},
   "source": [
    "## 학습 목표 및 평가 기준 (Rubric)\n",
    "- **최종 목표:** 아래 전략 중 하나 이상을 사용하여 모델의 정량적/정성적 성능 향상을 달성합니다.\n",
    "\n",
    "- **평가 기준 1**: 모델 성능 향상\n",
    "  1. **기존 데이터셋 정제:** 데이터셋을 추가 정제하고, Beam search, Top-k sampling 등 Generation 기법 실험을 통해 모델 성능을 향상시킨다.\n",
    "  2. **새로운 데이터셋 추가:** 새로운 데이터를 직접 수집/전처리하여 모델 성능을 향상시킨다.\n",
    "  3. **학습 전략/모델 변경:** 더 적절한 학습 전략(SFT, RM, PPO)을 적용하거나 Initial model을 변경하여 모델 성능을 향상시킨다.\n",
    "\n",
    "- **평가 기준 2: 결과 분석**\n",
    "  1. **KoGPT2 vs SFT:** 기존 KoGPT2 모델과 SFT 적용 모델의 결과물을 정량/정성적으로 비교 분석한다.\n",
    "  2. **SFT vs RM:** SFT 모델과 RM(Reward Model)을 적용한 모델의 결과물을 정량/정성적으로 비교 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec94bf1",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 설치 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e0ba3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 (최초 1회만 실행)\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install pandas\n",
    "!pip install dataclasses\n",
    "!pip install accelerate\n",
    "!pip install loralib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6711871",
   "metadata": {},
   "source": [
    "##### 한글 폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c70bb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [Working]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu noble InRelease [256 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Packages [2306 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]m\n",
      "Get:6 http://archive.ubuntu.com/ubuntu noble/multiverse amd64 Packages [331 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu noble/restricted amd64 Packages [117 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu noble/universe amd64 Packages [19.3 MB] \u001b[0m\u001b[33m\n",
      "Get:9 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Packages [34.6 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages [1136 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu noble-security/main amd64 Packages [1439 kB]33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu noble/main amd64 Packages [1808 kB]33m \u001b[0m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Packages [2414 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Packages [38.9 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1922 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [1793 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Packages [35.6 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Packages [48.8 kB]\n",
      "Fetched 33.4 MB in 5s (6379 kB/s)33m                        \u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "62 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... 0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  fonts-nanum\n",
      "0 upgraded, 1 newly installed, 0 to remove and 62 not upgraded.\n",
      "Need to get 10.3 MB of archives.\n",
      "After this operation, 34.1 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu noble/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
      "Fetched 10.3 MB in 2s (4180 kB/s)      \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package fonts-nanum.\n",
      "(Reading database ... 52749 files and directories currently installed.)\n",
      "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking fonts-nanum (20200506-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up fonts-nanum (20200506-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Processing triggers for fontconfig (2.15.0-1.1ubuntu2) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J설정된 폰트: NanumBarunGothic\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install fonts-nanum -y\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.ticker as ticker\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "\n",
    "fontpath = \"/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf\"\n",
    "fontprop = fm.FontProperties(fname=fontpath, size=12)\n",
    "plt.rcParams[\"font.family\"] = fontprop.get_name()\n",
    "\n",
    "print(f\"설정된 폰트: {fontprop.get_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72228f",
   "metadata": {},
   "source": [
    "##### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caa9c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import json\n",
    "from typing import Optional, Dict, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dce765",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8b0c3",
   "metadata": {},
   "source": [
    "## 2. 모델 성능 비교 분석 (평가 기준 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ee57e",
   "metadata": {},
   "source": [
    "### 2.1. 기존 KoGPT2 vs SFT 적용 모델 결과 분석\n",
    "> **[평가 기준 2.1]** 기존 모델의 결과물과 SFT를 적용한 모델의 결과물을 정량/정성적으로 비교/분석했는가?\n",
    "\n",
    "먼저, 튜닝하지 않은 원본 `skt/kogpt2-base-v2` 모델의 답변을 생성하여 베이스라인으로 삼습니다.\n",
    "이후 SFT(Supervised Fine-Tuning)를 진행하고, 같은 프롬프트에 대해 SFT 모델이 어떻게 답변하는지 비교하여 성능 향상을 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3acdf",
   "metadata": {},
   "source": [
    "#### **[SFT 이전] 베이스라인 모델 성능 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7706b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: skt/kogpt2-base-v2\n",
      "Baseline Model and Tokenizer loaded successfully.\n",
      "Successfully loaded 12000 prompts from dataset.\n",
      "--- Prompt (Index: 0) ---\n",
      "\"불고기용 고기 한우에요?\"\n",
      "\n",
      "==================================================\n",
      "\n",
      "### 베이스라인 1: Beam Search (num_beams=5) ###\n",
      "불고기용 고기 한우에요? ᄒᄒ\n",
      "이거 진짜 맛있더라구요 ᄏᄏ\n",
      "고기랑 같이 먹으니까 더 맛있어요!!!\n",
      "이렇게 고기랑 함께 먹을 수 있어서 너무 좋았어요~~\n",
      "고기가 정말 맛있게 잘 익었습니다!\n",
      "고기를 구워먹을 수 있는 고기집이라고 하네요!\n",
      "고기집답게 고기 굽는 과정이 굉장히 간단하답니다  \n",
      "고기 굽기 전에 고기를 굽고 나서 고기를 구워야하는데\n",
      "그런데 고기를 구울 때 고기를 다 굽지 않고 그냥 굽기만 하면 되더\n",
      "\n",
      "==================================================\n",
      "\n",
      "### 베이스라인 2: Moderated Sampling (temperature=0.7, top_k=50) ###\n",
      "불고기용 고기 한우에요?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 기본 설정 ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# --- 2. 베이스라인 모델 및 토크나이저 로드 ---\n",
    "try:\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    base_model.eval()\n",
    "    print(\"Baseline Model and Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "\n",
    "# --- 3. 데이터셋에서 프롬프트 로드 ---\n",
    "prompts = []\n",
    "try:\n",
    "    with open('./KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        prompts = [item['prompt'] for item in dataset]\n",
    "    print(f\"Successfully loaded {len(prompts)} prompts from dataset.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing 'data/kochatgpt_1_SFT.jsonl': {e}\")\n",
    "    prompts.append(\"불고기용 고기 한우에요?\")\n",
    "\n",
    "# --- 4. 텍스트 생성 함수 ---\n",
    "def generate_text(model, tokenizer, prompt_text, generation_params):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            output_sequences = model.generate(input_ids=input_ids, **generation_params)\n",
    "        return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during text generation: {e}\"\n",
    "\n",
    "# --- 5. 다양한 디코딩 전략으로 베이스라인 성능 테스트 ---\n",
    "prompt_index = 0\n",
    "prompt = prompts[prompt_index] if prompts and 0 <= prompt_index < len(prompts) else \"불고기용 고기 한우에요?\"\n",
    "print(f\"--- Prompt (Index: {prompt_index}) ---\")\n",
    "print(f'\"{prompt}\"')\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 추천 베이스라인 1: Beam Search (품질/일관성 측정용) ---\n",
    "print(\"### 베이스라인 1: Beam Search (num_beams=5) ###\")\n",
    "params_beam = {\"max_length\": 128, \"num_beams\": 5, \"no_repeat_ngram_size\": 2, \"do_sample\": False, \"early_stopping\": True}\n",
    "output = generate_text(base_model, base_tokenizer, prompt, params_beam)\n",
    "print(output)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 추천 베이스라인 2: Moderated Sampling (자연스러움/창의성 측정용) ---\n",
    "print(\"### 베이스라인 2: Moderated Sampling (temperature=0.7, top_k=50) ###\")\n",
    "params_sampling = {\"max_length\": 128, \"do_sample\": True, \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95}\n",
    "output = generate_text(base_model, base_tokenizer, prompt, params_sampling)\n",
    "print(output)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55bb40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2de07",
   "metadata": {},
   "source": [
    "#### **[SFT] Supervised Fine-Tuning 진행**\n",
    "\n",
    "이제 본격적으로 SFT를 진행합니다. SFT는 미리 준비된 (질문, 답변) 쌍 데이터셋을 이용해 모델을 지도 학습시키는 과정입니다.\n",
    "이를 통해 모델이 사용자의 질문(Instruction) 의도에 맞는 답변(Response)을 생성하도록 길들일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebdf9f",
   "metadata": {},
   "source": [
    "##### **SFT 1단계: 모델 및 토크나이저 재설정**\n",
    "SFT를 위해 원본 `skt/kogpt2-base-v2` 모델을 다시 로드합니다.\n",
    "토크나이저에는 문장의 시작(`bos`), 끝(`eos`), 패딩(`pad`) 등을 명확히 알려주기 위해 special token들을 설정합니다. 이는 모델이 문장의 구조를 더 잘 학습하게 도와줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cd72f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t215: AddedToken(\"ㅜㅜ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t216: AddedToken(\"ㅠㅠ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t217: AddedToken(\"ㅡㅡ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t218: AddedToken(\"😠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t219: AddedToken(\"👿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t220: AddedToken(\"😧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t221: AddedToken(\"😰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t222: AddedToken(\"😲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t223: AddedToken(\"😁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t224: AddedToken(\"🐻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t225: AddedToken(\"🐱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t226: AddedToken(\"😹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t227: AddedToken(\"😼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t228: AddedToken(\"🤡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t229: AddedToken(\"🥶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t230: AddedToken(\"😖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t231: AddedToken(\"😕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t232: AddedToken(\"🐮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t233: AddedToken(\"🤠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t234: AddedToken(\"😿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t235: AddedToken(\"😢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t236: AddedToken(\"😞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t237: AddedToken(\"😵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t238: AddedToken(\"🐶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t239: AddedToken(\"😓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t240: AddedToken(\"🐲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t241: AddedToken(\"🤤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t242: AddedToken(\"😑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t243: AddedToken(\"😘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t244: AddedToken(\"😋\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t245: AddedToken(\"😱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t246: AddedToken(\"🤮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t247: AddedToken(\"🤭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t248: AddedToken(\"🤕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t249: AddedToken(\"😷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250: AddedToken(\"🧐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t251: AddedToken(\"😮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t252: AddedToken(\"🤨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t253: AddedToken(\"🙄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t254: AddedToken(\"😤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t255: AddedToken(\"🤬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256: AddedToken(\"😂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t257: AddedToken(\"🤒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t258: AddedToken(\"😛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t259: AddedToken(\"😶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"😨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"🌛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"😳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"🦊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"🐸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"☹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"☹️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"😦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"🌝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"😬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"😺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"😸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"😀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"😃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"😄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"😅\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"😆\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"🐹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"🐴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"🥵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"🤗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"😯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"😽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"😗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"😚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"😙\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"🌜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"🦁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"😭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"🤥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"🤦🏿‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"🤦🏻‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"🤦🏾‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"🤦🏼‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"🤦🏽‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"🤦‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"🤦🏿‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"🤦🏻‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"🤦🏾‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"🤦🏼‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"🤦🏽‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"🤦‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"🤑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"🐵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"🐭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"🤢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"🤓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"😐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"🌚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"🐼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"🥳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"😔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"😣\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"🤦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"🤦🏿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"🤦🏻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"🤦🏾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"🤦🏼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"🤦🏽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"🐷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"🥺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"😾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"😡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"🐰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"😌\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"🤖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"😥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"🤫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"😴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"😪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"🙁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"🙂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"😻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"☺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"☺️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"🥰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"😇\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"😍\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"😈\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"😊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"😎\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"😏\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"🤧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"😝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"🌞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"🤔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"🐯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"😫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"😒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"🦄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"🙃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"🙀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"😩\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"🌬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"🌬️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"😉\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"😜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"🐺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"🤦🏿‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"🤦🏻‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"🤦🏾‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"🤦🏼‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t362: AddedToken(\"🤦🏽‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t363: AddedToken(\"🤦‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t364: AddedToken(\"🤦🏿‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t365: AddedToken(\"🤦🏻‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t366: AddedToken(\"🤦🏾‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t367: AddedToken(\"🤦🏼‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t368: AddedToken(\"🤦🏽‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t369: AddedToken(\"🤦‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t370: AddedToken(\"🥴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t371: AddedToken(\"😟\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t372: AddedToken(\"🥱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t373: AddedToken(\"🤪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t374: AddedToken(\"🤐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# SFT를 위한 모델과 토크나이저를 새로 로드합니다.\n",
    "sft_model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(sft_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329b9a9",
   "metadata": {},
   "source": [
    "##### **SFT 2단계: 데이터셋 클래스 정의**\n",
    "`kochatgpt_1_SFT.jsonl` 데이터를 불러와 SFT 학습에 적합한 형태로 가공하는 `Dataset` 클래스를 정의합니다.\n",
    "\n",
    "- **Instruction 형식 변환**: \"질문:{prompt} 답변:{completion}\" 과 같은 형태로 데이터를 변환하여 모델이 질문과 답변의 관계를 학습하도록 합니다.\n",
    "- **Label 마스킹**: 모델이 답변 부분만 학습하도록, 질문(prompt) 부분의 레이블을 `-100`으로 마스킹 처리합니다. 이는 SFT의 핵심적인 부분입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d127ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFT_dataset(Dataset):\n",
    "    \"\"\"SFT 학습을 위한 데이터셋 클래스\"\"\"\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        # 데이터 파일 로드\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        # 프롬프트 형식 정의\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        # 소스(질문)와 타겟(답변) 분리 및 형식 적용\n",
    "        sources = [prompt_input.format_map(example) for example in list_data_dict]\n",
    "        targets = [f\"{example['completion']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        # 소스와 타겟을 합쳐 전체 학습 샘플 생성\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # 소스와 전체 샘플 토크나이징\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)\n",
    "\n",
    "        # 레이블 생성 및 마스킹\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100  # 질문 부분은 loss 계산에서 제외\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "        logging.warning(\"Loading data done!!: %d\" % (len(self.labels)))\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"문자열 시퀀스를 토크나이징하는 내부 함수\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(text, return_tensors=\"pt\", padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True)\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = [tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n",
    "        return dict(input_ids=input_ids, input_ids_lens=input_ids_lens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93fe53",
   "metadata": {},
   "source": [
    "##### **SFT 3단계: 데이터 콜레이터 정의**\n",
    "`Dataset`에서 가져온 개별 샘플들을 하나의 배치(batch)로 묶는 역할을 하는 `DataCollator` 클래스를 정의합니다. 이 과정에서 배치 내 샘플들의 길이를 맞추기 위해 패딩(padding)을 적용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d06a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"데이터 샘플들을 배치로 만들기 위한 데이터 콜레이터\"\"\"\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "\n",
    "        # 입력과 레이블을 패딩 토큰으로 패딩\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab193ccc",
   "metadata": {},
   "source": [
    "##### **SFT 4단계: 데이터셋 및 트레이너 준비**\n",
    "앞서 정의한 클래스들을 이용해 실제 데이터셋과 데이터 콜레이터 객체를 만듭니다. 그리고 `TrainingArguments`로 학습 설정을 정의한 후, `Trainer` 객체를 생성하여 모든 것을 하나로 묶습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40a45d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 및 데이터 콜레이터 인스턴스 생성\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='./KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=sft_tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=sft_tokenizer)\n",
    "\n",
    "# 데이터셋 샘플 확인 (input_ids와 masking 처리된 labels)\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])\n",
    "\n",
    "# 트레이닝 파라미터 설정\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"sft_model_test\",        # 모델과 체크포인트가 저장될 디렉토리\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,                 # 전체 데이터셋에 대한 학습 횟수\n",
    "    per_device_train_batch_size=8,      # 장치(GPU)당 배치 크기\n",
    "    warmup_steps=5,                     # 학습률 스케줄러를 위한 웜업 스텝 수\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True                           # 혼합 정밀도 학습(Mixed Precision Training) 활성화\n",
    ")\n",
    "\n",
    "# 트레이너 객체 생성\n",
    "trainer = transformers.Trainer(\n",
    "    model=sft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1227a88",
   "metadata": {},
   "source": [
    "##### **SFT 5단계: 모델 학습 및 저장**\n",
    "`trainer.train()`을 호출하여 SFT를 시작합니다. 학습이 완료되면, `save_pretrained`를 사용해 튜닝된 모델의 가중치를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08180668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.781800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.685000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SFT 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# 학습된 모델 저장\n",
    "sft_model.save_pretrained('./here_models/sft_output_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a076b",
   "metadata": {},
   "source": [
    "##### **SFT 6단계: SFT 모델 성능 평가**\n",
    "학습이 완료된 SFT 모델을 로드하여 베이스라인과 동일한 프롬프트로 답변을 생성해봅니다. 베이스라인 모델의 결과와 비교하여 모델이 얼마나 개선되었는지 정성적으로 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b0aa082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT Model Generation Results ---\n",
      "Prompt: 불고기용 고기 한우에요?\n",
      "Generated: ### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다, 저는 인공지능 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 일반적으로 불고기용 고기는 건강에 좋아서 많은 사람들이 즐겨 먹는 음식 중 하나입니다. 그러나 일부 식당에서는 불고기용 고기를 판매하지 않는 경우도 있으니 해당 식당의 공식 홈페이지나 전화로 문의해보\n",
      "------------------------------\n",
      "Prompt: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "Generated: ### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 47대 부통령직을 수행했습니다.賢子, Johnson, Michaelson 등이 그 주인공입니다.賢子와 Johnson은 리처드 닉슨이 41대 부통령직을 맡은 년도를 알지 못합니다.賢子는 리처드 닉슨이 46대 부통령직을 맡았던 년도를\n",
      "------------------------------\n",
      "Prompt: 시카고 오헤어 국제공항은 어디에 있어?\n",
      "Generated: ### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 시카고에 대한 정보를 가지고 있지 않습니다. 하지만 시카고는 미국 캘리포니아주 로스앤젤레스에 위치한 도시입니다. 시카고는 미국에서 가장 유명한 도시 중 하나이며, 많은 사람들이 방문하고 있는 곳 중 하나입니다. 따라서 시카고는 미국의 대표적인 도시 중 하나입니다.ч\n",
      "------------------------------\n",
      "Prompt: 오늘 미세먼지 어때?\n",
      "Generated: ### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 미세먼지에 대한 정보를 알 수 없습니다. 하지만, 미세먼지가 심한 날에는 마스크를 착용하거나 손세정제를 사용하는 것이 좋습니다. 또한, 미세먼지 농도가 높은 날에는 대중교통을 이용하는 것도 도움이 될 수 있습니다. 따라서 미세먼지를 줄이기 위해 대중\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 학습된 SFT 모델과 토크나이저로 텍스트 생성 파이프라인 생성\n",
    "sft_generator = transformers.pipeline('text-generation', model='./here_models/sft_output_model', tokenizer=sft_tokenizer, device=0)\n",
    "\n",
    "# SFT 모델 테스트를 위한 생성 파라미터\n",
    "generation_args = dict(\n",
    "    num_beams=5,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=False,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 베이스라인 테스트와 동일한 프롬프트 리스트\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "            '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "            '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "            '오늘 미세먼지 어때?']\n",
    "\n",
    "# SFT 모델에 맞는 프롬프트 형식으로 변환\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "list_formatted_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "# SFT 모델로 결과 생성 및 출력\n",
    "print(\"--- SFT Model Generation Results ---\")\n",
    "list_result = sft_generator(list_formatted_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {result[0]['generated_text']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# GPU 메모리 정리\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b00b31",
   "metadata": {},
   "source": [
    "### 2.2. SFT 모델 vs RM 적용 모델 결과 분석\n",
    "> **[평가 기준 2.2]** SFT를 적용한 모델의 결과물과 RM을 적용한 모델의 결과물을 정량/정성적으로 비교/분석했는가?\n",
    "\n",
    "여기서는 좋은 답변에 높은 점수(보상, Reward)를, 나쁜 답변에 낮은 점수를 주도록 학습된 보상 모델(Reward Model, RM)을 만듭니다. 이 보상 모델은 이후 3단계 PPO 학습에서 생성 모델(Actor)을 올바른 방향으로 강화하는 \"평가자\" 역할을 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf2133",
   "metadata": {},
   "source": [
    "##### **RM 1단계: 필요 라이브러리 임포트 및 모델/토크나이저 준비**\n",
    "RM 학습을 위해 필요한 클래스들을 `chatgpt` 폴더에서 임포트합니다.\n",
    "SFT 단계와 마찬가지로, `skt/kogpt2-base-v2`를 기반으로 보상 모델을 만들고, special token이 추가된 토크나이저를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4eb13e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM 학습에 필요한 클래스 임포트\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from chatgpt.trainer.rm import RewardModelTrainer\n",
    "\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# SFT와 동일한 토크나이저 설정 사용\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90d25f",
   "metadata": {},
   "source": [
    "##### **RM 2단계: Custom Reward Model 클래스 정의**\n",
    "Hugging Face의 `GPT2Model`을 기반으로, 문장의 좋고 나쁨을 판단하여 점수(reward)를 출력하는 Custom Reward Model 클래스(`GPTRM_custom`)를 정의합니다.\n",
    "\n",
    "- `GPT2Model` 위에 `value_head`라는 이름의 Linear 레이어를 추가하여, 모델의 최종 출력이 단일 값(점수)이 되도록 설계합니다.\n",
    "- SFT에서 확장된 토크나이저의 어휘 크기에 맞게 모델의 임베딩 크기를 조정(`resize_token_embeddings`)합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2e00313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT-2를 기반으로 하는 Custom Reward Model.\n",
    "    입력된 텍스트의 좋고 나쁨을 판단하여 단일 점수(reward)를 출력합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "\n",
    "        if pretrained is not None:\n",
    "            # 사전 학습된 GPT2 모델을 불러옵니다.\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            # Special token이 추가된 토크나이저에 맞게 임베딩 크기를 조정합니다.\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        # 모델의 마지막 hidden state를 입력으로 받아 단일 점수를 출력하는 value_head를 정의합니다.\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "\n",
    "        # 부모 클래스인 RewardModel을 초기화합니다.\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)\n",
    "\n",
    "# NaiveStrategy 컨텍스트 내에서 RM 모델을 초기화합니다.\n",
    "with NaiveStrategy().model_init_context():\n",
    "    rm_model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=rm_tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54904010",
   "metadata": {},
   "source": [
    "##### **RM 3단계: 데이터 전처리 (Pairwise)**\n",
    "RM 학습을 위해서는 \"더 좋은 답변(chosen)\"과 \"더 나쁜 답변(rejected)\"을 한 쌍으로 묶은 데이터가 필요합니다. `kochatgpt_2_RM.jsonl` 파일에는 1개의 프롬프트와 3개의 순위가 매겨진 답변이 들어있습니다.\n",
    "\n",
    "이것을 `(prompt, chosen, rejected)` 형태의 여러 쌍으로 분해하여 데이터셋을 재구성합니다. 예를 들어, (답변0, 답변1, 답변2)가 있고 순위가 (1, 2, 0)이라면, (답변2 > 답변1), (답변2 > 답변0), (답변1 > 답변0) 이므로 3개의 쌍을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24c5dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "# RM 데이터셋 로드\n",
    "with open('./KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "# 랭킹 정보를 바탕으로 (chosen, rejected) 쌍 만들기\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # 3개의 답변 중 2개를 뽑는 모든 조합(3가지)에 대해 쌍을 생성\n",
    "    # completion_0 vs completion_1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # completion_0 vs completion_2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # completion_1 vs completion_2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b1072",
   "metadata": {},
   "source": [
    "##### **RM 4단계: 데이터셋 분리 및 생성**\n",
    "전처리된 Pairwise 데이터를 학습용(train)과 검증용(eval)으로 나눈 뒤, `RewardDataset` 클래스를 이용해 토크나이징 및 최종 데이터셋 객체를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7573b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1000\n",
      "Eval data size: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83b165ad3984285b2adf473af13be3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80717777579e49119c1c962b1122dc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 셔플 및 분할\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "\n",
    "train_data = total_data_ranking2chosen[:1000]\n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Eval data size: {len(eval_data)}\")\n",
    "\n",
    "# RewardDataset 객체 생성\n",
    "train_dataset = RewardDataset(train_data, rm_tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, rm_tokenizer, 512)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2978254",
   "metadata": {},
   "source": [
    "**RM 5단계: 보상 모델 학습**\n",
    "`RewardModelTrainer`를 사용하여 보상 모델 학습을 시작합니다. 이 트레이너는 `chosen` 답변의 점수는 높이고 `rejected` 답변의 점수는 낮추는 방향으로 모델을 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83ee20eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<02:47,  1.49it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<02:47,  1.49it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:00,  1.38it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:00,  1.38it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:17,  1.25it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:17,  1.25it/s, loss=0.257]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:25,  1.20it/s, loss=0.257]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:25,  1.20it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:29,  1.17it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:29,  1.17it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:32,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:32,  1.15it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:05<03:33,  1.14it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:33,  1.14it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:33,  1.13it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:33,  1.13it/s, loss=2.02] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:34,  1.12it/s, loss=2.02]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:34,  1.12it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:34,  1.12it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:34,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:34,  1.11it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:34,  1.11it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:33,  1.11it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:33,  1.11it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:33,  1.11it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:33,  1.11it/s, loss=0.106]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:33,  1.11it/s, loss=0.106]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:33,  1.11it/s, loss=1.32] \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:33,  1.10it/s, loss=1.32]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:33,  1.10it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:32,  1.10it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:32,  1.10it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:32,  1.10it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:32,  1.10it/s, loss=0.807]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:31,  1.10it/s, loss=0.807]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:16<03:31,  1.10it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:30,  1.10it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:17<03:30,  1.10it/s, loss=0.712]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:30,  1.09it/s, loss=0.712]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:18<03:30,  1.09it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:29,  1.09it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:19<03:29,  1.09it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:28,  1.09it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:20<03:28,  1.09it/s, loss=0.803]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:28,  1.09it/s, loss=0.803]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:28,  1.09it/s, loss=0.894]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:27,  1.09it/s, loss=0.894]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:27,  1.09it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:27,  1.08it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:27,  1.08it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:27,  1.08it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:27,  1.08it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:27,  1.08it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:27,  1.08it/s, loss=0.708]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:26,  1.08it/s, loss=0.708]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:26,  1.08it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:25,  1.07it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:25,  1.07it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:24,  1.07it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:27<03:24,  1.07it/s, loss=0.6]  \u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:24,  1.07it/s, loss=0.6]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:28<03:24,  1.07it/s, loss=0.505]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:23,  1.07it/s, loss=0.505]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:29<03:23,  1.07it/s, loss=0.837]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:22,  1.07it/s, loss=0.837]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:30<03:22,  1.07it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:30<03:21,  1.07it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:31<03:21,  1.07it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:31<03:20,  1.07it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:32<03:20,  1.07it/s, loss=0.794]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:32<03:19,  1.07it/s, loss=0.794]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:33<03:19,  1.07it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:18,  1.07it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:18,  1.07it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:17,  1.07it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:17,  1.07it/s, loss=0.711]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:16,  1.07it/s, loss=0.711]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:16,  1.07it/s, loss=0.856]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:36<03:14,  1.08it/s, loss=0.856]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:36<03:14,  1.08it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:37<03:12,  1.08it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:37<03:12,  1.08it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:38<03:11,  1.09it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:38<03:11,  1.09it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:39<03:10,  1.09it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:39<03:10,  1.09it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:39<03:09,  1.09it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:40<03:09,  1.09it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:40<03:07,  1.09it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:41<03:07,  1.09it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:41<03:06,  1.09it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:42<03:06,  1.09it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:42<03:05,  1.10it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:43<03:05,  1.10it/s, loss=0.7] \u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:43<03:03,  1.10it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:44<03:03,  1.10it/s, loss=0.686]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:44<03:02,  1.10it/s, loss=0.686]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:44<03:02,  1.10it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:45<03:01,  1.10it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:45<03:01,  1.10it/s, loss=0.755]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:46<03:00,  1.10it/s, loss=0.755]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:46<03:00,  1.10it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:47<02:58,  1.11it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:47<02:58,  1.11it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:48<02:57,  1.11it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:48<02:57,  1.11it/s, loss=0.463]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:48<02:56,  1.11it/s, loss=0.463]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:49<02:56,  1.11it/s, loss=0.308]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:49<02:55,  1.11it/s, loss=0.308]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:50<02:55,  1.11it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:50<02:54,  1.11it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:51<02:54,  1.11it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:51<02:53,  1.11it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:52<02:53,  1.11it/s, loss=0.499]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:52<02:52,  1.11it/s, loss=0.499]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:53<02:52,  1.11it/s, loss=0.51] \u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:53<02:51,  1.12it/s, loss=0.51]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:53<02:51,  1.12it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:54<02:49,  1.12it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:54<02:49,  1.12it/s, loss=0.289]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:55<02:48,  1.12it/s, loss=0.289]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:55<02:48,  1.12it/s, loss=1.23] \u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:56<02:47,  1.12it/s, loss=1.23]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:56<02:47,  1.12it/s, loss=0.164]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:57<02:47,  1.12it/s, loss=0.164]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:57<02:47,  1.12it/s, loss=0.913]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.913]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:58<02:46,  1.12it/s, loss=0.355]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.355]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:59<02:45,  1.12it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [01:00<02:44,  1.12it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:00<02:43,  1.12it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:01<02:43,  1.12it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:01<02:42,  1.12it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:01<02:42,  1.12it/s, loss=0.321]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:02<02:41,  1.12it/s, loss=0.321]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:02<02:41,  1.12it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:03<02:40,  1.12it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:03<02:40,  1.12it/s, loss=0.82] \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:04<02:39,  1.12it/s, loss=0.82]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:04<02:39,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:05<02:38,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:05<02:38,  1.12it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:05<02:37,  1.12it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:06<02:37,  1.12it/s, loss=0.836]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:06<02:36,  1.13it/s, loss=0.836]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:07<02:36,  1.13it/s, loss=0.57] \u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:07<02:35,  1.13it/s, loss=0.57]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:08<02:35,  1.13it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:08<02:34,  1.13it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:09<02:34,  1.13it/s, loss=0.736]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:09<02:33,  1.13it/s, loss=0.736]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:09<02:33,  1.13it/s, loss=0.472]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:10<02:32,  1.12it/s, loss=0.472]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:10<02:32,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:11<02:31,  1.13it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:11<02:31,  1.13it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:12<02:30,  1.13it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:12<02:30,  1.13it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:13<02:29,  1.13it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:13<02:29,  1.13it/s, loss=0.42] \u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:13<02:29,  1.13it/s, loss=0.42]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:14<02:29,  1.13it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:14<02:28,  1.13it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:15<02:28,  1.13it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:15<02:27,  1.13it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:16<02:27,  1.13it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:16<02:26,  1.13it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:17<02:26,  1.13it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:17<02:25,  1.13it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:17<02:25,  1.13it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:18<02:24,  1.13it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:18<02:24,  1.13it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:19<02:23,  1.13it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:19<02:23,  1.13it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:20<02:22,  1.13it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:20<02:22,  1.13it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:21<02:21,  1.13it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:21<02:21,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:21<02:20,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:22<02:20,  1.13it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:22<02:20,  1.13it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:23<02:20,  1.13it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:23<02:19,  1.13it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:24<02:19,  1.13it/s, loss=0.819]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:24<02:18,  1.13it/s, loss=0.819]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:25<02:18,  1.13it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:25<02:17,  1.13it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:25<02:17,  1.13it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:26<02:16,  1.13it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:26<02:16,  1.13it/s, loss=0.743]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:27<02:15,  1.13it/s, loss=0.743]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:27<02:15,  1.13it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:28<02:15,  1.12it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:28<02:15,  1.12it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:29<02:14,  1.12it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:29<02:14,  1.12it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:29<02:13,  1.12it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:30<02:13,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:30<02:12,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:31<02:12,  1.12it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:31<02:11,  1.12it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:32<02:11,  1.12it/s, loss=0.69] \u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:32<02:11,  1.12it/s, loss=0.69]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:33<02:11,  1.12it/s, loss=0.7] \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:33<02:10,  1.12it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:33<02:10,  1.12it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:34<02:09,  1.12it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:34<02:09,  1.12it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:35<02:08,  1.12it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:35<02:08,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:36<02:07,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:36<02:07,  1.12it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:37<02:06,  1.12it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:37<02:06,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:37<02:05,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:38<02:05,  1.12it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:38<02:04,  1.12it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:39<02:04,  1.12it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:39<02:04,  1.12it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:40<02:04,  1.12it/s, loss=0.628]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:40<02:03,  1.12it/s, loss=0.628]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:41<02:03,  1.12it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:41<02:02,  1.12it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:41<02:02,  1.12it/s, loss=0.686]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:42<02:01,  1.12it/s, loss=0.686]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:42<02:01,  1.12it/s, loss=0.82] \u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:43<02:01,  1.12it/s, loss=0.82]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:43<02:01,  1.12it/s, loss=0.69]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:44<02:00,  1.11it/s, loss=0.69]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:44<02:00,  1.11it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:45<01:59,  1.11it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:45<01:59,  1.11it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:46<01:58,  1.11it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:46<01:58,  1.11it/s, loss=0.554]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:46<01:57,  1.11it/s, loss=0.554]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:47<01:57,  1.11it/s, loss=0.505]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:47<01:56,  1.11it/s, loss=0.505]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:48<01:56,  1.11it/s, loss=1.23] \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:48<01:56,  1.11it/s, loss=1.23]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:49<01:56,  1.11it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:49<01:55,  1.11it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:50<01:55,  1.11it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:50<01:54,  1.11it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:50<01:54,  1.11it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:51<01:53,  1.11it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:51<01:53,  1.11it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:52<01:52,  1.11it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:52<01:52,  1.11it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:53<01:51,  1.11it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:53<01:51,  1.11it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:54<01:50,  1.11it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:54<01:50,  1.11it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:55<01:49,  1.11it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:55<01:49,  1.11it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:55<01:49,  1.11it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:56<01:49,  1.11it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:56<01:48,  1.11it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:57<01:48,  1.11it/s, loss=0.586]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:57<01:47,  1.11it/s, loss=0.586]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:58<01:47,  1.11it/s, loss=0.921]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:58<01:46,  1.11it/s, loss=0.921]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:59<01:46,  1.11it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:59<01:45,  1.11it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:59<01:45,  1.11it/s, loss=0.804]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:00<01:44,  1.11it/s, loss=0.804]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:00<01:44,  1.11it/s, loss=0.71] \u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:01<01:43,  1.11it/s, loss=0.71]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:01<01:43,  1.11it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:02<01:43,  1.11it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:02<01:43,  1.11it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:03<01:42,  1.11it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:03<01:42,  1.11it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:04<01:41,  1.11it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:04<01:41,  1.11it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:04<01:40,  1.11it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:05<01:40,  1.11it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:05<01:39,  1.11it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:06<01:39,  1.11it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:06<01:38,  1.11it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:07<01:38,  1.11it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:07<01:37,  1.11it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:08<01:37,  1.11it/s, loss=0.841]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:08<01:36,  1.11it/s, loss=0.841]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:09<01:36,  1.11it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:09<01:35,  1.11it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:09<01:35,  1.11it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:10<01:34,  1.11it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:10<01:34,  1.11it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:11<01:33,  1.11it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:11<01:33,  1.11it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:12<01:32,  1.11it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:12<01:32,  1.11it/s, loss=0.393]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:13<01:31,  1.11it/s, loss=0.393]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:13<01:31,  1.11it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:13<01:31,  1.11it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:14<01:31,  1.11it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:14<01:30,  1.11it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:15<01:30,  1.11it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:15<01:29,  1.11it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:16<01:29,  1.11it/s, loss=0.249]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:16<01:28,  1.11it/s, loss=0.249]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:17<01:28,  1.11it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:17<01:27,  1.11it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:18<01:27,  1.11it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:18<01:26,  1.11it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:18<01:26,  1.11it/s, loss=1.08] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:19<01:25,  1.11it/s, loss=1.08]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:19<01:25,  1.11it/s, loss=0.868]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:20<01:24,  1.11it/s, loss=0.868]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:20<01:24,  1.11it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:21<01:23,  1.11it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:21<01:23,  1.11it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:22<01:22,  1.11it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:22<01:22,  1.11it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:22<01:21,  1.11it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:23<01:21,  1.11it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:23<01:20,  1.11it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:24<01:20,  1.11it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:24<01:19,  1.11it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:25<01:19,  1.11it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:25<01:19,  1.11it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:26<01:19,  1.11it/s, loss=0.52] \u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:26<01:18,  1.11it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:27<01:18,  1.11it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:27<01:17,  1.11it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:27<01:17,  1.11it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:28<01:16,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:28<01:16,  1.12it/s, loss=1.08] \u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:29<01:15,  1.12it/s, loss=1.08]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:29<01:15,  1.12it/s, loss=0.314]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:30<01:14,  1.11it/s, loss=0.314]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:30<01:14,  1.11it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:31<01:13,  1.11it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:31<01:13,  1.11it/s, loss=0.479]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:31<01:12,  1.11it/s, loss=0.479]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:32<01:12,  1.11it/s, loss=0.4]  \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:32<01:11,  1.11it/s, loss=0.4]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:33<01:11,  1.11it/s, loss=0.911]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:33<01:10,  1.11it/s, loss=0.911]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:34<01:10,  1.11it/s, loss=0.74] \u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:34<01:09,  1.12it/s, loss=0.74]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:35<01:09,  1.12it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:35<01:08,  1.12it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:35<01:08,  1.12it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:36<01:08,  1.11it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:36<01:08,  1.11it/s, loss=0.389]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:37<01:07,  1.12it/s, loss=0.389]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:37<01:07,  1.12it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:38<01:06,  1.12it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:38<01:06,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:39<01:05,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:39<01:05,  1.12it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:40<01:04,  1.12it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:40<01:04,  1.12it/s, loss=0.331]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:40<01:03,  1.11it/s, loss=0.331]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:41<01:03,  1.11it/s, loss=0.53] \u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:41<01:02,  1.12it/s, loss=0.53]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:42<01:02,  1.12it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:42<01:01,  1.12it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:43<01:01,  1.12it/s, loss=0.277]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:43<01:00,  1.12it/s, loss=0.277]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:44<01:00,  1.12it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:44<01:00,  1.12it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:44<01:00,  1.12it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:45<00:59,  1.12it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:45<00:59,  1.12it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:46<00:58,  1.12it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:46<00:58,  1.12it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:47<00:57,  1.12it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:47<00:57,  1.12it/s, loss=0.382]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:48<00:56,  1.12it/s, loss=0.382]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:48<00:56,  1.12it/s, loss=0.315]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.315]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:49<00:55,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:49<00:54,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:50<00:54,  1.12it/s, loss=0.85] \u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:50<00:53,  1.12it/s, loss=0.85]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:51<00:53,  1.12it/s, loss=0.34]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:51<00:52,  1.12it/s, loss=0.34]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:52<00:52,  1.12it/s, loss=0.256]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:52<00:51,  1.12it/s, loss=0.256]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:52<00:51,  1.12it/s, loss=0.478]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:53<00:50,  1.12it/s, loss=0.478]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:53<00:50,  1.12it/s, loss=0.877]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:54<00:50,  1.12it/s, loss=0.877]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:54<00:50,  1.12it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:55<00:49,  1.12it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:55<00:49,  1.12it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:56<00:48,  1.12it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:56<00:48,  1.12it/s, loss=1.09] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:57<00:47,  1.12it/s, loss=1.09]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:57<00:47,  1.12it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:57<00:46,  1.12it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:58<00:46,  1.12it/s, loss=0.542]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:58<00:45,  1.12it/s, loss=0.542]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:59<00:45,  1.12it/s, loss=0.421]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:59<00:44,  1.12it/s, loss=0.421]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [03:00<00:44,  1.12it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:00<00:43,  1.12it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:01<00:43,  1.12it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:01<00:42,  1.12it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:01<00:42,  1.12it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:02<00:41,  1.12it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:02<00:41,  1.12it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:03<00:41,  1.12it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:03<00:41,  1.12it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:04<00:40,  1.12it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:04<00:40,  1.12it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:05<00:39,  1.12it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:05<00:39,  1.12it/s, loss=0.763]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.763]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:06<00:38,  1.12it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:06<00:37,  1.12it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:07<00:37,  1.12it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:07<00:36,  1.12it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:08<00:36,  1.12it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:08<00:35,  1.12it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:09<00:35,  1.12it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:09<00:34,  1.12it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:09<00:34,  1.12it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:10<00:33,  1.12it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:10<00:33,  1.12it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:11<00:33,  1.12it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:11<00:33,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:12<00:32,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:12<00:32,  1.12it/s, loss=1.1]  \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:13<00:31,  1.12it/s, loss=1.1]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:13<00:31,  1.12it/s, loss=0.56]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.56]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:14<00:30,  1.12it/s, loss=0.793]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:14<00:29,  1.12it/s, loss=0.793]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:15<00:29,  1.12it/s, loss=0.85] \u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:15<00:28,  1.12it/s, loss=0.85]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:16<00:28,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:16<00:27,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:17<00:27,  1.12it/s, loss=0.405]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:17<00:26,  1.12it/s, loss=0.405]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:18<00:26,  1.12it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:18<00:25,  1.12it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:18<00:25,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:19<00:25,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:19<00:25,  1.12it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:20<00:24,  1.12it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:20<00:24,  1.12it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:21<00:23,  1.12it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:21<00:23,  1.12it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:22<00:22,  1.12it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:22<00:22,  1.12it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:23<00:21,  1.12it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:23<00:20,  1.12it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:24<00:20,  1.12it/s, loss=0.782]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:24<00:19,  1.12it/s, loss=0.782]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:25<00:19,  1.12it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:25<00:18,  1.12it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:26<00:18,  1.12it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:26<00:17,  1.12it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:26<00:17,  1.12it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:27<00:17,  1.12it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:27<00:17,  1.12it/s, loss=0.406]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:28<00:16,  1.12it/s, loss=0.406]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:28<00:16,  1.12it/s, loss=0.842]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:29<00:15,  1.12it/s, loss=0.842]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:29<00:15,  1.12it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:30<00:14,  1.12it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:30<00:14,  1.12it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:31<00:13,  1.12it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:31<00:13,  1.12it/s, loss=0.869]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:31<00:12,  1.12it/s, loss=0.869]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:32<00:12,  1.12it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:32<00:11,  1.12it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:33<00:11,  1.12it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:33<00:10,  1.12it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:34<00:10,  1.12it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:34<00:09,  1.12it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:35<00:09,  1.12it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:35<00:08,  1.11it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:35<00:08,  1.11it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:36<00:08,  1.11it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:36<00:08,  1.11it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:37<00:07,  1.11it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:37<00:07,  1.11it/s, loss=0.866]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:38<00:06,  1.12it/s, loss=0.866]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:38<00:06,  1.12it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:39<00:05,  1.12it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:39<00:05,  1.12it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:39<00:04,  1.11it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:40<00:04,  1.11it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:40<00:03,  1.11it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:41<00:03,  1.11it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:41<00:02,  1.11it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:42<00:02,  1.11it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:42<00:01,  1.11it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:43<00:01,  1.11it/s, loss=1.23] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:43<00:00,  1.11it/s, loss=1.23]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:44<00:00,  1.11it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:44<00:00,  1.12it/s, loss=0.638]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:58<00:00, 238.59s/it]0,  1.12it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:58<00:00,  1.05it/s, loss=0.629, dist_mean=0.269]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:58<00:00, 238.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# RM 트레이너 설정\n",
    "rm_trainer = RewardModelTrainer(model=rm_model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=torch.optim.Adam(rm_model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)\n",
    "\n",
    "# RM 학습 시작\n",
    "rm_trainer.fit(use_lora=0)\n",
    "\n",
    "# 학습된 RM 모델 저장\n",
    "rm_model.save_pretrained('./here_models/output_2_RM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb39955",
   "metadata": {},
   "source": [
    "##### **RM 6단계: RM 성능 평가**\n",
    "학습된 보상 모델이 실제로 문장의 퀄리티에 따라 점수를 잘 매기는지 몇 가지 예시 문장으로 테스트합니다. 긍정적이고 잘 구성된 문장일수록 높은 점수가 나와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fe4199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: 2.7\n",
      "input: 인공지능(AI)은 매우 유용합니다.\n",
      "reward score: 2.9\n",
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다.\n",
      "reward score: 2.9\n"
     ]
    }
   ],
   "source": [
    "# RM 추론 함수 정의\n",
    "def inference_RM(input_text):\n",
    "    input_ids = rm_tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device()\n",
    "    )\n",
    "    output = rm_model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f' % (input_text, output_reward))\n",
    "    return output_reward\n",
    "\n",
    "# 테스트 1: 부정적인 문장\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)\n",
    "\n",
    "# 테스트 2: 긍정적이고 짧은 문장\n",
    "input_text = '인공지능(AI)은 매우 유용합니다.'\n",
    "output_reward = inference_RM(input_text=input_text)\n",
    "\n",
    "# 테스트 3: 긍정적이고 상세한 문장\n",
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다.\"\n",
    "output_reward = inference_RM(input_text=input_text)\n",
    "\n",
    "# GPU 메모리 정리\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983fb16",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3. PPO를 통한 최종 모델 튜닝 및 비교\n",
    "> **[평가 기준 2.1, 2.2]** SFT, RM 모델의 결과와 최종 모델의 결과를 종합적으로 비교 분석합니다.\n",
    "\n",
    "이제 마지막 단계인 PPO(Proximal Policy Optimization)를 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d1637",
   "metadata": {},
   "source": [
    "##### **PPO 1단계: 필요 라이브러리 임포트**\n",
    "PPO 학습에 필요한 `GPTActor`, `GPTCritic`, `PPOTrainer` 등의 클래스를 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2574a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421ca9c",
   "metadata": {},
   "source": [
    "##### **PPO 2단계: Actor, Critic, Reward Model 등 준비**\n",
    "PPO 학습을 위해서는 여러 모델이 필요합니다.\n",
    "\n",
    "- **Actor**: 실제로 답변을 생성하는 모델입니다. 이전에 학습한 **SFT 모델(`output_1_SFT`)**을 불러와 초기 Actor로 사용합니다.\n",
    "- **Critic**: Actor가 생성한 답변의 품질을 평가하여 점수(value)를 매기는 모델입니다. 이전에 학습한 **RM 모델(`output_2_RM`)**을 불러와 Critic으로 사용합니다.\n",
    "- **Initial Model**: PPO 학습 과정에서 Actor가 기존 SFT 모델의 생성 방식에서 너무 멀어지지 않도록 제어(KL-divergence 페널티)하기 위한 레퍼런스 모델입니다. Actor 모델을 그대로 복사하여 만듭니다.\n",
    "- **Reward Model**: Critic 모델을 기반으로 생성된 답변에 대한 최종 보상(reward)을 계산하는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54ea02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveStrategy 컨텍스트 내에서 PPO 학습에 필요한 모든 모델을 준비합니다.\n",
    "with NaiveStrategy().model_init_context():\n",
    "    # Actor: SFT 모델을 불러옵니다.\n",
    "    actor = GPTActor(pretrained='./here_models/sft_output_model', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    # Critic: RM 모델을 불러옵니다.\n",
    "    critic = GPTCritic(pretrained='./here_models/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    # Tokenizer: 이전과 동일한 설정을 사용합니다.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    # Initial Model: SFT 모델을 복사하여 KL 페널티 계산에 사용합니다.\n",
    "    initial_model = deepcopy(actor)\n",
    "\n",
    "    # Reward Model: Critic 모델을 기반으로 보상 계산에 사용합니다.\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n",
    "\n",
    "# Actor와 Critic을 위한 옵티마이저를 설정합니다.\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "# Strategy를 통해 모델과 옵티마이저를 래핑합니다.\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a281c",
   "metadata": {},
   "source": [
    "##### **PPO 3단계: PPO 학습 데이터 준비**\n",
    "PPO 학습 단계에서는 답변이 없는 프롬프트만 필요합니다. 모델(Actor)이 프롬프트에 대한 답변을 직접 생성하고, 그 생성된 답변을 Critic이 평가하여 학습을 진행하기 때문입니다. `kochatgpt_3_PPO.jsonl` 파일을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4a504c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO 학습에 사용될 프롬프트 개수: 12000\n"
     ]
    }
   ],
   "source": [
    "# PPO 학습을 위한 프롬프트 데이터 로드\n",
    "with open('./KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "# PPO Trainer 내부에서 사용할 토크나이저 함수 정의\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(f\"PPO 학습에 사용될 프롬프트 개수: {len(list_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4119f62",
   "metadata": {},
   "source": [
    "##### **PPO 4단계: PPOTrainer 설정 및 학습**\n",
    "준비된 모든 컴포넌트(Actor, Critic, 모델, 옵티마이저 등)를 `PPOTrainer`에 전달하여 학습을 준비하고, `fit` 메소드를 호출하여 PPO 학습을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0113c364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.26s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000586]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0, critic_loss=0.000586]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0, critic_loss=0.243]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.243]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.0132]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0.0132]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:20<00:00,  6.78s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.30s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.252, critic_loss=0.064]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0.252, critic_loss=0.064]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0.251, critic_loss=0.149]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.251, critic_loss=0.149]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.282, critic_loss=0.107]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0.282, critic_loss=0.107]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:20<00:00,  6.80s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.23s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.143, critic_loss=0.0155]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.143, critic_loss=0.0155]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.147, critic_loss=0.00245]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.147, critic_loss=0.00245]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.153, critic_loss=0.0307] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.153, critic_loss=0.0307]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:20<00:00,  6.75s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.27s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.234, critic_loss=0.0781]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-0.234, critic_loss=0.0781]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-0.219, critic_loss=0.056] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=-0.219, critic_loss=0.056]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=-0.236, critic_loss=0.0249]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=-0.236, critic_loss=0.0249]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:20<00:00,  6.75s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.15s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.0576, critic_loss=0.0021]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-0.0576, critic_loss=0.0021]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-0.0511, critic_loss=0.00458]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-0.0511, critic_loss=0.00458]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-0.0477, critic_loss=0.026]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-0.0477, critic_loss=0.026]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:20<00:00,  6.74s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.17s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.212, critic_loss=0.0438]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.212, critic_loss=0.0438]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.2, critic_loss=0.0295]  \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.2, critic_loss=0.0295]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.193, critic_loss=0.0105]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=0.193, critic_loss=0.0105]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.19s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0016, critic_loss=0.000659]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.0016, critic_loss=0.000659]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.0485, critic_loss=0.0115]  \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0485, critic_loss=0.0115]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0079, critic_loss=0.022] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.0079, critic_loss=0.022]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:20<00:00,  6.74s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.17s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.148, critic_loss=0.0234]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=-0.148, critic_loss=0.0234]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=-0.145, critic_loss=0.0157]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-0.145, critic_loss=0.0157]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-0.149, critic_loss=0.00756]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-0.149, critic_loss=0.00756]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:20<00:00,  6.71s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.51s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.0239, critic_loss=0.00051]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-0.0239, critic_loss=0.00051]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-0.0182, critic_loss=0.00298]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-0.0182, critic_loss=0.00298]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-0.0142, critic_loss=0.011]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=-0.0142, critic_loss=0.011]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:18<00:00,  6.32s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.20s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.141, critic_loss=0.0116]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.141, critic_loss=0.0116]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.124, critic_loss=0.0101]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.124, critic_loss=0.0101]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.121, critic_loss=0.0045]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.121, critic_loss=0.0045]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:18<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# PPOTrainer 초기화\n",
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,\n",
    "                     train_batch_size=8,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# PPO 학습 시작\n",
    "trainer.fit(list_prompt,\n",
    "            num_episodes=10,\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "# 최종 PPO 모델 저장\n",
    "actor.model.save_pretrained('./here_models/output_3_PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef97e3",
   "metadata": {},
   "source": [
    "##### **PPO 5단계: 최종 모델 성능 평가**\n",
    "PPO 학습까지 완료된 최종 모델을 로드하여, 베이스라인 및 SFT 모델과 동일한 프롬프트에 대한 답변을 생성하고 비교합니다. RLHF의 모든 과정을 거친 후 모델의 답변이 얼마나 더 자연스럽고 유용하게 변화했는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5e1502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PPO Model Generation Results ---\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이므로, 질문에 답변을 제공하기는 어렵습니다. \\n\\n저는 AI 어시스턴트로서, 한우 육우를 판매하는 식당이나 웹사이트를 통해 정보를 얻으실 수 있습니다. 따라서, 해당 식당이나 웹사이트에서 직접 확인하실 수 있도록 노력하실 예정이다. \\n\\n하지만 한우 고기 한우의 경우 정확한 답변을 드릴 수 없어서 불고기용 고기의 추가 정보를 확인하시면 정확한 답변을 드릴 수 있을 것입니다. 또한, 한우 육우의 등급이나 부위를 알려주시면 더욱 정확한 답변을 드릴 수 있을 것입니다.\n",
      "------------------------------\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'저는 제임스 메서리가 사용한 답변을 사용하는 기능을 제공하지 않습니다. 제가 AI 언어모델이므로, 리처드 닉슨이 34대 부통령직을 수행한 년도와 관련된 구체적인 정보를 알 수 없습니다. 더 많은 정보를 제공해주시면 제가 답변을 더 정확하게 제공할 수 있습니다. 감사합니다.\n",
      "------------------------------\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국의 뉴욕주 시카고에서 워싱턴 D.C.를 연결하는 뉴욕 공항으로 이름이 높습니다. 그러나 워싱턴 D.C.에는 세계 무역센터나 워싱턴 D.C.에 위치한 미국 대사관이나 미국의 대통령 도서관, 워싱턴 D.C.에는 대통령 전용 공간도 위치하고 있어 미국 대사관이나 워싱턴 D.C.에 위치한 미국 대사관 등의 공공장소에서도 볼 수 있습니다. 이러한 국제공항은 뉴욕을 중심으로 한 도시로서 세계 전역으로 퍼져 나가는 경향이 있습니다.\n",
      "------------------------------\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이므로 미세먼지 정보를 알 수 없습니다. 그러나, 미세먼지에 대한 걱정이나 우려 때문에 정확한 답변을 드리기 어렵습니다. 미세먼지가 나쁜 날이라면 대중교통을 이용하거나 전문가들의 조언을 받는 것이 좋습니다. 또한 주변에 있는 미세먼지 감시소를 방문하거나 전문가와 상담할 수도 있습니다.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 최종 PPO 모델로 답변 생성\n",
    "def generation(input_text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "\n",
    "    # PPO 모델은 Actor 클래스로 래핑되어 있으므로, 내부 모델을 직접 사용\n",
    "    outputs = model.model.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "# 테스트용 프롬프트 리스트\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "# SFT/PPO 모델에 맞는 프롬프트 형식으로 변환\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "list_formatted_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "print(\"--- PPO Model Generation Results ---\")\n",
    "for input_text in list_formatted_prompt:\n",
    "    generation(input_text, actor, tokenizer)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e1ad20a-fe8d-404b-9639-fb1212fee598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.2.6)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.35.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from bert-score) (2.7.1+cu118)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from bert-score) (4.56.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from bert-score) (3.10.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.0.0->bert-score) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: evaluate, bert-score\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [bert-score]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bert-score-0.3.13 evaluate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b9890d39-6eab-4c62-b354-54cb3b031528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from rouge_score) (2.2.6)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=d7bbf2facc31af9ac2bba010d384fc17f11b670b5af245f932d39a6983487e8b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: nltk, absl-py, rouge_score\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [rouge_score]\u001b[0m [absl-py]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 nltk-3.9.1 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score nltk absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7137aa6a-9a28-447a-8d51-38fcb8b8917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "\n",
    "BASE_ID = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Baseline (Hugging Face Hub에서 바로 불러오기)\n",
    "kogpt_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# SFT (실제로 저장된 경로: ./here_models/sft_output_model)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./here_models/sft_output_model\",\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# PPO (실제로 저장된 경로: ./here_models/output_3_PPO)\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./here_models/output_3_PPO\",\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07b874ae-8ae5-4922-bf2f-68f43d96effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(prompt, reference, kogpt_model, sft_model, ppo_model, tokenizer):\n",
    "    models = {\n",
    "        \"KoGPT2 (Baseline)\": kogpt_model,\n",
    "        \"SFT Model\": sft_model,\n",
    "        \"PPO Model\": ppo_model,\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # 정량 평가\n",
    "        bleu_score = bleu.compute(predictions=[generated_text], references=[[reference]])\n",
    "        rouge_score = rouge.compute(predictions=[generated_text], references=[reference])\n",
    "\n",
    "        # 정성 평가 (간단한 rule-based)\n",
    "        qualitative = []\n",
    "        if prompt[:10] in generated_text:\n",
    "            qualitative.append(\"문맥 일관성: 높음\")\n",
    "        else:\n",
    "            qualitative.append(\"문맥 일관성: 낮음\")\n",
    "\n",
    "        if len(generated_text.split()) > 5:\n",
    "            qualitative.append(\"유창성: 양호\")\n",
    "        else:\n",
    "            qualitative.append(\"유창성: 부족\")\n",
    "\n",
    "        if \"새로운\" in generated_text or \"다양한\" in generated_text:\n",
    "            qualitative.append(\"창의성: 있음\")\n",
    "        else:\n",
    "            qualitative.append(\"창의성: 보통\")\n",
    "\n",
    "        results[name] = {\n",
    "            \"Generated Text\": generated_text,\n",
    "            \"BLEU\": bleu_score[\"bleu\"],\n",
    "            \"ROUGE\": rouge_score[\"rougeL\"],\n",
    "            \"Qualitative\": qualitative\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "97b0ed0f-2dad-422f-ae7f-b71d594de3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[KoGPT2 (Baseline)]\n",
      "- 생성 텍스트:\n",
      "인공지능의 미래는 무엇일까요? 교육, 의료, 산업, 일상생활 측면에서 각각 어떤 변화를 가져올지 설명해주세요.\"\n",
      "\"그렇다면 어떻게 하면 더 많은 사람들이 더 많은 정보를 얻을 수 있을까요?\"\n",
      "\"그렇다면 어떻게 하면 더 많은 사람들이 더 많은 정보를 얻을 수 있을까요?\"\n",
      "\"그렇다면 어떻게 하면 더 많은 사람들이 더 많은\n",
      "- BLEU: 0.0\n",
      "- ROUGE: 0.0\n",
      "==================================================\n",
      "[SFT Model]\n",
      "- 생성 텍스트:\n",
      "인공지능의 미래는 무엇일까요? 교육, 의료, 산업, 일상생활 측면에서 각각 어떤 변화를 가져올지 설명해주세요.\\n\\n1. 인공지능: 인간의 지능은 매우 다양하며, 다양한 분야에서 활용됩니다. 예를 들어, 컴퓨터, 스마트폰, TV, 냉장고, 세탁기 등 다양한 분야에서 활용됩니다.\\n\\n2. 인공지능: 인간의 지능은\n",
      "- BLEU: 0.0\n",
      "- ROUGE: 0.0\n",
      "==================================================\n",
      "[PPO Model]\n",
      "- 생성 텍스트:\n",
      "인공지능의 미래는 무엇일까요? 교육, 의료, 산업, 일상생활 측면에서 각각 어떤 변화를 가져올지 설명해주세요. 예를 들어, 의료, 산업, 일상생활에서의 변화는 다양한 분야에서 나타날 수 있습니다. 예를 들어, 의료 분야에서는 의료, 산업, 일상생활에서의 변화는 다양한 분야에서 나타날 수 있습니다. 예를 들어, 의료 분야에서는 의료, 산업, 일상생활에서의 변화는 다양한 분야에서 나타날 수 있습니다. 예를 들어, 의료 분야에서는\n",
      "- BLEU: 0.0\n",
      "- ROUGE: 0.0\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"인공지능의 미래는 무엇일까요? \"\n",
    "    \"교육, 의료, 산업, 일상생활 측면에서 각각 어떤 변화를 가져올지 설명해주세요.\"\n",
    ")\n",
    "\n",
    "references = [\n",
    "    \"인공지능은 다양한 산업과 일상생활에서 점점 더 중요한 역할을 할 것입니다.\",\n",
    "    \"AI는 의료, 교육, 산업 전반에서 핵심적인 기술이 되어 인간의 삶을 크게 바꿀 것입니다.\",\n",
    "    \"앞으로 인공지능은 인간과 협력하며 생산성과 창의성을 높이는 방향으로 발전할 것입니다.\"\n",
    "]\n",
    "\n",
    "results = evaluate_models(prompt, reference, kogpt_model, sft_model, ppo_model, tokenizer)\n",
    "\n",
    "for model_name, evals in results.items():\n",
    "    print(\"=\"*50)\n",
    "    print(f\"[{model_name}]\")\n",
    "    print(\"- 생성 텍스트:\")\n",
    "    print(evals[\"Generated Text\"])\n",
    "    print(\"- BLEU:\", evals[\"BLEU\"])\n",
    "    print(\"- ROUGE:\", evals[\"ROUGE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1cde5",
   "metadata": {},
   "source": [
    "## 3. 프로젝트 전략 선택 및 수행 (평가 기준 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c14a9",
   "metadata": {},
   "source": [
    "### 전략 1: 기존 데이터셋 추가 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549de460",
   "metadata": {},
   "source": [
    "#### 1.1. 데이터셋 로드 및 EDA\n",
    "SFT, RM, PPO 각 말뭉치의 도메인, 문체, 길이 분포, 완성도 등을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac24ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c74a390-3ffb-428e-8720-7ffc2833b7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/0917'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7314839-283c-4e10-880d-3371bf2eaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "sft_file = './KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "rm_file = './KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "ppo_file = './KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "\n",
    "# 데이터 로딩 함수\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    .jsonl 또는 .json 배열 형식의 파일을 읽어 파이썬 리스트로 반환합니다.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        # 먼저 파일 전체를 하나의 JSON 배열로 읽어봅니다.\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # 위 방식이 실패하면, .jsonl 형식(한 줄에 하나의 JSON)으로 다시 시도합니다.\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"'{file_path}' 파일의 다음 줄을 파싱하는 데 실패했습니다: {line.strip()}\")\n",
    "    return data\n",
    "\n",
    "sft_data = load_data(sft_file)\n",
    "rm_data = load_data(rm_file)\n",
    "ppo_data = load_data(ppo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4fc6669-93dc-40d2-811d-5b25558bcbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SFT 데이터셋 분석 결과\n",
      "전체 데이터 개수: 12000\n",
      "         prompt_len  completion_len     total_len\n",
      "count  12000.000000    12000.000000  12000.000000\n",
      "mean      22.180583      144.107250    166.287833\n",
      "std       14.110028      122.843692    122.202738\n",
      "min        0.000000        4.000000     11.000000\n",
      "25%       13.000000       62.000000     85.000000\n",
      "50%       19.000000      118.000000    139.000000\n",
      "75%       28.000000      185.000000    207.000000\n",
      "max      295.000000     1553.000000   1580.000000\n",
      "\n",
      "SFT 데이터 샘플 :\n",
      "           prompt                                         completion  tokens  \\\n",
      "0   불고기용 고기 한우에요?  '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...     193   \n",
      "1  쓰던 앱이 유료로 전환됐어  '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...     288   \n",
      "2         여친이랑 다툼  '저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...     153   \n",
      "3         술 먹고 싶어  '술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...     189   \n",
      "4           잊고싶다.  '저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...     147   \n",
      "\n",
      "   prompt_len  completion_len  total_len  \n",
      "0          13             203        216  \n",
      "1          14             305        319  \n",
      "2           7             160        167  \n",
      "3           7             180        187  \n",
      "4           5             152        157  \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# SFT 데이터프레임 생성 및 분석\n",
    "df_sft = pd.DataFrame(sft_data)\n",
    "df_sft['prompt_len'] = df_sft['prompt'].str.len()\n",
    "df_sft['completion_len'] = df_sft['completion'].str.len()\n",
    "df_sft['total_len'] = df_sft['prompt_len'] + df_sft['completion_len']\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"SFT 데이터셋 분석 결과\")\n",
    "print(f\"전체 데이터 개수: {len(df_sft)}\")\n",
    "print(df_sft[['prompt_len', 'completion_len', 'total_len']].describe())\n",
    "print(\"\\nSFT 데이터 샘플 :\")\n",
    "print(df_sft.head())\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed7bb74-7728-45ee-afe3-fc731a5cd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RM 데이터셋 분석 결과\n",
      "전체 데이터 개수: 10220\n",
      "\n",
      "통계 정보 :\n",
      "         prompt_len  completions_len_mean\n",
      "count  10220.000000          10220.000000\n",
      "mean      22.203229            116.768102\n",
      "std       14.297097             75.593683\n",
      "min        0.000000              4.666667\n",
      "25%       13.000000             70.333333\n",
      "50%       19.000000            101.333333\n",
      "75%       28.000000            141.666667\n",
      "max      295.000000           1296.000000\n",
      "\n",
      "RM 데이터 샘플 :\n",
      "                                              prompt  prompt_len  \\\n",
      "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...          59   \n",
      "1                            개포주공아파트는 몇 단지로 이루어져 있나?          23   \n",
      "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?          34   \n",
      "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야          24   \n",
      "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?          30   \n",
      "\n",
      "   completions_len_mean  \n",
      "0             39.000000  \n",
      "1             22.666667  \n",
      "2            209.333333  \n",
      "3            212.666667  \n",
      "4             43.333333  \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# RM 데이터프레임 생성 및 분석\n",
    "df_rm = pd.DataFrame(rm_data)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RM 데이터셋 분석 결과\")\n",
    "print(f\"전체 데이터 개수: {len(df_rm)}\")\n",
    "\n",
    "if not df_rm.empty:\n",
    "    # 'prompt' 컬럼 길이 계산\n",
    "    df_rm['prompt_len'] = df_rm['prompt'].str.len()\n",
    "    \n",
    "    # 3개의 completion 컬럼의 길이를 각각 구한 뒤, 평균을 계산합니다.\n",
    "    df_rm['completions_len_mean'] = df_rm[['completion_0', 'completion_1', 'completion_2']].apply(\n",
    "        lambda row: np.mean([len(text) for text in row]), \n",
    "        axis=1 # row(행) 단위로 함수를 적용\n",
    "    )\n",
    "    \n",
    "    print(\"\\n통계 정보 :\")\n",
    "    print(df_rm[['prompt_len', 'completions_len_mean']].describe())\n",
    "    print(\"\\nRM 데이터 샘플 :\")\n",
    "    print(df_rm[['prompt', 'prompt_len', 'completions_len_mean']].head())\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8d0bf71-63e6-406a-bfbb-7c63c68694c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PPO 데이터셋 분석 결과\n",
      "전체 데이터 개수: 12000\n",
      "count    12000.000000\n",
      "mean        22.180583\n",
      "std         14.110028\n",
      "min          0.000000\n",
      "25%         13.000000\n",
      "50%         19.000000\n",
      "75%         28.000000\n",
      "max        295.000000\n",
      "Name: prompt_len, dtype: float64\n",
      "\n",
      "PPO 데이터 샘플 :\n",
      "                                              prompt  prompt_len\n",
      "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...          59\n",
      "1                            개포주공아파트는 몇 단지로 이루어져 있나?          23\n",
      "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?          34\n",
      "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야          24\n",
      "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?          30\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# PPO 데이터프레임 생성 및 분석\n",
    "df_ppo = pd.DataFrame(ppo_data)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PPO 데이터셋 분석 결과\")\n",
    "print(f\"전체 데이터 개수: {len(df_ppo)}\")\n",
    "if not df_ppo.empty:\n",
    "    df_ppo['prompt_len'] = df_ppo['prompt'].str.len()\n",
    "    print(df_ppo['prompt_len'].describe())\n",
    "    print(\"\\nPPO 데이터 샘플 :\")\n",
    "    print(df_ppo.head())\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0acb6b19-0f77-4d6b-9a6c-9e7a072f2b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path, size=14)\n",
    "title_fontprop = fm.FontProperties(fname=font_path, size=20)\n",
    "\n",
    "# 데이터 길이 분포 시각화\n",
    "if not df_sft.empty and not df_rm.empty and not df_ppo.empty:\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "    # fig.suptitle()에 fontproperties를 직접 전달합니다.\n",
    "    fig.suptitle('데이터셋별 텍스트 길이 분포', fontproperties=title_fontprop)\n",
    "\n",
    "    # 1행: SFT 데이터\n",
    "    sns.histplot(df_sft['prompt_len'], ax=axes[0, 0], kde=True, bins=50)\n",
    "    axes[0, 0].set_title('SFT Prompt 길이', fontproperties=fontprop)\n",
    "\n",
    "    sns.histplot(df_sft['completion_len'], ax=axes[0, 1], kde=True, color='orange', bins=50)\n",
    "    axes[0, 1].set_title('SFT Completion 길이', fontproperties=fontprop)\n",
    "\n",
    "    # 2행: RM 데이터\n",
    "    sns.histplot(df_rm['prompt_len'], ax=axes[1, 0], kde=True, bins=50)\n",
    "    axes[1, 0].set_title('RM Prompt 길이', fontproperties=fontprop)\n",
    "\n",
    "    sns.histplot(df_rm['completions_len_mean'], ax=axes[1, 1], kde=True, color='purple', bins=50)\n",
    "    axes[1, 1].set_title('RM Completions 평균 길이', fontproperties=fontprop)\n",
    "\n",
    "    # 3행: PPO 데이터\n",
    "    sns.histplot(df_ppo['prompt_len'], ax=axes[2, 0], kde=True, color='green', bins=50)\n",
    "    axes[2, 0].set_title('PPO Prompt 길이', fontproperties=fontprop)\n",
    "    \n",
    "    axes[2, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"데이터프레임이 비어 있어 시각화를 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae57365-23e9-466c-9ef0-4c4daf843cb5",
   "metadata": {},
   "source": [
    "#### 📊 EDA 결과 해석\n",
    "그래프를 통해 데이터 길이 분포의 두 가지 주요 특징을 파악할 수 있습니다.\n",
    "\n",
    "1. Prompt(지시어) 길이 분포\n",
    "- SFT, RM, PPO 세 데이터셋 모두에서 Prompt의 길이는 매우 유사한 패턴을 보입니다.\n",
    "- 분포가 오른쪽으로 긴 꼬리를 가지는 Right-Skewed 형태입니다. 이는 대부분의 Prompt가 50자 미만으로 매우 짧다는 것을 의미하며, 0에 가까운 길이의 데이터도 일부 존재합니다.\n",
    "- 소수의 Prompt는 250자를 넘어가는 긴 길이를 가지는 **이상치(Outlier)**로 보입니다.\n",
    "\n",
    "2. Completion(응답) 길이 분포\n",
    "- SFT와 RM 데이터셋의 Completion 길이는 Prompt보다 길고 데이터가 더 넓게 분포해 있습니다.\n",
    "- 마찬가지로 오른쪽으로 긴 꼬리를 가지는 형태이며, 대부분의 응답 길이는 50자에서 200자 사이에 집중되어 있습니다.\n",
    "- 1,000자를 훌쩍 넘는 매우 긴 Completion 데이터들이 존재하는데, 이는 모델 학습에 부정적인 영향을 줄 수 있는 이상치일 가능성이 높습니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🛠️ 데이터 정제 방향\n",
    "위 해석을 바탕으로 모델의 학습 효율과 안정성을 높이기 위한 데이터 정제 전략은 다음과 같습니다.\n",
    "\n",
    "1. 너무 짧은 데이터 제거 (Minimum Length Filtering)\n",
    "- 근거: Prompt나 Completion의 길이가 0에 가깝거나 지나치게 짧은 데이터(예: 10자 미만)는 유의미한 정보를 담고 있지 않을 가능성이 높으며, 이는 모델이 의미 없는 패턴을 학습하게 만들 수 있습니다.\n",
    "- 전략: 최소 글자 수를 기준으로 이보다 짧은 데이터는 학습 데이터셋에서 제외합니다. 예를 들어, Prompt 길이 > 10, Completion 길이 > 20과 같은 기준을 적용할 수 있습니다.\n",
    "\n",
    "2. 너무 긴 데이터(이상치) 제거 (Maximum Length Filtering)\n",
    "- 근거: 그래프의 긴 꼬리 부분에 해당하는 매우 긴 데이터들은 다음과 같은 문제를 유발할 수 있습니다.\n",
    "- 메모리 문제: 학습 시 더 많은 GPU 메모리를 차지합니다.\n",
    "- 학습 불안정: 긴 텍스트 하나가 Loss 계산에 큰 영향을 주어 학습을 불안정하게 만듭니다.\n",
    "- 품질 저하: 기계로 생성된 매우 긴 텍스트는 의미 없는 내용이 반복될 수 있습니다.\n",
    "- 잘림(Truncation): 대부분의 모델은 최대 입력 토큰 길이가 정해져 있어(예: 512, 1024), 이보다 긴 텍스트는 어차피 잘려서 정보 손실이 발생합니다.\n",
    "\n",
    "---\n",
    "\n",
    "- 최대 글자 수를 기준으로 이를 초과하는 이상치를 제거합니다. 그래프 분포를 고려할 때, 대부분의 데이터를 유지하면서 긴 꼬리 부분만 잘라내는 값을 기준으로 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1592389",
   "metadata": {},
   "source": [
    "#### 1.2. 데이터 정제 및 증강\n",
    "분석 결과를 바탕으로 데이터를 정제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991a1cf5-42fd-4f20-a0c9-f3ca221f8003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT 데이터 로드 완료: 12000개\n"
     ]
    }
   ],
   "source": [
    "df_sft = pd.DataFrame(sft_data)\n",
    "print(f\"SFT 데이터 로드 완료: {len(df_sft)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df75f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SFT 데이터셋 정제 시작 ---\n",
      "정제 전 데이터 수: 12000\n",
      "중복 제거 후 데이터 수: 12000 (0개 제거)\n",
      "길이 필터링 후 데이터 수: 9745 (2255개 제거)\n",
      "상투적 답변 필터링: 1533개 제거\n",
      "\n",
      "--- 정제 완료 ---\n",
      "최종 데이터 수: 8212\n",
      "정제된 SFT 데이터를 'kochatgpt_1_SFT_refined.jsonl'에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 데이터 정제 함수 정의\n",
    "def normalize_text(text):\n",
    "    text = str(text); text = re.sub(r'\\s+', ' ', text).strip(); text = text.replace('\\\\n', ' '); return text\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ一-龥.,?!\\\"\\'\\s]', '', text)\n",
    "def filter_canned_responses(df):\n",
    "    canned_patterns = [\"저는 인공지능\", \"저는 AI\", \"제공해 드릴 수 없습니다\", \"도움을 드릴 수는 없습니다\"]\n",
    "    original_count = len(df)\n",
    "    for pattern in canned_patterns:\n",
    "        df = df[~df['completion'].str.contains(pattern, case=False, na=False)]\n",
    "    removed_count = original_count - len(df)\n",
    "    if removed_count > 0: print(f\"상투적 답변 필터링: {removed_count}개 제거\")\n",
    "    return df\n",
    "\n",
    "# SFT 데이터셋 정제 파이프라인\n",
    "if not df_sft.empty:\n",
    "    print(\"\\n--- SFT 데이터셋 정제 시작 ---\")\n",
    "    initial_rows = len(df_sft)\n",
    "    print(f\"정제 전 데이터 수: {initial_rows}\")\n",
    "\n",
    "    df_sft['prompt'] = df_sft['prompt'].apply(normalize_text).apply(remove_special_chars)\n",
    "    df_sft['completion'] = df_sft['completion'].apply(normalize_text).apply(remove_special_chars)\n",
    "\n",
    "    df_sft.drop_duplicates(subset=['prompt', 'completion'], inplace=True)\n",
    "    rows_after_dedup = len(df_sft)\n",
    "    print(f\"중복 제거 후 데이터 수: {rows_after_dedup} ({initial_rows - rows_after_dedup}개 제거)\")\n",
    "\n",
    "    df_sft['prompt_len'] = df_sft['prompt'].str.len()\n",
    "    df_sft['completion_len'] = df_sft['completion'].str.len()\n",
    "    df_sft_refined = df_sft[(df_sft['prompt_len'] > 10) & (df_sft['completion_len'] > 20) & (df_sft['prompt_len'] < 2048) & (df_sft['completion_len'] < 2048)].copy()\n",
    "    rows_after_len_filter = len(df_sft_refined)\n",
    "    print(f\"길이 필터링 후 데이터 수: {rows_after_len_filter} ({rows_after_dedup - rows_after_len_filter}개 제거)\")\n",
    "    \n",
    "    df_sft_refined = filter_canned_responses(df_sft_refined)\n",
    "\n",
    "    refined_sft_path = 'kochatgpt_1_SFT_refined.jsonl'\n",
    "    df_to_save = df_sft_refined[['prompt', 'completion']]\n",
    "    df_to_save.to_json(refined_sft_path, orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    print(f\"\\n--- 정제 완료 ---\")\n",
    "    print(f\"최종 데이터 수: {len(df_to_save)}\")\n",
    "    print(f\"정제된 SFT 데이터를 '{refined_sft_path}'에 저장했습니다.\")\n",
    "else:\n",
    "    print(\"\\nSFT 데이터 로딩에 실패하여 정제 작업을 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba8aed",
   "metadata": {},
   "source": [
    "#### 1.3. 모델 재학습 및 성능 비교\n",
    "> **[평가 기준 1.1]** 기존 데이터셋을 추가로 정제하고, generation 성능을 올리기 위한 기법(Beam search, Top-k sampling 등)을 실험해 모델 성능을 향상시켰는가?\n",
    "\n",
    "정제된 데이터셋으로 모델을 재학습시키고, 주관적 평가와 BLEU, ROUGE 등 정량적 평가 결과를 비교 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ea46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc1c0ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956732f9",
   "metadata": {},
   "source": [
    "### 전략 2: 새로운 데이터셋 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef5adc",
   "metadata": {},
   "source": [
    "#### 2.1. 데이터 수집\n",
    "웹 크롤링 등을 통해 하나의 질문에 다양한 품질의 댓글이 달린 한국어 데이터를 수집합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd7d8e5-5359-43a5-ae33-5dcbe7dbc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "# 데이터 로드\n",
    "koalpaca_dataset = load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train\")\n",
    "\n",
    "def convert_koalpaca(example):\n",
    "    if example.get(\"input\", \"\").strip():\n",
    "        prompt = f\"{example['instruction']}\\n\\nInput: {example['input']}\"\n",
    "    else:\n",
    "        prompt = example[\"instruction\"]\n",
    "    return {\"prompt\": prompt, \"completion\": example[\"output\"]}\n",
    "\n",
    "koalpaca_dataset = koalpaca_dataset.map(\n",
    "    convert_koalpaca,\n",
    "    remove_columns=koalpaca_dataset.column_names\n",
    ")\n",
    "\n",
    "# 기존 SFT jsonl 데이터셋 불러오기\n",
    "sft_dataset = load_dataset(\"json\", data_files={\"train\": \"kochatgpt_1_SFT_refined.jsonl\"})\n",
    "\n",
    "# 두 데이터셋 합치기\n",
    "combined_dataset = concatenate_datasets([sft_dataset[\"train\"], koalpaca_dataset])\n",
    "\n",
    "# 토크나이저 준비\n",
    "model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# EOS / PAD 보정\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"\"})\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. 전처리\n",
    "def preprocess_function(examples):\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n{c}\"\n",
    "             for p, c in zip(examples[\"prompt\"], examples[\"completion\"])]\n",
    "    out = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()   # ← 중요\n",
    "    return out\n",
    "\n",
    "tokenized_dataset = combined_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e6527-9070-4147-9e34-1f5b84d61851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4594' max='7342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4594/7342 26:40 < 15:57, 2.87 it/s, Epoch 0.63/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.998300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.937700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.922200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.970800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.035900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "if len(tokenizer) != model.get_input_embeddings().weight.size(0):\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 학습 중 캐시 끄기\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Trainer 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results-strategy2-full-kogpt2\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=True,                      # bf16=True 가능 환경이면 bf16 권장\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b1597-99ba-418f-a345-a20ebb6436ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator 함수 (추론용)\n",
    "def generator(prompt, max_new_tokens=128):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():  # 추론에서는 gradient 불필요\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id  # ★ PAD 토큰 명시 (중요)\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 테스트 예시\n",
    "print(generator(\"인공지능이 사회에 미치는 영향에 대해 설명해 주세요.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b92964",
   "metadata": {},
   "source": [
    "#### 2.2. Ranking 데이터셋 구축\n",
    "수집한 데이터를 기반으로 Reward 점수를 차등적으로 적용할 수 있는 Instruction 및 Ranking 데이터셋을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea76c8-5c29-455c-8440-116a3575536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. instruction + output 묶기\n",
    "#    instruction 별로 여러 답변이 있을 수 있도록 그룹화\n",
    "from collections import defaultdict\n",
    "grouped = defaultdict(list)\n",
    "\n",
    "for row in koalpaca:\n",
    "    inst = row[\"instruction\"]\n",
    "    # input이 있으면 합쳐줌\n",
    "    if row.get(\"input\", \"\").strip():\n",
    "        prompt = f\"{inst}\\n\\nInput: {row['input']}\"\n",
    "    else:\n",
    "        prompt = inst\n",
    "    grouped[prompt].append(row[\"output\"])\n",
    "\n",
    "# 3. pairwise ranking 데이터 생성\n",
    "ranking_data = []\n",
    "for inst, responses in grouped.items():\n",
    "    if len(responses) < 2:\n",
    "        continue  # 최소 2개 이상 답변 필요\n",
    "    # 랜덤하게 2개 뽑기\n",
    "    r1, r2 = random.sample(responses, 2)\n",
    "    # 임의 기준으로 chosen / rejected 나누기\n",
    "    # (실제 연구에서는 human preference 필요!)\n",
    "    chosen, rejected = (r1, r2) if len(r1) >= len(r2) else (r2, r1)\n",
    "\n",
    "    ranking_data.append({\n",
    "        \"instruction\": inst,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected\n",
    "    })\n",
    "\n",
    "# 4. Dataset 객체로 변환\n",
    "ranking_dataset = Dataset.from_list(ranking_data)\n",
    "\n",
    "print(ranking_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a3769a-0fb0-477c-a7cd-d1f86336dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "# 1. 베이스 모델 (KoGPT2)\n",
    "base_model_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Reward Model 정의 (num_labels=1 → 스칼라 점수 예측)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "# 3. 전처리 함수\n",
    "def preprocess_rm_function(examples):\n",
    "    # chosen/rejected 둘 다 토큰화\n",
    "    chosen_encodings = tokenizer(\n",
    "        [f\"{examples['instruction']}\\n\\n{c}\" for c in examples[\"chosen\"]],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    rejected_encodings = tokenizer(\n",
    "        [f\"{examples['instruction']}\\n\\n{r}\" for r in examples[\"rejected\"]],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_encodings[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_encodings[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_encodings[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_encodings[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 4. 데이터셋 토큰화\n",
    "tokenized_ranking = ranking_dataset.map(preprocess_rm_function, batched=True)\n",
    "\n",
    "# 5. RewardTrainer 설정\n",
    "reward_args = RewardConfig(\n",
    "    output_dir=\"./results-strategy2-rm\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_args,\n",
    "    train_dataset=tokenized_ranking,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 6. 학습 시작\n",
    "reward_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e534b8",
   "metadata": {},
   "source": [
    "#### 2.3. 모델 재학습 및 추론 결과 비교\n",
    "> **[평가 기준 1.2]** 새로운 데이터를 수집해 전처리를 수행하여 모델의 성능을 향상시켰는가?\n",
    "\n",
    "기존 데이터셋에 새로운 데이터셋을 추가하여 모델을 재학습시키고, 추론 결과를 비교 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "# 디렉터리들 (앞 단계에서 사용한 경로와 맞춰주세요)\n",
    "BASE_ID = \"skt/kogpt2-base-v2\"\n",
    "SFT_OUT_DIR = \"./results-strategy2-full-kogpt2\"        # 전략2 SFT 결과(학습된 LM) 저장된 폴더\n",
    "RM_DIR = \"./results-strategy2-rm\"                      # 앞서 학습한 Reward Model 저장 폴더\n",
    "PPO_OUT_DIR = \"./results-strategy2-ppo-kogpt2\"        # PPO 결과 저장 폴더\n",
    "\n",
    "# 토크나이저\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# SFT 최종 체크포인트 자동 선택(마지막 checkpoint-XXXX 찾기, 없으면 SFT_OUT_DIR 자체 사용)\n",
    "ckpts = sorted(glob.glob(os.path.join(SFT_OUT_DIR, \"checkpoint-*\")), key=lambda p: int(p.split(\"-\")[-1]))\n",
    "POLICY_INIT_DIR = ckpts[-1] if ckpts else SFT_OUT_DIR\n",
    "print(\"PPO 초기 정책(checkpoint):\", POLICY_INIT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43104c88-df5c-492d-bf7e-bae11f789b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1A) 업로드된 PPO 데이터 (권장)\n",
    "try:\n",
    "    ppo_ds = load_dataset(\"json\", data_files={\"train\": \"./KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl\"}, split=\"train\")\n",
    "    # 컬럼 자동 감지\n",
    "    PROMPT_KEYS = [\"prompt\",\"instruction\",\"query\",\"input\",\"question\"]\n",
    "    p_key = next((k for k in PROMPT_KEYS if k in ppo_ds.column_names), None)\n",
    "    assert p_key, f\"프롬프트 컬럼을 찾지 못했습니다. 후보={PROMPT_KEYS}, 실제={ppo_ds.column_names}\"\n",
    "    ppo_ds = ppo_ds.rename_column(p_key, \"query\")\n",
    "    cols_to_remove = [c for c in ppo_ds.column_names if c != \"query\"]\n",
    "    if cols_to_remove: ppo_ds = ppo_ds.remove_columns(cols_to_remove)\n",
    "    print(\"PPO prompts:\", ppo_ds[0])\n",
    "except Exception as e:\n",
    "    print(\"업로드 PPO 데이터 사용 실패. KoAlpaca에서 instruction만 추출해 사용합니다.\", e)\n",
    "    # 1B) KoAlpaca에서 instruction만 사용\n",
    "    koa = load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train\")\n",
    "    def to_query(ex):\n",
    "        inst = ex[\"instruction\"]\n",
    "        if ex.get(\"input\",\"\").strip():\n",
    "            inst = f\"{inst}\\n\\nInput: {ex['input']}\"\n",
    "        return {\"query\": inst}\n",
    "    ppo_ds = koa.map(to_query, remove_columns=koa.column_names)\n",
    "    print(\"PPO prompts (from KoAlpaca):\", ppo_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c9d1e-cf9d-420f-b1c2-0fa155546159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM 로드\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    RM_DIR, num_labels=1, device_map=\"auto\"\n",
    ")\n",
    "rm_model.eval()\n",
    "\n",
    "def get_reward(prompts, responses):\n",
    "    \"\"\"RM으로 스칼라 보상 계산\"\"\"\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n{r}\" for p, r in zip(prompts, responses)]\n",
    "    toks = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(next(rm_model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = rm_model(**toks).logits.squeeze(-1)  # (batch,)\n",
    "    # 보상 스케일을 살짝 안정화하고 싶다면 .tanh() 등도 가능\n",
    "    return out.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b2604-4239-4979-903b-c3a9146e2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정책 모델 (SFT 결과 체크포인트)\n",
    "policy = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    POLICY_INIT_DIR, device_map=\"auto\"\n",
    ")\n",
    "policy.pretrained_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "policy.eval()\n",
    "\n",
    "# 레퍼런스(초기 정책 스냅샷)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    POLICY_INIT_DIR, device_map=\"auto\"\n",
    ")\n",
    "ref_model.pretrained_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "ref_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d25030-2f38-4de6-99e5-51eae95b7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=POLICY_INIT_DIR,\n",
    "    learning_rate=1e-6,         # 너무 크면 망가집니다. (1e-6 ~ 5e-6 범위 탐색 권장)\n",
    "    batch_size=2,               # 한 번에 처리할 샘플 수\n",
    "    mini_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    ppo_epochs=4,\n",
    "    target_kl=0.1,              # KL 제어 (0.05~0.2 사이 튜닝)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    model=policy,\n",
    "    ref_model=ref_model,\n",
    "    dataset=ppo_ds,            # \"query\" 컬럼 사용\n",
    "    data_collator=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43d7b6-120f-4116-aeb2-028c6a518b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_new_tokens = 128\n",
    "steps = len(ppo_trainer.dataset)  # 전량 학습은 오래 걸릴 수 있음 → 일부만 돌리려면 min(steps, N)\n",
    "\n",
    "for step, batch in zip(trange(steps), ppo_trainer.dataloader):\n",
    "    queries = batch[\"query\"]\n",
    "\n",
    "    # 1) 응답 생성\n",
    "    q_tok = tokenizer(list(queries), return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(ppo_trainer.model.pretrained_model.device)\n",
    "    with torch.no_grad():\n",
    "        resp_ids = ppo_trainer.model.generate(\n",
    "            **q_tok,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, top_p=0.9, temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # 응답 텍스트 추출\n",
    "    responses = []\n",
    "    for q, ids in zip(queries, resp_ids):\n",
    "        text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        # 프롬프트 포함 시 잘라내기(간단 버전)\n",
    "        cut = text.find(q)\n",
    "        gen = text[cut+len(q):].strip() if cut >= 0 else text.strip()\n",
    "        responses.append(gen)\n",
    "\n",
    "    # 2) RM 보상\n",
    "    rewards = get_reward(queries, responses)  # tensor on CPU\n",
    "\n",
    "    # 3) PPO 스텝\n",
    "    q_toks = q_tok[\"input_ids\"]\n",
    "    r_toks = tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(q_toks.device)[\"input_ids\"]\n",
    "    stats = ppo_trainer.step(q_toks, r_toks, torch.tensor(rewards, device=q_toks.device, dtype=torch.float))\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # 주기적으로 저장\n",
    "    if (step + 1) % 200 == 0:\n",
    "        ppo_trainer.save_pretrained(PPO_OUT_DIR)\n",
    "\n",
    "# 최종 저장\n",
    "ppo_trainer.save_pretrained(PPO_OUT_DIR)\n",
    "tokenizer.save_pretrained(PPO_OUT_DIR)\n",
    "print(\"PPO 학습/저장 완료:\", PPO_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2998637-9c1c-4f36-88fe-23ac6777b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT(초기 정책) 로드\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(POLICY_INIT_DIR, device_map=\"auto\")\n",
    "sft_model.eval()\n",
    "\n",
    "# PPO 결과 로드 (ValueHead 붙은 형태)\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(PPO_OUT_DIR, device_map=\"auto\")\n",
    "ppo_model.eval()\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=128):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inp, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.9, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "test_prompt = \"인공지능이 한국 사회에 미치는 긍정적/부정적 영향을 각각 요약해줘.\"\n",
    "print(\"=== SFT 출력 ===\")\n",
    "print(generate(sft_model, test_prompt))\n",
    "\n",
    "print(\"\\n=== PPO 출력 ===\")\n",
    "print(generate(ppo_model.pretrained_model, test_prompt))  # ValueHead 래핑 내부의 LM 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85cd98-6778-4ec1-aba5-e782cf16d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Metric 불러오기\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# 2. 예시 데이터\n",
    "predictions = [\n",
    "    \"인공지능은 사회 전반에 큰 변화를 일으킨다.\",\n",
    "    \"AI는 사회에 새로운 기회를 제공한다.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\"인공지능은 사회에 큰 영향을 끼친다.\"],\n",
    "    [\"AI는 새로운 기회를 창출한다.\"]\n",
    "]\n",
    "\n",
    "# 3. BLEU\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# 4. ROUGE\n",
    "rouge_result = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "\n",
    "# 5. BERTScore (한국어 모델 사용 가능 → 'microsoft/deberta-xlarge-mnli' 또는 'klue/bert-base')\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=[r[0] for r in references], lang=\"ko\")\n",
    "\n",
    "print(\"=== BLEU ===\")\n",
    "print(bleu_result)\n",
    "\n",
    "print(\"\\n=== ROUGE ===\")\n",
    "print(rouge_result)\n",
    "\n",
    "print(\"\\n=== BERTScore (F1 평균) ===\")\n",
    "print(sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3bf10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418d88d",
   "metadata": {},
   "source": [
    "### 전략 3: Foundation Model 교체 / 학습 전략 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47412f",
   "metadata": {},
   "source": [
    "#### 3.1. 모델 교체 및 최적화\n",
    "`skt/ko-gpt-trinity-1.2B-v0.5` 모델로 교체하거나, SFT, RM, PPO 학습 전략을 변경/개선합니다. OOM 문제 해결을 위해 허깅페이스의 training arguments를 조합하여 최적의 하이퍼파라미터를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a52a65-b156-478f-bc1c-caa04cacf2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate bitsandbytes peft datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd2ae20-08ca-45e5-a062-833d6ca25bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec277b98-4406-40af-91a8-d3f0b829cf57",
   "metadata": {},
   "source": [
    "### 1. SFT 데이터셋 전처리 / 토크나이징 / 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715106ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bead5a14834d4691bb5e4d90fd72fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Base 모델 로드 (Polyglot-Ko 1.3B)\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b631442b-23c0-46ef-9e88-9153f69b8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba899363-9462-4e23-b5fe-4116e46b0956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a6d1e0f1024e4081a3aa48480e9158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"./KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl\"},\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c636c594-abbc-4791-b8b4-bdfc6cd7145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts = []\n",
    "    for p, c in zip(examples[\"prompt\"], examples[\"completion\"]):\n",
    "        prompt = f\"### Instruction:\\n{p}\\n\\n### Response:\\n{c}\"\n",
    "        texts.append(prompt)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(processed_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc19402b-7ab5-498a-9975-cd80265a0320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c123b72a5f4b10a558fa6a557661c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8aa166-1269-4c09-a031-7b64a145614b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 56:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.635100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.642800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.670100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.628300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.557100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.584200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.587600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.586800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.550900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=1.6376587905883788, metrics={'train_runtime': 3406.1748, 'train_samples_per_second': 3.523, 'train_steps_per_second': 1.762, 'total_flos': 4.6882873147392e+16, 'train_loss': 1.6376587905883788, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38a9f30c-09aa-4b6e-bd20-9ae00b8b9d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./polyglot-1.3b-lora-adapter/tokenizer_config.json',\n",
       " './polyglot-1.3b-lora-adapter/special_tokens_map.json',\n",
       " './polyglot-1.3b-lora-adapter/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어댑터 저장\n",
    "model.save_pretrained(\"./polyglot-1.3b-lora-adapter\")\n",
    "tokenizer.save_pretrained(\"./polyglot-1.3b-lora-adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59559baa-650f-49c6-83b9-ffd115b2e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT 모델 + LoRA 어댑터 저장 완료: ./results-strategy2-full-kogpt2-lora\n"
     ]
    }
   ],
   "source": [
    "# === SFT 학습 후 저장 ===\n",
    "OUTPUT_DIR = \"./results-strategy2-full-kogpt2-lora\"\n",
    "\n",
    "# Hugging Face Trainer로 학습 마친 후 ↓ 이 부분 추가\n",
    "model.save_pretrained(OUTPUT_DIR)      # LoRA 어댑터 저장 (adapter_config.json, adapter_model.bin 포함)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)  # 토크나이저도 같이 저장\n",
    "print(f\"SFT 모델 + LoRA 어댑터 저장 완료: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c99e6-d7d0-4f42-b28f-aa45970774fd",
   "metadata": {},
   "source": [
    "### 2. RM 학습 / 텍스트 합치기 / 토크나이징 / 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e144c7f1-165c-49c9-b1f2-65b498a8f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ID = \"EleutherAI/polyglot-ko-1.3b\"            # 베이스 모델\n",
    "SFT_ADAPTER_DIR = \"./polyglot-1.3b-lora-adapter\"   # 이미 학습/저장된 SFT LoRA 어댑터 경로\n",
    "RM_DIR = \"./rm-polyglot-1.3b\"                      # RM 저장 경로\n",
    "PPO_OUT_DIR = \"./ppo-polyglot-1.3b-lora\"           # PPO 결과 경로\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01bbeb32-3482-480d-add1-2d919a2d69f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a04a6ea96ba453db3ec0f87d34b1913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM columns: ['prompt', 'completion_0', 'completion_1', 'completion_2', 'ranking']\n",
      "Sample: {'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?', 'completion_0': 'Allow me to answer your question. I know that you are curious about me.', 'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.', 'completion_2': '라이언에게 말했다.', 'ranking': [2, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "rm_ds = load_dataset(\"json\", data_files={\"train\": \"./KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl\"}, split=\"train\")\n",
    "\n",
    "print(\"RM columns:\", rm_ds.column_names[:10])\n",
    "print(\"Sample:\", rm_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0aeef571-6a9c-4a2d-9ced-87c72d74f762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b69ec9f744458093f71c3a5f2e65db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?', 'completion_0': 'Allow me to answer your question. I know that you are curious about me.', 'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.', 'completion_2': '라이언에게 말했다.', 'ranking': [2, 1, 0], 'chosen': '라이언에게 말했다.', 'rejected': 'Allow me to answer your question. I know that you are curious about me.'}\n"
     ]
    }
   ],
   "source": [
    "# ranking 데이터셋에서 chosen / rejected 뽑기\n",
    "def convert_rm_format(example):\n",
    "    chosen_idx = example[\"ranking\"][0]      # 가장 선호\n",
    "    rejected_idx = example[\"ranking\"][-1]   # 가장 낮음\n",
    "    return {\n",
    "        \"prompt\": example[\"prompt\"],\n",
    "        \"chosen\": example[f\"completion_{chosen_idx}\"],\n",
    "        \"rejected\": example[f\"completion_{rejected_idx}\"]\n",
    "    }\n",
    "\n",
    "rm_ds_pairwise = rm_ds.map(convert_rm_format)\n",
    "print(rm_ds_pairwise[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67986f6b-bf0c-4940-a3b8-3f3cd1a0a416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2119cc5b9b90462caa063e963f30b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?', 'completion_0': 'Allow me to answer your question. I know that you are curious about me.', 'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.', 'completion_2': '라이언에게 말했다.', 'ranking': [2, 1, 0], 'chosen': '### Instruction:\\n번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?\\n\\n### Response:\\n라이언에게 말했다.', 'rejected': '### Instruction:\\n번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?\\n\\n### Response:\\nAllow me to answer your question. I know that you are curious about me.'}\n"
     ]
    }
   ],
   "source": [
    "# RewardTrainer는 chosen / rejected 텍스트만 있으면 충분\n",
    "def rm_format(example):\n",
    "    return {\n",
    "        \"chosen\": f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['chosen']}\",\n",
    "        \"rejected\": f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['rejected']}\"\n",
    "    }\n",
    "\n",
    "rm_ds_proc = rm_ds_pairwise.map(rm_format)\n",
    "print(rm_ds_proc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "476169a2-c22d-4e3c-b98f-89d437266974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: trl\n",
      "Version: 0.23.0\n",
      "Summary: Train transformer language models with reinforcement learning.\n",
      "Home-page: https://github.com/huggingface/trl\n",
      "Author: Leandro von Werra\n",
      "Author-email: leandro.vonwerra@gmail.com\n",
      "License: \n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: accelerate, datasets, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "787a5375-2fe9-415d-88e2-9d59314277b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,776 || all params: 125,976,576 || trainable%: 0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e427e8e9463d475ebbbbd92efe52c0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': None, 'pad_token_id': 2}.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2534' max='2534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2534/2534 27:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 기반 RM 학습/저장 완료: ./reward_model_lora\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "BASE_ID = \"skt/kogpt2-base-v2\"\n",
    "RM_DIR = \"./reward_model_lora\"\n",
    "\n",
    "# Reward Model (Classification Head 달기)\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_ID,\n",
    "    num_labels=1,        \n",
    "    device_map=\"auto\"     \n",
    ")\n",
    "\n",
    "# LoRA 어댑터 붙이기\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT2/KoGPT2 attention 구조\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"   # Reward Model → Classification task\n",
    ")\n",
    "rm_model = get_peft_model(rm_model, lora_config)\n",
    "rm_model.print_trainable_parameters()\n",
    "\n",
    "# 학습 설정\n",
    "rm_args = RewardConfig(\n",
    "    output_dir=RM_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,    \n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# RewardTrainer (trl 0.23.0)\n",
    "rm_trainer = RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=rm_args,\n",
    "    processing_class=tokenizer,  # tokenizer 대신 processing_class로 전달\n",
    "    train_dataset=rm_ds_proc,    # convert_rm_format → rm_format 거친 데이터\n",
    "    eval_dataset=None\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "rm_trainer.train()\n",
    "\n",
    "# 저장\n",
    "rm_trainer.save_model(RM_DIR)\n",
    "tokenizer.save_pretrained(RM_DIR)\n",
    "print(\"LoRA 기반 RM 학습/저장 완료:\", RM_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742544ef-6c93-4797-b518-c214ad57a2ee",
   "metadata": {},
   "source": [
    "### 3. PPO 데이터 로드 / 학습 / 결과 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39ef9822-2044-4f0e-9e42-9f34d1a6821e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'query_key_value'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m base = AutoModelForCausalLM.from_pretrained(BASE_ID, device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m policy_lm = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSFT_ADAPTER_DIR\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# SFT 어댑터 로드\u001b[39;00m\n\u001b[32m     19\u001b[39m policy_lm.eval()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Value Head를 부착한 정책 모델 생성\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:547\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    540\u001b[39m         model,\n\u001b[32m    541\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    545\u001b[39m     )\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     model = \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m load_result = model.load_adapter(\n\u001b[32m    556\u001b[39m     model_id,\n\u001b[32m    557\u001b[39m     adapter_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     **kwargs,\n\u001b[32m    563\u001b[39m )\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:1815\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1814\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1816\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:130\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    128\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    134\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:209\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:654\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    653\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[32m    657\u001b[39m     error_msg = (\n\u001b[32m    658\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo modules were targeted for adaptation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check your `target_modules` and `exclude_modules` configuration. You may also have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly targeted modules that are marked to be saved (`modules_to_save`).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    662\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Target modules {'query_key_value'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model\n",
    "\n",
    "# ===== 기본 설정 =====\n",
    "BASE_ID = \"skt/kogpt2-base-v2\"\n",
    "SFT_ADAPTER_DIR = \"./results-strategy2-full-kogpt2-lora\"\n",
    "RM_DIR = \"./reward_model_lora\"                              # 앞서 학습 완료한 RM (LoRA 학습본 또는 FT본)\n",
    "PPO_OUT_DIR = \"./ppo_policy_out\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ===== 정책 모델: full precision + SFT LoRA 장착 → ValueHead 부착 =====\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_ID, device_map=\"auto\")\n",
    "from peft import PeftModel\n",
    "policy_lm = PeftModel.from_pretrained(base, SFT_ADAPTER_DIR)  # SFT 어댑터 로드\n",
    "policy_lm.eval()\n",
    "\n",
    "# Value Head를 부착한 정책 모델 생성\n",
    "policy = AutoModelForCausalLMWithValueHead.from_pretrained(policy_lm)\n",
    "policy.eval()\n",
    "\n",
    "# ===== 레퍼런스 모델(동일 구조 스냅샷; KL 기준) =====\n",
    "ref_model = create_reference_model(policy)  # policy의 스냅샷을 참조모델로\n",
    "ref_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2dd78a-ed88-4ef5-bec1-a164c4de3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PPO 프롬프트 데이터 =====\n",
    "ppo_ds = load_dataset(\"json\", data_files={\"train\": \"/mnt/data/kochatgpt_3_PPO.jsonl\"}, split=\"train\")\n",
    "PPO_PROMPT_KEYS = [\"prompt\", \"instruction\", \"query\", \"input\", \"question\"]\n",
    "def pick_first_key(keys, columns):\n",
    "    for k in keys:\n",
    "        if k in columns: return k\n",
    "    return None\n",
    "pkey = pick_first_key(PPO_PROMPT_KEYS, ppo_ds.column_names)\n",
    "assert pkey, f\"PPO 프롬프트 컬럼을 찾지 못했습니다. 후보={PPO_PROMPT_KEYS}, 실제={ppo_ds.column_names}\"\n",
    "\n",
    "def ppo_format(ex):\n",
    "    return {\"query\": ex[pkey]}\n",
    "ppo_ds_proc = ppo_ds.map(ppo_format, remove_columns=[c for c in ppo_ds.column_names if c != pkey])\n",
    "\n",
    "# ===== RM 스코어러 (평가만 하므로 양자화 유지해도 됨; 문제시 quantization_config 제거) =====\n",
    "try:\n",
    "    rm_scoring = AutoModelForSequenceClassification.from_pretrained(\n",
    "        RM_DIR, num_labels=1, device_map=\"auto\"  # quantization_config=bnb_config ← 필요시만 사용\n",
    "    )\n",
    "except Exception:\n",
    "    # bnb 문제가 있으면 full precision로 로드\n",
    "    rm_scoring = AutoModelForSequenceClassification.from_pretrained(\n",
    "        RM_DIR, num_labels=1, device_map=\"auto\"\n",
    "    )\n",
    "rm_scoring.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f2381-3db7-41c0-8ac2-2cff77b9e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(prompts, responses):\n",
    "    # prompts, responses: list[str]\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n{r}\" for p, r in zip(prompts, responses)]\n",
    "    toks = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(next(rm_scoring.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        out = rm_scoring(**toks).logits.squeeze(-1)  # (batch,)\n",
    "    return out.detach()\n",
    "\n",
    "# ===== PPO 설정 및 트레이너 =====\n",
    "config = PPOConfig(\n",
    "    model_name=BASE_ID,\n",
    "    learning_rate=1e-6,          # LoRA기반 SFT 위에서 미세 조정 → 작게 시작\n",
    "    batch_size=2,\n",
    "    mini_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    ppo_epochs=4,\n",
    "    target_kl=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    model=policy,\n",
    "    ref_model=ref_model,\n",
    "    dataset=ppo_ds_proc,   # \"query\" 필드 사용\n",
    "    data_collator=None\n",
    ")\n",
    "\n",
    "# ===== PPO 학습 루프 =====\n",
    "from tqdm import trange\n",
    "max_new_tokens = 128\n",
    "\n",
    "for step, batch in zip(trange(len(ppo_trainer.dataset)), ppo_trainer.dataloader):\n",
    "    queries = batch[\"query\"]\n",
    "\n",
    "    # 1) 응답 생성\n",
    "    q_tok = tokenizer(\n",
    "        list(queries), return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(ppo_trainer.accelerator.device)\n",
    "    with torch.no_grad():\n",
    "        resp_ids = policy.generate(\n",
    "            **q_tok,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, top_p=0.9, temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # 2) 응답 텍스트 분리(프롬프트 제거)\n",
    "    responses = []\n",
    "    for q, ids in zip(queries, resp_ids):\n",
    "        text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        cut = text.find(q)\n",
    "        gen = text[cut+len(q):].strip() if cut >= 0 else text.strip()\n",
    "        responses.append(gen)\n",
    "\n",
    "    # 3) RM 보상\n",
    "    rewards = get_reward(queries, responses).to(ppo_trainer.accelerator.device)\n",
    "\n",
    "    # 4) PPO 업데이트 (텐서 입력)\n",
    "    query_tensors = q_tok[\"input_ids\"]\n",
    "    response_tensors = tokenizer(\n",
    "        responses, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(ppo_trainer.accelerator.device)[\"input_ids\"]\n",
    "\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    if (step + 1) % 200 == 0:\n",
    "        ppo_trainer.save_pretrained(PPO_OUT_DIR)\n",
    "\n",
    "# 최종 저장\n",
    "ppo_trainer.save_pretrained(PPO_OUT_DIR)\n",
    "tokenizer.save_pretrained(PPO_OUT_DIR)\n",
    "print(\"PPO 학습/저장 완료:\", PPO_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887cfb2-fa4c-4774-87d8-8aae0ab5be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 추론 테스트 =====\n",
    "ppo_policy = AutoModelForCausalLMWithValueHead.from_pretrained(PPO_OUT_DIR)\n",
    "ppo_policy.eval()\n",
    "tok = AutoTokenizer.from_pretrained(PPO_OUT_DIR)\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "prompt = \"인공지능이 한국 사회에 미치는 긍정적/부정적 영향을 요약해줘.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(ppo_policy.pretrained_model.device)\n",
    "with torch.no_grad():\n",
    "    out_ids = ppo_policy.generate(\n",
    "        **inputs, max_new_tokens=200, do_sample=True, top_p=0.9, temperature=0.7,\n",
    "        pad_token_id=tok.eos_token_id\n",
    "    )\n",
    "print(tok.decode(out_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613d1a5-7ad6-4892-9b9e-560abbebce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_ds = load_dataset(\"json\", data_files={\"train\": \"./KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl\"}, split=\"train\")\n",
    "print(\"PPO columns:\", ppo_ds.column_names[:10])\n",
    "print(\"Sample:\", ppo_ds[0])\n",
    "\n",
    "PPO_PROMPT_KEYS = [\"prompt\", \"instruction\", \"query\", \"input\", \"question\"]\n",
    "ppo_p_key = pick_first_key(PPO_PROMPT_KEYS, ppo_ds.column_names)\n",
    "assert ppo_p_key, f\"PPO 프롬프트 컬럼을 찾지 못했습니다: 후보={PPO_PROMPT_KEYS}\"\n",
    "\n",
    "def ppo_format(example):\n",
    "    return {\"query\": example[ppo_p_key]}\n",
    "ppo_ds_proc = ppo_ds.map(ppo_format, remove_columns=[c for c in ppo_ds.column_names if c != ppo_p_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fcba2-70e6-41ea-b14a-b3752a339003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM 로드 (fp16/4bit 혼용 환경에서 eval만 수행)\n",
    "rm_scoring = AutoModelForSequenceClassification.from_pretrained(\n",
    "    RM_DIR, num_labels=1, quantization_config=bnb_config, device_map=\"auto\"\n",
    ")\n",
    "rm_scoring.eval()\n",
    "\n",
    "def get_reward(prompts, responses):\n",
    "    # prompts, responses: list[str]\n",
    "    texts = [f\"### Instruction:\\n{p}\\n\\n### Response:\\n{r}\" for p, r in zip(prompts, responses)]\n",
    "    toks = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(policy.device)\n",
    "    with torch.no_grad():\n",
    "        out = rm_scoring(**toks)\n",
    "        # shape: (batch, 1)\n",
    "        rewards = out.logits.squeeze(-1).detach()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52185468-596c-43de-b143-db24cd026cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=BASE_ID,\n",
    "    learning_rate=1e-6,\n",
    "    batch_size=2,                 # GPU 여건에 맞게 조절\n",
    "    mini_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    ppo_epochs=4,\n",
    "    kl_penalty=\"kl\",              # 기본 KL\n",
    "    target_kl=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    model=policy,\n",
    "    ref_model=ref_model,\n",
    "    dataset=ppo_ds_proc,          # HF Dataset 가능 (query 칼럼 사용)\n",
    "    data_collator=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309fbb3-eaa8-4665-b485-8ba545f0b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# 간단한 PPO 루프 (전량 학습 시 시간 오래 걸릴 수 있음)\n",
    "max_new_tokens = 128\n",
    "\n",
    "for step, batch in zip(trange(len(ppo_trainer.dataset)), ppo_trainer.dataloader):\n",
    "    queries = batch[\"query\"]\n",
    "    # 1) 응답 생성\n",
    "    query_toks = tokenizer(list(queries), return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(policy.device)\n",
    "    with torch.no_grad():\n",
    "        responses_toks = policy.generate(\n",
    "            **query_toks,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, top_p=0.9, temperature=0.7, pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    # 응답 텍스트 분리(프롬프트 부분 제외)\n",
    "    responses = []\n",
    "    for q, resp_ids in zip(queries, responses_toks):\n",
    "        text = tokenizer.decode(resp_ids, skip_special_tokens=True)\n",
    "        # 단순 분리(포맷에 맞춰 커스텀 가능)\n",
    "        # prompt가 text 앞부분에 포함되어 있다면 잘라내기\n",
    "        cut = text.find(q)\n",
    "        if cut >= 0:\n",
    "            gen = text[cut + len(q):].strip()\n",
    "        else:\n",
    "            gen = text.strip()\n",
    "        responses.append(gen)\n",
    "\n",
    "    # 2) RM으로 보상 계산\n",
    "    rewards = get_reward(queries, responses)\n",
    "\n",
    "    # 3) PPO 업데이트\n",
    "    # note: PPOTrainer는 텐서 입력을 기대\n",
    "    query_tensors = query_toks[\"input_ids\"]\n",
    "    response_tensors = tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(policy.device)[\"input_ids\"]\n",
    "\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # 샘플 저장 주기\n",
    "    if (step + 1) % 200 == 0:\n",
    "        ppo_trainer.save_pretrained(PPO_OUT_DIR)\n",
    "\n",
    "# 최종 저장\n",
    "ppo_trainer.save_pretrained(PPO_OUT_DIR)\n",
    "tokenizer.save_pretrained(PPO_OUT_DIR)\n",
    "print(\"PPO 학습/저장 완료:\", PPO_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e64aa-ffec-4540-8cc5-f8d8284291d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정책 모델 로드(LoRA+ValueHead 포함 저장본)\n",
    "ppo_policy = AutoModelForCausalLMWithValueHead.from_pretrained(PPO_OUT_DIR)\n",
    "ppo_policy.eval()\n",
    "tok = AutoTokenizer.from_pretrained(PPO_OUT_DIR)\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "prompt = \"인공지능이 한국 사회에 미치는 긍정적/부정적 영향을 요약해줘.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(ppo_policy.pretrained_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_ids = ppo_policy.generate(**inputs, max_new_tokens=200, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "print(tok.decode(out_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd297b",
   "metadata": {},
   "source": [
    "#### 3.2. Generator 함수 수정 및 인퍼런스\n",
    "> **[평가 기준 1.3]** 더 적절한 학습 전략(SFT, RM, PPO)을 적용하거나 initial model을 변경해 모델의 성능을 향상시켰는가?\n",
    "\n",
    "변경된 모델에 맞게 `generator` 함수를 수정하고, 모델 추론 결과를 제시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452df8-9f48-43f4-9e30-348a37373462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "def load_model(model_dir, base_id=\"EleutherAI/polyglot-ko-1.3b\"):\n",
    "    \"\"\"저장된 LoRA 어댑터 모델 불러오기\"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    # 베이스 모델\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # LoRA 어댑터 병합\n",
    "    model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generator(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    \"\"\"Generator 함수\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a072e4a-dcb7-4219-ac69-5a210573af0e",
   "metadata": {},
   "source": [
    "#### ✅ SFT 모델 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445d241-d89f-489b-861f-ba46cac616c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT 결과 모델 불러오기\n",
    "sft_model_dir = \"./polyglot-1.3b-lora-adapter\"\n",
    "model, tokenizer = load_model(sft_model_dir)\n",
    "\n",
    "prompt = \"불고기용 고기 한우에요?\"\n",
    "result = generator(model, tokenizer, prompt)\n",
    "\n",
    "print(\"=== [SFT 모델 출력] ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b888a-9e24-4c34-93b7-f2de63a7d25f",
   "metadata": {},
   "source": [
    "#### ✅ RM 모델 인퍼런스\n",
    "- 텍스트 평가용이므로 문장이 좋은지/나쁜지 점수화하는 과정을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfb77e-7488-47e1-816f-2a7842aec2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_model_dir = \"./rm-polyglot-1.3b\"\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    rm_model_dir, num_labels=1, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(rm_model_dir)\n",
    "\n",
    "prompt = \"불고기용 고기 한우에요?\"\n",
    "response = \"불고기용 고기는 한우뿐만 아니라 다양한 고기를 사용할 수 있습니다.\"\n",
    "\n",
    "inputs = tokenizer(f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{response}\",\n",
    "                   return_tensors=\"pt\", truncation=True, padding=True).to(rm_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = rm_model(**inputs).logits.item()\n",
    "\n",
    "print(\"=== [RM 점수] ===\")\n",
    "print(f\"응답 적합도 점수: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd43cd-33f6-490b-a221-39224f679e79",
   "metadata": {},
   "source": [
    "#### ✅ PPO 모델 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f29d6-a2e6-4760-a794-593a67ef97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 결과 모델 불러오기\n",
    "ppo_model_dir = \"./ppo-polyglot-1.3b-lora\"\n",
    "model, tokenizer = load_model(ppo_model_dir)\n",
    "\n",
    "prompt = \"인공지능이 한국 사회에 미치는 긍정적/부정적 영향을 요약해줘.\"\n",
    "result = generator(model, tokenizer, prompt)\n",
    "\n",
    "print(\"=== [PPO 모델 출력] ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4751cd9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c6d72",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "- 아직 이번 노드에 대해 완전히 이해하지 못한 것 같아 프로젝트를 하다 방향을 많이 잃었습니다. 다시 공부하고 프로젝트를 시작하니 더 오래 걸려서 프로젝트를 시간 내에 완전히 끝내지 못했습니다. \n",
    "- 이번 프로젝트는 실험 조건에 따라 파일을 나눠서 작업하는게 좋을 것 같다는 생각이 들었습니다. (모델들끼리 꼬이는 경우가 너무 많이 발생해서 CUDA 관련 오류도 많이 발생했고, 그냥 커널 자체가 꺼져버리는 문제가 많이 발생했었습니다.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
