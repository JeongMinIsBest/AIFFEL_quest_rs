{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c872522",
   "metadata": {},
   "source": [
    "1. 데이터셋 추가 및 통합 📚\n",
    "\n",
    "- KoAlpaca 데이터셋 로드 : 먼저 datasets 라이브러리를 사용해 공개된 한국어 데이터셋인 beomi/KoAlpaca-v1.1a를 불러옵니다.\n",
    "- 데이터 형식 맞추기 : KoAlpaca 데이터셋의 컬럼 이름('instruction', 'output')을 기존 KoChatGPT 데이터셋과 동일하게('prompt', 'completion') 변경합니다.\n",
    "- 데이터셋 통합 : 기존에 사용하던 kochatgpt_1_SFT.jsonl 데이터와 새로 불러온 KoAlpaca 데이터를 하나로 합쳐 combined_data라는 통합 데이터셋을 만듭니다. 이를 통해 모델이 더 다양하고 많은 양의 데이터를 학습할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7453d-eb09-4862-a6e2-b26d5eaff7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1. Ko-Alpaca 데이터셋 로드\n",
    "new_data = load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train\")\n",
    "\n",
    "# 2. 데이터 형식 변환 함수\n",
    "# Ko-Alpaca는 'instruction'과 'output'으로 구성되어 있으므로,\n",
    "# 이를 'prompt'와 'completion'으로 이름을 바꿔줍니다.\n",
    "def transform_ko_alpaca(example):\n",
    "    return {\n",
    "        'prompt': example['instruction'],\n",
    "        'completion': example['output']\n",
    "    }\n",
    "\n",
    "# 함수를 적용하여 컬럼 이름 변경\n",
    "new_data_transformed = new_data.map(transform_ko_alpaca, remove_columns=new_data.column_names)\n",
    "\n",
    "# 3. 기존 데이터셋 로드 (만약 로컬에 저장된 jsonl을 사용한다면)\n",
    "# 이전에 data 변수에 로드했다면 그 변수를 그대로 사용해도 됩니다.\n",
    "# 여기서는 다시 로드하는 코드를 예시로 보여드립니다.\n",
    "original_data = load_dataset(\"json\", data_files=\"KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl\", split=\"train\")\n",
    "\n",
    "\n",
    "# 4. 두 데이터셋 통합\n",
    "# concatenate_datasets 함수를 사용하여 두 데이터를 합칩니다.\n",
    "combined_data = concatenate_datasets([original_data, new_data_transformed])\n",
    "\n",
    "print(f\"--- 데이터셋 통합 완료 ---\")\n",
    "print(f\"원본 데이터셋 크기: {len(original_data)}\")\n",
    "print(f\"추가된 데이터셋 크기: {len(new_data_transformed)}\")\n",
    "print(f\"통합 후 전체 데이터셋 크기: {len(combined_data)}\")\n",
    "\n",
    "# 메모리 관리를 위해 사용하지 않는 변수는 삭제\n",
    "del new_data, new_data_transformed, original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e398bfe",
   "metadata": {},
   "source": [
    "2. 데이터 정제\n",
    "- 필터링 : 통합된 데이터셋(33,155개)에서 'prompt'나 'completion'의 길이가 5글자 이하인 짧은 데이터들을 제거하여, 최종적으로 32,906개의 데이터를 학습에 사용하도록 정제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16ca3ff-47b0-42ac-b437-c957e744a453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5db15c546614fa5ab4390c4bdfd672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/33155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 전 데이터셋 크기: 33155\n",
      "정제 후 데이터셋 크기: 32906\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 정제하는 함수입니다.\n",
    "def refine_dataset(example):\n",
    "    if example['prompt'] is None or example['completion'] is None:\n",
    "        return False\n",
    "    return len(example['prompt']) > 5 and len(example['completion']) > 5\n",
    "\n",
    "# 'combined_data'를 입력으로 사용합니다.\n",
    "refined_data = combined_data.filter(refine_dataset, num_proc=4)\n",
    "\n",
    "print(f\"정제 전 데이터셋 크기: {len(combined_data)}\")\n",
    "print(f\"정제 후 데이터셋 크기: {len(refined_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ddb04",
   "metadata": {},
   "source": [
    "3. 모델 학습 준비 및 실행 ⚙️\n",
    "\n",
    "- 모델 및 토크나이저 로드 : 첫 번째 실험과 동일하게 skt/kogpt2-base-v2 모델과 토크나이저를 준비합니다.\n",
    "- 데이터 전처리 : 통합되고 정제된 텍스트 데이터를 모델이 이해할 수 있는 숫자 형태(토큰)로 변환합니다. 이때 모델이 'completion'(응답) 부분만 학습하도록 'prompt'(지시) 부분은 레이블에서 제외(-100으로 마스킹)합니다.\n",
    "- 모델 학습 : Transformers 라이브러리의 Trainer를 사용해 추가된 데이터로 미세 조정(SFT) 학습을 진행하도록 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f9de1-67ef-4955-a5ac-b92bb69fd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoGPT2 모델과 토크나이저를 로드합니다.\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='</s>', eos_token='</s>', pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515689d-33e3-43d5-81f7-899c30d633b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 모델 입력 형식에 맞게 토크나이징하는 함수입니다.\n",
    "def preprocess(example):\n",
    "    # BOS(Beginning of Sentence)와 EOS(End of Sentence) 토큰을 추가합니다.\n",
    "    prompt = base_tokenizer.bos_token + example['prompt']\n",
    "    completion = example['completion'] + base_tokenizer.eos_token\n",
    "    \n",
    "    # 프롬프트와 정답을 합쳐 전체 텍스트를 생성합니다.\n",
    "    full_text = prompt + completion\n",
    "    \n",
    "    # 토크나이징을 수행합니다.\n",
    "    encodings = base_tokenizer(full_text, truncation=True, padding='max_length', max_length=256)\n",
    "    \n",
    "    # 레이블을 생성합니다. 프롬프트 부분은 loss 계산에서 제외하기 위해 -100으로 마스킹합니다.\n",
    "    labels = encodings['input_ids'][:]\n",
    "    prompt_tokens = len(base_tokenizer(prompt, truncation=True)['input_ids'])\n",
    "    labels[:prompt_tokens] = [-100] * prompt_tokens\n",
    "    \n",
    "    encodings['labels'] = labels\n",
    "    return encodings\n",
    "\n",
    "# 정제된 데이터셋에 전처리 함수를 적용합니다.\n",
    "processed_data = refined_data.map(preprocess, remove_columns=refined_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ff9e6-5856-448a-88a0-94877f09f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 인자들을 설정합니다.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft-output\",\n",
    "    num_train_epochs=3, # 시간 관계상 1 에폭만 설정, 실제로는 더 많은 학습 필요\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# SFT를 수행할 모델과 토크나이저를 복사하여 사용합니다.\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "\n",
    "# Trainer 객체를 생성합니다.\n",
    "trainer = Trainer(\n",
    "    model=sft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_data,\n",
    ")\n",
    "\n",
    "# 학습을 시작합니다.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb561be",
   "metadata": {},
   "source": [
    "4. 모델 성능 평가 및 비교 🔍\n",
    "\n",
    "- 정성적 평가 (사용자 입력 기반) : \n",
    "    - 사용자가 직접 프롬프트를 입력하면, 원본 KoGPT-2 모델과 새롭게 미세 조정한 SFT 모델이 각각 생성한 답변을 나란히 보여줍니다.\n",
    "    - 특히 SFT 모델은 Greedy, Beam Search, Top-k Sampling 등 다양한 생성 방식으로 답변을 만들어 성능을 다각도로 비교할 수 있게 합니다.\n",
    "\n",
    "- 정량적 평가 (ROUGE 점수) :\n",
    "    - 모델이 생성한 답변과 실제 정답이 얼마나 유사한지를 ROUGE 점수라는 지표로 측정합니다.\n",
    "    - ROUGE-1, ROUGE-2, ROUGE-L 점수를 통해 단어, 어구, 문장 구조의 유사도를 각각 수치로 평가하여 모델 성능을 객관적으로 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3862791-1b2d-462b-8707-827155d78a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변을 생성하는 함수\n",
    "def generate_answer(model, tokenizer, prompt, generation_type, max_length=128):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    output_sequences = None\n",
    "    if generation_type == \"greedy\":\n",
    "        output_sequences = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "    elif generation_type == \"beam_search\":\n",
    "        output_sequences = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    elif generation_type == \"top_k_sampling\":\n",
    "        output_sequences = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)\n",
    "\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "# 모델들을 평가 모드로 설정\n",
    "base_model.eval()\n",
    "sft_model.eval()\n",
    "\n",
    "# GPU가 사용 가능하다면 모델을 GPU로 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "sft_model.to(device)\n",
    "\n",
    "\n",
    "# 사용자 입력을 받아 답변 생성 및 비교\n",
    "while True:\n",
    "    prompt = input(\"프롬프트를 입력하세요 (종료하려면 'exit' 입력): \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"[입력 프롬프트]: {prompt}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # 원본 KoGPT-2 답변\n",
    "    base_answer = generate_answer(base_model, base_tokenizer, prompt, \"greedy\")\n",
    "    print(f\"[KoGPT-2 원본]: {base_answer}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    # SFT 모델 답변\n",
    "    sft_greedy = generate_answer(sft_model, sft_tokenizer, prompt, \"greedy\")\n",
    "    sft_beam = generate_answer(sft_model, sft_tokenizer, prompt, \"beam_search\")\n",
    "    sft_top_k = generate_answer(sft_model, sft_tokenizer, prompt, \"top_k_sampling\")\n",
    "    print(f\"[SFT - Greedy]: {sft_greedy}\")\n",
    "    print(f\"[SFT - Beam Search]: {sft_beam}\")\n",
    "    print(f\"[SFT - Top-k Sampling]: {sft_top_k}\")\n",
    "    print(\"--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ecb2f-31b5-4837-abce-ef2f734db732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE 평가 지표를 로드합니다.\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# 평가를 위한 데이터셋 일부를 샘플링합니다. (시간 단축을 위해 100개만 사용)\n",
    "eval_dataset = refined_data.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# SFT 모델의 예측과 실제 정답을 저장할 리스트\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for item in eval_dataset:\n",
    "    prompt = item['prompt']\n",
    "    reference = item['completion']\n",
    "    \n",
    "    # SFT 모델로 예측 생성\n",
    "    prediction = generate_answer(sft_model, sft_tokenizer, prompt, \"greedy\")\n",
    "    # 생성된 답변에서 프롬프트를 제거합니다 (순수 생성 부분만 남기기 위함).\n",
    "    if prediction.startswith(prompt):\n",
    "        prediction = prediction[len(prompt):]\n",
    "        \n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "    \n",
    "# ROUGE 점수 계산\n",
    "# nltk.sent_tokenize를 사용하여 문장 단위로 분리 후 계산합니다.\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references, tokenizer=lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"--- 정량적 평가 (ROUGE Score) ---\")\n",
    "print(f\"ROUGE-1: {rouge_results['rouge1'] * 100:.2f}\") # Unigrams (단일 단어) 기반 유사도\n",
    "print(f\"ROUGE-2: {rouge_results['rouge2'] * 100:.2f}\") # Bigrams (연속된 두 단어) 기반 유사도\n",
    "print(f\"ROUGE-L: {rouge_results['rougeL'] * 100:.2f}\") # Longest Common Subsequence (가장 긴 공통 부분 문자열) 기반 유사도"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
