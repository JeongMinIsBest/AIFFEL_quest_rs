{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee279c3-375c-457f-9d6c-dfb0a37af60f",
   "metadata": {},
   "source": [
    "# 트랜스포머로 만드는 대화형 챗봇 프로젝트 🌠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca2b82-eec8-47e3-9d40-f288310726df",
   "metadata": {},
   "source": [
    "## Transformer 대비 변경이 필요한 부분\n",
    "\n",
    "**1. 모델 구조**\n",
    "- 기존 프로젝트 : Encoder + Decoder 두 블록으로 구성되어 있습니다.\n",
    "- 변경 내역 : Decoder-only 구조로, Encoder 제거한 다음 Decoder 블록만 사용했습니다. Self-Attention에 미래 토큰 마스킹 추가도 진행했습니다.\n",
    "\n",
    "**2. 입력 포맷**\n",
    "- 기존 프로젝트 : \"USER:\", \"ASSISTANT:\" 접두사를 문자열 그대로 붙여 SentencePiece로 토큰화했습니다.\n",
    "- 변경 내역 : 특수 역할 토큰 [USR], [BOT], [DELIM]을 명시적으로 부여하여, 입력을 [BOS][USR] 질문 [DELIM][BOT] 답변 [DELIM] … [EOS] 형태로 구성했습니다.\n",
    "\n",
    "**3. 위치 정보(Positional Encoding)**\n",
    "- 기존 프로젝트 : Transformer 논문과 동일한 sinusoidal positional encoding 사용했습니다.\n",
    "- 변경 내역 : 학습 가능한 positional embedding(pos_emb)을 추가하여 토큰 임베딩과 합산했습니다.\n",
    "\n",
    "**4. 학습 목표**\n",
    "- 기존 프로젝트 : Encoder 입력(질문) → Decoder 출력(답변) 방식으로 sequence-to-sequence 학습이 목표였습니다.\n",
    "- 변경 내역 : 입력 전체 시퀀스에서 다음 토큰을 예측하도록 변경했습니다.\n",
    "\n",
    "**5. 손실 계산 방식**\n",
    "- 기존 프로젝트 : Decoder 출력 전체에 대해 손실 계산했습니다.\n",
    "- 변경 내역 : 사용자 발화 부분은 마스킹하여 손실에서 제외하고, 답변 토큰만 손실 계산하도록 했습니다.\n",
    "\n",
    "**6. 출력 생성**\n",
    "- 기존 프로젝트 : Greedy Search 또는 단순 샘플링 기반입니다.\n",
    "- 변경 내역 : 샘플링 강화 후에 temperature, top-p(nucleus sampling), repetition penalty, stop tokens을 적용했습니다.\n",
    "\n",
    "**7. 파라미터 공유**\n",
    "- 기존 프로젝트 : 입력 임베딩과 출력 레이어(Softmax)가 별개였습니다.\n",
    "- 변경 내역 : 임베딩 ↔ LM Head weight tying 적용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca601ea-1827-4f15-8168-722000337741",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35529d33-2c74-4d3e-9bb4-7e9c42f42bde",
   "metadata": {},
   "source": [
    "- ```SentencePiece``` : 텍스트 토큰화 라이브러리로, 특히 한국어와 일본어 같이 띄어쓰기 구분이 애매한 언어에서 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e16eefe-dd9e-421f-a6c5-4edf07f58c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b102366c-9f45-46a2-bd0e-eaaa86b1fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 임포트\n",
    "\n",
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ba4235-85df-40c0-9260-0f465b9dccf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU를 사용할 수 있다면?\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d62ef2-e95e-47da-a8c4-0b688fed4d27",
   "metadata": {},
   "source": [
    "### 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5a20e-01e2-4c9d-b030-98870a59cf05",
   "metadata": {},
   "source": [
    "[입력 구성 방식]\n",
    "\n",
    "1. 기존 프로젝트\n",
    "- CSV의 Q, A를 불러와서 각각 Encoder 입력(질문), Decoder 목표(답변)으로 분리했습니다.\n",
    "- 전처리 단계에서 질문 시퀀스, 답변 시퀀스를 따로 만들어서 DataLoader에 넣었습니다.\n",
    "- 입력 포맷은 \"USER:\" + Q, \"ASSISTANT:\" + A 같은 문자열 접두사 기반이었습니다.\n",
    "- SentencePiece로 subword 단위 토큰화 → Encoder/Decoder 각각 padding을 적용했습니다.\n",
    "\n",
    "2. 변경 내역\n",
    "- Encoder가 없으므로 Q, A를 하나의 시퀀스로 이어 붙였습니다.\n",
    "- ```[BOS][USR] Q [DELIM][BOT] A [DELIM] … [EOS]``` 형태로 구성됩니다.\n",
    "- ```DialogDataset``` 클래스에서 turn 단위로 묶어서 block size 기준으로 슬라이싱합니다.\n",
    "- x와 y는 단일 시퀀스에서 shifted LM 방식으로 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c94e9f9-2c91-4a10-a8fc-da86531d81e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/0818'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8035b399-c132-4eab-99f7-04ca88aacd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 경로 지정\n",
    "CSV_PATH = \"./data/ChatbotData.csv\"\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# 데이터 프레임에 Q/A가 들어와 확인해주는 용도\n",
    "assert \"Q\" in df.columns and \"A\" in df.columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73adedba-180a-472b-b526-6cb369f764eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11750, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q            A  label\n",
       "0        12시 땡!   하루가 또 가네요.      0\n",
       "1   1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2  3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본 전처리 - 결측치와 중복값 제거\n",
    "\n",
    "df = df.dropna(subset=[\"Q\",\"A\"]).copy()\n",
    "df[\"Q\"] = df[\"Q\"].astype(str).str.strip()\n",
    "df[\"A\"] = df[\"A\"].astype(str).str.strip()\n",
    "df = df.drop_duplicates(subset=[\"Q\",\"A\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cab8390-9558-4151-b077-19bf1eaea1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5581, 294)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train / valid 분할\n",
    "train_df, val_df = train_test_split(df, test_size=0.05, random_state=42, shuffle=True)\n",
    "\n",
    "# 대화를 묶는 함수 - [(질문, 답변)] 형태로 반환\n",
    "def df_to_dialogs(df, turns_per_dialog=2):\n",
    "    pairs = list(zip(df[\"Q\"].tolist(), df[\"A\"].tolist()))\n",
    "    dialogs, buf = [], []\n",
    "    for q,a in pairs:\n",
    "        buf.append((q,a))\n",
    "        if len(buf) == turns_per_dialog:\n",
    "            dialogs.append(buf); buf = []\n",
    "    if buf: dialogs.append(buf)\n",
    "    return dialogs\n",
    "\n",
    "TURNS_PER_DIALOG = 2\n",
    "dialogs_train = df_to_dialogs(train_df, TURNS_PER_DIALOG)\n",
    "dialogs_val   = df_to_dialogs(val_df,   TURNS_PER_DIALOG)\n",
    "len(dialogs_train), len(dialogs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f6be2-63c1-497a-a75b-0506a6553589",
   "metadata": {},
   "source": [
    "[특수 토큰 처리]\n",
    "\n",
    "1. 기존 프로젝트\n",
    "- USER:, ASSISTANT:는 그냥 문자열이라 SentencePiece에서 여러 subword로 분리했었습니다. (학습 시 의미 혼동 발생 가능성이 있다고 합니다..)\n",
    "- 위치 구분은 문자열 패턴에 의존하는 방식이었습니다.\n",
    "\n",
    "2. 변경 내역\n",
    "- [USR], [BOT], [DELIM]을 단일 토큰으로 추가했습니다.\n",
    "- 역할과 턴 경계를 명확하게 하여 모델이 학습하기 쉬워집니다.\n",
    "- 디코딩 시에도 이 토큰들을 제거하고 자연스러운 문장만 출력하도록 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5082570a-376d-4262-a03b-e6201905c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SentencePiece tokenizer, vocab=4003\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def build_tokenizer(spm_path=\"spm_ko.model\", prefer_sp=True):\n",
    "    if prefer_sp and os.path.exists(spm_path):\n",
    "        # SentencePiece 가져오기\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(spm_path)\n",
    "        sp_vocab = sp.get_piece_size()\n",
    "\n",
    "        # SentencePiece에 특수 토큰이 없다면 추가 ID를 뒤쪽에 할당\n",
    "        PAD = sp.pad_id() if sp.pad_id() >= 0 else sp_vocab; sp_vocab += (sp.pad_id()<0)\n",
    "        UNK = sp.unk_id() if sp.unk_id() >= 0 else sp_vocab; sp_vocab += (sp.unk_id()<0)\n",
    "        BOS = sp.bos_id() if sp.bos_id() >= 0 else sp_vocab; sp_vocab += (sp.bos_id()<0)\n",
    "        EOS = sp.eos_id() if sp.eos_id() >= 0 else sp_vocab; sp_vocab += (sp.eos_id()<0)\n",
    "\n",
    "        # 턴 구분자 처리\n",
    "        DELIM = sp.piece_to_id(\"<DELIM>\") if \"<DELIM>\" in [sp.id_to_piece(i) for i in range(sp.get_piece_size())] else sp_vocab; sp_vocab += (\"<DELIM>\" not in [sp.id_to_piece(i) for i in range(sp.get_piece_size())])\n",
    "\n",
    "        # 역할 토큰 경계 명확히 하기 (짤리지 않게)\n",
    "        USR, BOT = sp_vocab, sp_vocab+1\n",
    "        sp_vocab += 2\n",
    "\n",
    "        class SPTokenizer:\n",
    "            def __init__(self, sp):\n",
    "                self.sp=sp; self.PAD=PAD; self.UNK=UNK; self.BOS=BOS; self.EOS=EOS\n",
    "                self.DELIM=DELIM; self.USR=USR; self.BOT=BOT; self.vocab_size=sp_vocab\n",
    "            def encode_text(self,s,add_special=True):\n",
    "                ids=self.sp.encode(s,out_type=int)\n",
    "                return [self.BOS]+ids+[self.EOS] if add_special else ids\n",
    "            def join_with_delim(self,parts):\n",
    "                seq=[]\n",
    "                for i,p in enumerate(parts):\n",
    "                    if i>0: seq.append(self.DELIM)\n",
    "                    seq.extend(p)\n",
    "                return seq\n",
    "            def decode(self,ids):\n",
    "                drop={self.PAD,self.UNK,self.BOS,self.EOS,self.DELIM,self.USR,self.BOT}\n",
    "                keep=[i for i in ids if (i not in drop)]\n",
    "                return self.sp.decode(keep)\n",
    "        print(f\"Loaded SentencePiece tokenizer, vocab={sp_vocab}\")\n",
    "        return SPTokenizer(sp)\n",
    "\n",
    "tok = build_tokenizer(\"spm_ko.model\", prefer_sp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106a2bf-4482-476e-94f1-c8b12eff7eb9",
   "metadata": {},
   "source": [
    "[손실 계산용 마스크]\n",
    "\n",
    "1. 기존 프로젝트\n",
    "- Decoder의 모든 토큰(질문 포함)에 대해 손실을 계산했었습니다.\n",
    "- Q와 A를 명확히 분리했었기에 딱히 문제가 없었습니다.\n",
    "\n",
    "2. 변경 내역\n",
    "- 단일 시퀀스 안에서 사용자 발화 부분은 마스킹하여 loss에서 제외했습니다.\n",
    "- 답변 토큰 예측에만 집중할 수 있도록 ```mask_user_loss=True``` 옵션을 추가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35980b47-2377-4027-a277-4c2afafdcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    입력 시퀀스 구성 :\n",
    "    [BOS] [USR] user_ids [DELIM] [BOT] answer_ids [DELIM] ... [EOS]\n",
    "    \n",
    "    학습 타깃(y)은 x를 한 칸 오른쪽으로 민 형태(next-token 예측).\n",
    "    mask_user_loss=True이면 사용자 구간은 loss에서 제외함(PAD로 덮어써 ignore_index 처리).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dialogs, tokenizer, block_size:int, mask_user_loss=True):\n",
    "        self.tok=tokenizer; self.block_size=block_size; self.mask_user_loss=mask_user_loss\n",
    "        self.samples=[]; self.masks=[]\n",
    "        for dialog in dialogs:\n",
    "            ids=[self.tok.BOS]; keep=[False]\n",
    "            for (u,a) in dialog:\n",
    "                u_ids=self.tok.encode_text(u,add_special=False)\n",
    "                a_ids=self.tok.encode_text(a,add_special=False)\n",
    "                ids+=[self.tok.USR]+u_ids+[self.tok.DELIM]+[self.tok.BOT]+a_ids+[self.tok.DELIM]\n",
    "                keep+=[False]+([False]*len(u_ids) if mask_user_loss else [True]*len(u_ids))+[False]+[True]+[True]*len(a_ids)+[False]\n",
    "            ids+=[self.tok.EOS]; keep+=[False]\n",
    "            for i in range(0,len(ids)-1,self.block_size):\n",
    "                chunk=ids[i:i+self.block_size+1]; mchunk=keep[i:i+self.block_size+1]\n",
    "                if len(chunk)<2: break\n",
    "                self.samples.append(chunk); self.masks.append(mchunk)\n",
    "                \n",
    "    def __len__(self): return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        seq=self.samples[idx]; m=self.masks[idx]\n",
    "        x=seq[:-1]; y=seq[1:]; m=m[1:]\n",
    "        if len(x)<self.block_size:\n",
    "            pad=self.block_size-len(x)\n",
    "            x+=[self.tok.PAD]*pad; y+=[self.tok.PAD]*pad; m+=[False]*pad\n",
    "        else:\n",
    "            x=x[:self.block_size]; y=y[:self.block_size]; m=m[:self.block_size]\n",
    "\n",
    "        # 손실 마스킹 : keep=False 위치의 타깃을 PAD로 덮어서 ignore_index에 걸리게 함\n",
    "        y=[yy if keep else self.tok.PAD for yy,keep in zip(y,m)]\n",
    "        return torch.tensor(x),torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f716b8b-45db-4dba-8e79-12f9c33fd2ed",
   "metadata": {},
   "source": [
    "[위치 정보 추가]\n",
    "\n",
    "1. 기존 프로젝트\n",
    "- sinusoidal positional encoding은 별도 전처리 없이 모델 forward에서 자동 생성했습니다.\n",
    "\n",
    "2. 변경 내역\n",
    "- 전처리 단계에서 시퀀스를 block_size 단위로 잘라주고, 모델 내부에서 learned positional embedding을 시퀀스 길이에 맞춰 자동으로 더했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16476a6e-1417-4c33-bc59-f376a1fffd76",
   "metadata": {},
   "source": [
    "### 3. GPT 모델 구성하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335210a-7a88-472b-a55c-8faafbb16300",
   "metadata": {},
   "source": [
    "[전체 구조]\n",
    "\n",
    "1. 기존 프로젝트\n",
    "- Encoder와 Decoder 두 블록으로 구성\n",
    "- Encoder는 입력 질문을 self-attention으로 인코딩, Decoder는 Encoder 출력을 cross-attention으로 참조하면서 답변을 생성\n",
    "\n",
    "2. 변경 내역\n",
    "- Decoder-only 구조 (Encoder 완전히 제거)\n",
    "- Self-Attention만 사용하며 미래 토큰을 가리는 causal mask 적용\n",
    "- Cross-attention이 없는 단일 LM 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739c10ac-1af5-4104-87e9-88a831a421ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0.0, resid_dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "        self.resid_drop = nn.Dropout(resid_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        q,k,v = qkv.chunk(3,dim=-1)\n",
    "        q = q.view(B,T,self.n_heads,self.d_head).transpose(1,2)\n",
    "        k = k.view(B,T,self.n_heads,self.d_head).transpose(1,2)\n",
    "        v = v.view(B,T,self.n_heads,self.d_head).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2,-1))/ (self.d_head**0.5)\n",
    "        mask = torch.ones(T,T,device=x.device).triu(1)\n",
    "        att = att.masked_fill(mask.bool(),float(\"-inf\")).softmax(-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        return self.resid_drop(self.proj(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "844e2b57-e1be-4ef0-a72d-b97fc37e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff,d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,d_ff,attn_dropout=0.0,resid_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1=nn.LayerNorm(d_model)\n",
    "        self.attn=CausalSelfAttention(d_model,n_heads,attn_dropout,resid_dropout)\n",
    "        self.ln2=nn.LayerNorm(d_model)\n",
    "        self.mlp=MLP(d_model,d_ff,resid_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d2b384-9373-4e06-879a-d19874e0b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1(nn.Module):\n",
    "    def __init__(self,vocab_size,block_size,n_layer=4,n_head=8,d_model=256,d_ff=1024,\n",
    "                 emb_dropout=0.1,resid_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.block_size=block_size\n",
    "        self.tok_emb=nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_emb=nn.Embedding(block_size,d_model)\n",
    "        self.drop=nn.Dropout(emb_dropout)\n",
    "        self.blocks=nn.ModuleList([Block(d_model,n_head,d_ff,0.1,resid_dropout) for _ in range(n_layer)])\n",
    "        self.ln_f=nn.LayerNorm(d_model)\n",
    "        self.lm_head=nn.Linear(d_model,vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.tok_emb.weight\n",
    "        self.apply(self._init)\n",
    "        \n",
    "    def _init(self,m):\n",
    "        if isinstance(m,(nn.Linear,nn.Embedding)):\n",
    "            nn.init.normal_(m.weight,mean=0.0,std=0.02)\n",
    "        if isinstance(m,nn.Linear) and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "            \n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T=idx.size()\n",
    "        assert T<=self.block_size\n",
    "        pos=torch.arange(T,device=idx.device).unsqueeze(0) # Positional Encoding\n",
    "        x=self.tok_emb(idx)+self.pos_emb(pos)\n",
    "        x=self.drop(x)\n",
    "        for blk in self.blocks: x=blk(x)\n",
    "        x=self.ln_f(x)\n",
    "        logits=self.lm_head(x)\n",
    "        loss=None\n",
    "        if targets is not None:\n",
    "            PAD=getattr(tok,\"PAD\",0)\n",
    "            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1),ignore_index=PAD)\n",
    "        return logits,loss\n",
    "        \n",
    "    def summary_text(self):\n",
    "        params=sum(p.numel() for p in self.parameters())\n",
    "        return (\n",
    "            f\"GPT1(vocab={self.vocab_size},block={self.block_size},layers={len(self.blocks)})\\\\n\"\n",
    "            f\"d_model={self.tok_emb.embedding_dim}, d_ff={self.blocks[0].mlp.net[0].out_features}\\\\n\"\n",
    "            f\"params={params:,}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6edeac7a-8890-466d-b413-98c86276e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HParams:\n",
    "    block_size:int=256      # 한 시퀀스 최대 토큰 길이\n",
    "    batch_size:int=32       # 학습 미니배치 크기\n",
    "    n_layer:int=8           # Transformer Block 개수 (깊이)\n",
    "    n_head:int=8            # Multi-Head Attention의 head 수\n",
    "    d_model:int=512         # 임베딩 차원 (토큰 벡터 차원)\n",
    "    d_ff:int=2048           # 피드포워드 레이어 차원 (확장된 차원)\n",
    "    emb_dropout:float=0.1   # 임베딩 입력 직후 dropout 비율\n",
    "    resid_dropout:float=0.1 # 블록 내 residual connection 뒤 dropout 비율\n",
    "    lr:float=2e-4           # AdamW 학습률\n",
    "    weight_decay:float=0.01 # AdamW 정규화 (가중치 감쇠)\n",
    "    epochs:int=15           # 학습 epoch 수\n",
    "    device:str=device       # \"cuda\" 또는 \"cpu\"\n",
    "    \n",
    "    def model_kwargs(self):\n",
    "        return dict(block_size=self.block_size,n_layer=self.n_layer,n_head=self.n_head,\n",
    "                    d_model=self.d_model,d_ff=self.d_ff,\n",
    "                    emb_dropout=self.emb_dropout,resid_dropout=self.resid_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45912d53-4c4b-4c8f-903f-17d3ecc54014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_repetition_penalty(logits, prev_ids, penalty=1.1):\n",
    "    if penalty==1.0 or prev_ids is None: return logits\n",
    "    uniq=torch.unique(prev_ids)\n",
    "    logits[:,uniq]/=penalty\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda5970-d1d1-40c0-a2fb-0fa8fd51f3ed",
   "metadata": {},
   "source": [
    "- ```generate``` : 추론 전용(no_grad) 생성 함수\n",
    "- ```idx``` : 현재까지의 토큰(배치 1×길이)\n",
    "- ```max_new_tokens``` : 새로 만들 최대 토큰 수\n",
    "- ```temperature/top_p``` : 샘플링 제어\n",
    "- ```repetition_penalty``` : 반복 억제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c49b03e-b753-4904-b3cf-072285f8eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens=64, temperature=0.8, top_p=0.9, repetition_penalty=1.1, stop_ids=None):\n",
    "    model.eval()\n",
    "    stop_ids=set(stop_ids or [])\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-model.block_size:]\n",
    "        logits,_=model(idx_cond)\n",
    "        logits=logits[:,-1,:]\n",
    "        logits=apply_repetition_penalty(logits,idx,penalty=repetition_penalty)\n",
    "        logits=logits/temperature\n",
    "        probs=torch.softmax(logits,dim=-1)\n",
    "        sorted_probs,sorted_idx=torch.sort(probs,descending=True)\n",
    "        cum=torch.cumsum(sorted_probs,dim=-1)\n",
    "        mask=cum>top_p; mask[...,0]=False\n",
    "        sorted_probs[mask]=0\n",
    "        sorted_probs/=sorted_probs.sum(-1,keepdim=True)\n",
    "        next_sorted=torch.multinomial(sorted_probs,1)\n",
    "        next_id=sorted_idx.gather(-1,next_sorted)\n",
    "        idx=torch.cat([idx,next_id],dim=1)\n",
    "        token=int(next_id.item())\n",
    "        if token in stop_ids or token==getattr(tok,\"EOS\",None):\n",
    "            break\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94f9ac-8ca5-4299-839c-30d5390b765e",
   "metadata": {},
   "source": [
    "### 4. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9568b9-54e7-4807-b67e-208555d4ee5d",
   "metadata": {},
   "source": [
    "[출력 생성]\n",
    "\n",
    "1. 기존 프로젝트\n",
    "- 주로 Greedy search를 사용했었습니다.\n",
    "\n",
    "2. 변경 내역\n",
    "- 샘플링 기반 생성(temperature, top-p, reptition penalty, stop_ids)을 적용했습니다.\n",
    "- 반복을 줄이고 다양한 응답을 생성할 수 있도록 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74314299-1681-4567-95c9-eeb526e27e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== model.summary ===\n",
      "GPT1(vocab=4003,block=256,layers=8)\\nd_model=512, d_ff=2048\\nparams=27,384,320\n",
      "Epoch 1/15 - 82.8s - loss:5.3557 - val:4.9030\n",
      "Epoch 2/15 - 82.5s - loss:4.7257 - val:4.5055\n",
      "Epoch 3/15 - 82.6s - loss:4.3352 - val:4.2298\n",
      "Epoch 4/15 - 82.7s - loss:4.0591 - val:4.0490\n",
      "Epoch 5/15 - 82.6s - loss:3.8397 - val:3.9303\n",
      "Epoch 6/15 - 82.6s - loss:3.6336 - val:3.8477\n",
      "Epoch 7/15 - 82.4s - loss:3.4323 - val:3.7742\n",
      "Epoch 8/15 - 82.5s - loss:3.2129 - val:3.7392\n",
      "Epoch 9/15 - 82.5s - loss:2.9880 - val:3.6595\n",
      "Epoch 10/15 - 82.6s - loss:2.7412 - val:3.6389\n",
      "Epoch 11/15 - 82.8s - loss:2.4942 - val:3.6096\n",
      "Epoch 12/15 - 82.8s - loss:2.2295 - val:3.6010\n",
      "Epoch 13/15 - 82.8s - loss:1.9649 - val:3.5976\n",
      "Epoch 14/15 - 82.8s - loss:1.6985 - val:3.5984\n",
      "Epoch 15/15 - 82.7s - loss:1.4400 - val:3.5836\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋/로더 준비\n",
    "h=HParams()\n",
    "model=GPT1(vocab_size=tok.vocab_size,**h.model_kwargs()).to(h.device)\n",
    "train_ds=DialogDataset(dialogs_train,tok,h.block_size,mask_user_loss=True)\n",
    "val_ds=DialogDataset(dialogs_val,tok,h.block_size,mask_user_loss=True)\n",
    "train_dl=torch.utils.data.DataLoader(train_ds,batch_size=h.batch_size,shuffle=True)\n",
    "val_dl=torch.utils.data.DataLoader(val_ds,batch_size=h.batch_size)\n",
    "\n",
    "print(\"=== model.summary ===\")\n",
    "print(model.summary_text())\n",
    "\n",
    "opt=torch.optim.AdamW(model.parameters(),lr=h.lr,weight_decay=h.weight_decay)\n",
    "\n",
    "for ep in range(1,h.epochs+1):\n",
    "    model.train(); tot=0;n=0;t0=time.time()\n",
    "    for xb,yb in train_dl:\n",
    "        xb,yb=xb.to(h.device),yb.to(h.device)\n",
    "        opt.zero_grad();_,loss=model(xb,yb)\n",
    "        loss.backward(); opt.step()\n",
    "        tot+=float(loss.item())*xb.size(0); n+=xb.size(0)\n",
    "    tr_loss=tot/max(1,n)\n",
    "    model.eval();vt=0;vn=0\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in val_dl:\n",
    "            xb,yb=xb.to(h.device),yb.to(h.device)\n",
    "            _,vloss=model(xb,yb)\n",
    "            vt+=float(vloss.item())*xb.size(0); vn+=xb.size(0)\n",
    "    print(f\"Epoch {ep}/{h.epochs} - {time.time()-t0:.1f}s - loss:{tr_loss:.4f} - val:{vt/max(1,vn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a694b7d-5af0-4f3c-9b9b-6a8a2a1fe9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 사랑이란 뭘까?\n",
      "A: 사랑이란 뭘까? 직접 물어보세요. 차분히 당당하게 말해보세요. 그렇지 않아요. 무엇이 같이 들어질게요. 고민만 있다면 당신을 생각해보세요. 충분히 그럴게요. 힘들었을거라 생각해요. 내세요. 신경쓰지 마세요. 자연스러합니다. 기다리세요. 후회해졌하는 건 없어요. 있어요. 이별이었을? 생각해 볼게요. 좋아하는 기억으로 아파하게 표현해보는 것도 중요한 것 같네 도와주는\n"
     ]
    }
   ],
   "source": [
    "# 생성 테스트\n",
    "q=val_df.iloc[0][\"Q\"]\n",
    "ids=[getattr(tok,\"BOS\",0),getattr(tok,\"USR\",1)]+tok.encode_text(q,add_special=False)+[getattr(tok,\"BOT\",2)]\n",
    "idx=torch.tensor([ids],dtype=torch.long,device=h.device)\n",
    "out=generate(model,idx,max_new_tokens=80,temperature=0.8,top_p=0.9,repetition_penalty=1.1,\n",
    "             stop_ids={getattr(tok,\"USR\",-1),getattr(tok,\"DELIM\",-1)})\n",
    "print(\"Q:\",q)\n",
    "print(\"A:\",tok.decode(out[0].tolist()).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c360a06-bbd6-41bf-b0c3-4afa529e89a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
