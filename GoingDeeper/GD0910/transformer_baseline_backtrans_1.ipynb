{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Ko-En) 번역기 Baseline\n",
    "---\n",
    "### 프로젝트 목표\n",
    "한국어-영어 번역을 위한 트랜스포머 모델의 베이스라인에서 데이터 증강을 추가\n",
    "\n",
    "**주요 변경 사항 (baseline 대비):**\n",
    "1. **데이터 증강 (Back-translation)** 추가:\n",
    "      - Meta AI의 NLLB-200 모델을 이용하여 한국어 문장을 영어로 번역한 뒤, 다시 한국어로 번역합니다.\n",
    "      - 이렇게 얻은 역번역 문장은 원래 문장과는 표현이 다르지만 의미는 동일합니다.\n",
    "      - 원본 한국어-영어 쌍 데이터에 이 역번역된 한국어 문장을 추가해, 모델이 더 다양한 표현을 학습하도록 합니다.\n",
    "      - 데이터셋 전체를 증강하여, 학습 데이터의 크기를 약 2배로 확장했습니다.\n",
    "\n",
    "2. **형태소 분석기** 적용 (MeCab):\n",
    "      - 한국어 문장을 MeCab 형태소 분석기로 전처리하여, 어절 단위가 아닌 형태소 단위로 토큰화합니다.\n",
    "      - 이를 통해 한국어 문법적 특성을 더 잘 반영하고 희소성을 줄여 모델 학습 효율을 높입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [nltk][32m1/2\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nltk-3.9.1 regex-2025.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이퍼파라미터 및 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer 논문(Attention is All You Need) 기본 구조와 유사한 설정\n",
    "    - d_model=512, n_heads=8, n_layers=6 → 표준 Transformer base 모델 크기\n",
    "    - d_ff=2048 → Position-wise FFN 내부 차원\n",
    "    - max_len=50 → 문장 최대 길이 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "SRC_VOCAB_SIZE = 20000\n",
    "TGT_VOCAB_SIZE = 20000\n",
    "D_MODEL = 512\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.dev.en\n",
      "korean-english-park.dev.ko\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ~/work/0908/data/korean-english-park.dev.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.test.en\n",
      "korean-english-park.test.ko\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ~/work/0908/data/korean-english-park.test.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.train.en\n",
      "korean-english-park.train.ko\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ~/work/0908/data/korean-english-park.train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 94123, Dev: 1000, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 경로 설정\n",
    "data_dir = 'data'\n",
    "train_kor_path = os.path.join(data_dir, 'korean-english-park.train.ko')\n",
    "train_eng_path = os.path.join(data_dir, 'korean-english-park.train.en')\n",
    "dev_kor_path = os.path.join(data_dir, 'korean-english-park.dev.ko')\n",
    "dev_eng_path = os.path.join(data_dir, 'korean-english-park.dev.en')\n",
    "test_kor_path = os.path.join(data_dir, 'korean-english-park.test.ko')\n",
    "test_eng_path = os.path.join(data_dir, 'korean-english-park.test.en')\n",
    "\n",
    "# 2. 원본 데이터 로딩\n",
    "with open(train_kor_path, \"r\", encoding='utf-8') as f: train_kor_raw = f.read().splitlines()\n",
    "with open(train_eng_path, \"r\", encoding='utf-8') as f: train_eng_raw = f.read().splitlines()\n",
    "with open(dev_kor_path, \"r\", encoding='utf-8') as f: dev_kor_raw = f.read().splitlines()\n",
    "with open(dev_eng_path, \"r\", encoding='utf-8') as f: dev_eng_raw = f.read().splitlines()\n",
    "with open(test_kor_path, \"r\", encoding='utf-8') as f: test_kor_raw = f.read().splitlines()\n",
    "with open(test_eng_path, \"r\", encoding='utf-8') as f: test_eng_raw = f.read().splitlines()\n",
    "\n",
    "print(f\"Train: {len(train_kor_raw)}, Dev: {len(dev_kor_raw)}, Test: {len(test_kor_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daabbbfde6ea4de4ba02d2d54676ed46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c603d8bbee467792f01a0b00149c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd6f65ba5fc43f1af09c85639464356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53c89c084c04ad1865fdcda36bef91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e248ebe92c14a18aa3336b1cf1d7b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adba2e2044dc43e9a9f10e66e1cb3c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc7020c6ca34884bd4578d98f2614dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb2eaca2d8d431db0e50d3e8fa05d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "데이터셋의 전체 94123개 문장에 대해 역번역을 시작합니다...\n",
      "총 94123개의 문장을 역번역합니다...\n",
      "[1000/94123] 문장 처리 완료 | 경과 시간: 325.02초\n",
      "[2000/94123] 문장 처리 완료 | 경과 시간: 674.90초\n",
      "[3000/94123] 문장 처리 완료 | 경과 시간: 1097.46초\n",
      "[4000/94123] 문장 처리 완료 | 경과 시간: 1481.03초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/94123] 문장 처리 완료 | 경과 시간: 1973.58초\n",
      "[6000/94123] 문장 처리 완료 | 경과 시간: 2386.35초\n",
      "[7000/94123] 문장 처리 완료 | 경과 시간: 2819.15초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000/94123] 문장 처리 완료 | 경과 시간: 3324.58초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9000/94123] 문장 처리 완료 | 경과 시간: 3753.74초\n",
      "[10000/94123] 문장 처리 완료 | 경과 시간: 4101.36초\n",
      "[11000/94123] 문장 처리 완료 | 경과 시간: 4501.02초\n",
      "[12000/94123] 문장 처리 완료 | 경과 시간: 4897.49초\n",
      "[13000/94123] 문장 처리 완료 | 경과 시간: 5330.96초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14000/94123] 문장 처리 완료 | 경과 시간: 5783.26초\n",
      "[15000/94123] 문장 처리 완료 | 경과 시간: 6145.43초\n",
      "[16000/94123] 문장 처리 완료 | 경과 시간: 6509.44초\n",
      "[17000/94123] 문장 처리 완료 | 경과 시간: 6877.69초\n",
      "[18000/94123] 문장 처리 완료 | 경과 시간: 7236.48초\n",
      "[19000/94123] 문장 처리 완료 | 경과 시간: 7614.49초\n",
      "[20000/94123] 문장 처리 완료 | 경과 시간: 7983.53초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21000/94123] 문장 처리 완료 | 경과 시간: 8377.96초\n",
      "[22000/94123] 문장 처리 완료 | 경과 시간: 8749.50초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23000/94123] 문장 처리 완료 | 경과 시간: 9147.06초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24000/94123] 문장 처리 완료 | 경과 시간: 9569.46초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25000/94123] 문장 처리 완료 | 경과 시간: 9990.16초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26000/94123] 문장 처리 완료 | 경과 시간: 10381.19초\n",
      "[27000/94123] 문장 처리 완료 | 경과 시간: 10738.14초\n",
      "[28000/94123] 문장 처리 완료 | 경과 시간: 11113.57초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29000/94123] 문장 처리 완료 | 경과 시간: 11518.81초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30000/94123] 문장 처리 완료 | 경과 시간: 11940.50초\n",
      "[31000/94123] 문장 처리 완료 | 경과 시간: 12308.98초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32000/94123] 문장 처리 완료 | 경과 시간: 12703.58초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33000/94123] 문장 처리 완료 | 경과 시간: 13087.45초\n",
      "[34000/94123] 문장 처리 완료 | 경과 시간: 13435.64초\n",
      "[35000/94123] 문장 처리 완료 | 경과 시간: 13791.99초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36000/94123] 문장 처리 완료 | 경과 시간: 14190.65초\n",
      "[37000/94123] 문장 처리 완료 | 경과 시간: 14579.24초\n",
      "[38000/94123] 문장 처리 완료 | 경과 시간: 14950.52초\n",
      "[39000/94123] 문장 처리 완료 | 경과 시간: 15375.85초\n",
      "[40000/94123] 문장 처리 완료 | 경과 시간: 15765.46초\n",
      "[41000/94123] 문장 처리 완료 | 경과 시간: 16089.52초\n",
      "[42000/94123] 문장 처리 완료 | 경과 시간: 16420.66초\n",
      "[43000/94123] 문장 처리 완료 | 경과 시간: 16766.35초\n",
      "[44000/94123] 문장 처리 완료 | 경과 시간: 17103.08초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45000/94123] 문장 처리 완료 | 경과 시간: 17509.75초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46000/94123] 문장 처리 완료 | 경과 시간: 17908.61초\n",
      "[47000/94123] 문장 처리 완료 | 경과 시간: 18309.71초\n",
      "[48000/94123] 문장 처리 완료 | 경과 시간: 18742.47초\n",
      "[49000/94123] 문장 처리 완료 | 경과 시간: 19129.90초\n",
      "[50000/94123] 문장 처리 완료 | 경과 시간: 19525.33초\n",
      "[51000/94123] 문장 처리 완료 | 경과 시간: 19925.96초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 190 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52000/94123] 문장 처리 완료 | 경과 시간: 20355.38초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 190 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53000/94123] 문장 처리 완료 | 경과 시간: 20817.42초\n",
      "[54000/94123] 문장 처리 완료 | 경과 시간: 21259.50초\n",
      "[55000/94123] 문장 처리 완료 | 경과 시간: 21623.46초\n",
      "[56000/94123] 문장 처리 완료 | 경과 시간: 21974.31초\n",
      "[57000/94123] 문장 처리 완료 | 경과 시간: 22371.02초\n",
      "[58000/94123] 문장 처리 완료 | 경과 시간: 22721.55초\n",
      "[59000/94123] 문장 처리 완료 | 경과 시간: 23167.53초\n",
      "[60000/94123] 문장 처리 완료 | 경과 시간: 23621.50초\n",
      "[61000/94123] 문장 처리 완료 | 경과 시간: 24040.80초\n",
      "[62000/94123] 문장 처리 완료 | 경과 시간: 24430.29초\n",
      "[63000/94123] 문장 처리 완료 | 경과 시간: 24825.06초\n",
      "[64000/94123] 문장 처리 완료 | 경과 시간: 25215.47초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65000/94123] 문장 처리 완료 | 경과 시간: 25736.67초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66000/94123] 문장 처리 완료 | 경과 시간: 26156.20초\n",
      "[67000/94123] 문장 처리 완료 | 경과 시간: 26541.11초\n",
      "[68000/94123] 문장 처리 완료 | 경과 시간: 26913.26초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69000/94123] 문장 처리 완료 | 경과 시간: 27409.73초\n",
      "[70000/94123] 문장 처리 완료 | 경과 시간: 27776.17초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71000/94123] 문장 처리 완료 | 경과 시간: 28147.71초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72000/94123] 문장 처리 완료 | 경과 시간: 28573.74초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73000/94123] 문장 처리 완료 | 경과 시간: 29057.49초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 190 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74000/94123] 문장 처리 완료 | 경과 시간: 29544.24초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75000/94123] 문장 처리 완료 | 경과 시간: 30037.20초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76000/94123] 문장 처리 완료 | 경과 시간: 30515.62초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77000/94123] 문장 처리 완료 | 경과 시간: 31030.45초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78000/94123] 문장 처리 완료 | 경과 시간: 31490.22초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79000/94123] 문장 처리 완료 | 경과 시간: 31937.30초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80000/94123] 문장 처리 완료 | 경과 시간: 32455.31초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m train_eng_subset = train_eng_raw[:num_to_augment]\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m데이터셋의 전체 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_to_augment\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m개 문장에 대해 역번역을 시작합니다...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m augmented_train_kor_raw = \u001b[43mback_translate_nllb_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_kor_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m데이터 증강 완료\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 원본 데이터와 증강된 데이터를 합치기\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mback_translate_nllb_optimized\u001b[39m\u001b[34m(sentences, chunk_size, batch_size)\u001b[39m\n\u001b[32m     21\u001b[39m chunk_end = \u001b[38;5;28mmin\u001b[39m(i + chunk_size, total_sentences)\n\u001b[32m     23\u001b[39m translated_to_en = [\n\u001b[32m     24\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mtranslation_text\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m translator(sentences[chunk_start:chunk_end],\n\u001b[32m     25\u001b[39m                                                        src_lang=\u001b[33m'\u001b[39m\u001b[33mkor_Hang\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m                                                        tgt_lang=\u001b[33m'\u001b[39m\u001b[33meng_Latn\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m                                                        batch_size=batch_size)\n\u001b[32m     28\u001b[39m ]\n\u001b[32m     30\u001b[39m back_translated_to_ko = [\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mtranslation_text\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslated_to_en\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meng_Latn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkor_Hang\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m ]\n\u001b[32m     37\u001b[39m augmented_sentences.extend(back_translated_to_ko)\n\u001b[32m     39\u001b[39m elapsed_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:410\u001b[39m, in \u001b[36mTranslationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    381\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[33;03m    Translate the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    383\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    408\u001b[39m \u001b[33;03m          token ids of the translation.\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]], **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    165\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    193\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    194\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    195\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    196\u001b[39m     ):\n\u001b[32m    197\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/base.py:1448\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1445\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1446\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1447\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:127\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    126\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:220\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    218\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:2551\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2539\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2540\u001b[39m         input_ids,\n\u001b[32m   2541\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2546\u001b[39m         **model_kwargs,\n\u001b[32m   2547\u001b[39m     )\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2556\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2561\u001b[39m     logger.warning_once(\n\u001b[32m   2562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2563\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2564\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:3341\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3338\u001b[39m beam_indices = running_beam_indices.detach().clone()\n\u001b[32m   3340\u001b[39m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   3342\u001b[39m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[32m   3343\u001b[39m     flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3344\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:2662\u001b[39m, in \u001b[36mGenerationMixin._has_unfinished_sequences\u001b[39m\u001b[34m(self, this_peer_finished, synced_gpus, device)\u001b[39m\n\u001b[32m   2659\u001b[39m         result.past_key_values = result.past_key_values.to_legacy_cache()\n\u001b[32m   2660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2662\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch.device) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   2663\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2664\u001b[39m \u001b[33;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[32m   2665\u001b[39m \u001b[33;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[32m   2666\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[32m   2668\u001b[39m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[32m   2669\u001b[39m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# (이전에 정의한 back_translate_nllb_optimized 함수를 그대로 사용)\n",
    "def back_translate_nllb_optimized(sentences, chunk_size=1000, batch_size=32):\n",
    "    \"\"\"\n",
    "    역번역을 진행하면서 진행 상황을 출력하고, 효율성을 높이는 함수.\n",
    "    \"\"\"\n",
    "    augmented_sentences = []\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    print(f\"총 {total_sentences}개의 문장을 역번역합니다...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, total_sentences, chunk_size):\n",
    "        chunk_start = i\n",
    "        chunk_end = min(i + chunk_size, total_sentences)\n",
    "\n",
    "        translated_to_en = [\n",
    "            result['translation_text'] for result in translator(sentences[chunk_start:chunk_end],\n",
    "                                                               src_lang='kor_Hang',\n",
    "                                                               tgt_lang='eng_Latn',\n",
    "                                                               batch_size=batch_size)\n",
    "        ]\n",
    "\n",
    "        back_translated_to_ko = [\n",
    "            result['translation_text'] for result in translator(translated_to_en,\n",
    "                                                               src_lang='eng_Latn',\n",
    "                                                               tgt_lang='kor_Hang',\n",
    "                                                               batch_size=batch_size)\n",
    "        ]\n",
    "\n",
    "        augmented_sentences.extend(back_translated_to_ko)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[{chunk_end}/{total_sentences}] 문장 처리 완료 | 경과 시간: {elapsed_time:.2f}초\")\n",
    "\n",
    "    return augmented_sentences\n",
    "\n",
    "# NLLB 모델 로드\n",
    "translator = pipeline(\n",
    "    'translation',\n",
    "    model='facebook/nllb-200-distilled-600M',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# 데이터 증강을 적용할 데이터의 양을 결정\n",
    "num_to_augment = len(train_kor_raw)\n",
    "\n",
    "train_kor_subset = train_kor_raw[:num_to_augment]\n",
    "train_eng_subset = train_eng_raw[:num_to_augment]\n",
    "\n",
    "print(f\"\\n데이터셋의 전체 {num_to_augment}개 문장에 대해 역번역을 시작합니다...\")\n",
    "augmented_train_kor_raw = back_translate_nllb_optimized(train_kor_subset)\n",
    "print(\"데이터 증강 완료\")\n",
    "\n",
    "# 원본 데이터와 증강된 데이터를 합치기\n",
    "combined_train_kor_raw = train_kor_raw + augmented_train_kor_raw\n",
    "combined_train_eng_raw = train_eng_raw + train_eng_subset\n",
    "\n",
    "print(f\"최종 학습 데이터 크기: 한국어 {len(combined_train_kor_raw)}, 영어 {len(combined_train_eng_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중복된 문장 쌍 제거\n",
    "- 소문자화 + 구두점 분리 + 특수문자 제거(. ? ! , 만 남김)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"구두점, 특수문자 등 불필요한 부분을 제거하고 소문자로 변환합니다.\"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\\\" \\\"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def clean_and_preprocess_corpus(kor_raw, eng_raw):\n",
    "    \"\"\"문장 쌍의 중복을 제거하고 각 문장을 전처리합니다.\"\"\"\n",
    "    # 1. zip으로 문장 쌍 생성 후 set으로 중복 제거\n",
    "    cleaned_pairs = list(set(zip(kor_raw, eng_raw)))\n",
    "\n",
    "    # 2. 각 문장 전처리\n",
    "    kor_corpus, eng_corpus = [], []\n",
    "    for kor, eng in cleaned_pairs:\n",
    "        kor_corpus.append(preprocess_sentence(kor))\n",
    "        eng_corpus.append(preprocess_sentence(eng))\n",
    "\n",
    "    return kor_corpus, eng_corpus\n",
    "\n",
    "# 증강된 데이터를 포함한 최종 학습 데이터에 대해 정제 및 전처리 수행\n",
    "train_kor_corpus, train_eng_corpus = clean_and_preprocess_corpus(combined_train_kor_raw, combined_train_eng_raw)\n",
    "dev_kor_corpus, dev_eng_corpus = clean_and_preprocess_corpus(dev_kor_raw, dev_eng_raw)\n",
    "test_kor_corpus, test_eng_corpus = clean_and_preprocess_corpus(test_kor_raw, test_eng_raw)\n",
    "\n",
    "print(f\"\\n정제 및 전처리 후 최종 데이터 크기: Train: {len(train_kor_corpus)}, Dev: {len(dev_kor_corpus)}, Test: {len(test_kor_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 설치\n",
    "\n",
    "!pip install konlpy\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab/\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentencePiece 토크나이저 학습 + 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "\n",
    "def generate_tokenizer(corpus, vocab_size, lang, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    file = f'./{lang}_corpus.txt'\n",
    "    model_prefix = f'{lang}_spm'\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={file} --model_prefix={model_prefix} --vocab_size={vocab_size}' + \n",
    "        f' --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'{model_prefix}.model')\n",
    "    return tokenizer\n",
    "\n",
    "# 증강된 데이터로 SentencePiece 토크나이저 재학습\n",
    "SRC_VOCAB_SIZE = 20000\n",
    "TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(train_kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(train_eng_corpus, TGT_VOCAB_SIZE, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 파일 : ko_corpus.txt\n",
    "- 결과 파일 prefix: ko_spm → ko_spm.model, ko_spm.vocab 생성\n",
    "- vocab 크기 : 20,000\n",
    "- 특수 토큰 ID : pad=0, bos=1, eos=2, unk=3\n",
    "- 영어도 동일하게 en_corpus.txt → en_spm.model, en_spm.vocab 으로 학습\n",
    "<br/>\n",
    "\n",
    "- 모델 유형 : UNIGRAM → BPE 말고 유니그램 기반 서브워드 모델\n",
    "- 문자 커버리지 : 99.95% 문자를 커버 → 드물게 쓰이는 문자는 버림\n",
    "- 최대 문장 길이: 4192자\n",
    "- 최대 서브워드 길이: 16자\n",
    "- 스레드 수 : 병렬 학습을 위해 16개 스레드 사용\n",
    "<br/>\n",
    "\n",
    "- 학습에 사용된 문장 수 : 94,123개 (한/영 동일)\n",
    "- ```<pad>, <s>, </s>, <unk>``` 토큰을 vocab에 고정 추가\n",
    "<br/>\n",
    "\n",
    "- 한국어\n",
    "    - 전체 문자 개수 : 약 581만\n",
    "    - 고유 문자 수 : 1,324종 (한글 + 특수기호 포함)\n",
    "    - 커버율 : 99.95% → corpus 대부분의 문자가 반영됨\n",
    "- 영어\n",
    "    - 전체 문자 개수: 약 1,196만\n",
    "    - 고유 문자 수: 82종 (영문 알파벳 + 기호)\n",
    "<br/>\n",
    "\n",
    "- EM (Expectation-Maximization) 반복을 통해 서브워드를 점점 줄여감\n",
    "- size : 남은 서브워드 후보 개수\n",
    "- obj : likelihood 값 (클수록 모델이 데이터를 잘 설명함)\n",
    "- num_tokens : 전체 토큰화 결과 개수\n",
    "- num_tokens/piece : 평균적으로 하나의 서브워드가 몇 번 등장했는지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 및 DataLoader 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 번역 데이터셋을 PyTorch에서 학습 가능한 형태로 구성하는 과정\n",
    "- 문장 쌍을 토큰화하여 텐서로 변환\n",
    "- 배치마다 패딩 처리\n",
    "- DataLoader로 학습용 배치 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset 및 DataLoader 인스턴스 생성\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# TranslationDataset 클래스 정의\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_corpus, tgt_corpus, src_tokenizer, tgt_tokenizer):\n",
    "        self.src_corpus = src_corpus\n",
    "        self.tgt_corpus = tgt_corpus\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_tokenizer.encode_as_ids(self.src_corpus[idx])\n",
    "        tgt = self.tgt_tokenizer.encode_as_ids(self.tgt_corpus[idx])\n",
    "\n",
    "        # 텐서의 데이터 타입을 torch.long으로 명시적으로 지정\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "# collate_fn 함수\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # 여기에 SOS/EOS 토큰을 추가하거나, 이미 토크나이저에서 처리했다면 그대로 사용\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=ko_tokenizer.pad_id())\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=en_tokenizer.pad_id())\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# 증강된 데이터셋으로 TranslationDataset 인스턴스 생성\n",
    "train_dataset = TranslationDataset(train_kor_corpus, train_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "valid_dataset = TranslationDataset(dev_kor_corpus, dev_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "test_dataset = TranslationDataset(test_kor_corpus, test_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "\n",
    "# DataLoader 인스턴스 생성 (변경 없음)\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in valid_loader: {len(valid_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 트랜스포머 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    입력 임베딩에 위치 정보를 추가하는 클래스입니다.\n",
    "    Transformer 모델은 순서 정보가 없으므로, 토큰의 위치를 알려주기 위해 sin/cos 함수를 사용합니다.\n",
    "    이 방식은 고정 위치 인코딩으로, 학습되지 않는 파라미터(buffer)로 등록됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # sin/cos 함수에 사용할 div_term 계산: 주파수 조절을 위한 값\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(10000.0) / emb_size))\n",
    "        # 각 위치(0~maxlen)에 대한 인덱스 생성\n",
    "        position = torch.arange(maxlen).unsqueeze(1)\n",
    "        # 위치 임베딩 행렬 초기화 (maxlen, emb_size)\n",
    "        pos_embedding = torch.zeros(maxlen, emb_size)\n",
    "        # 짝수 인덱스: sin 함수 적용\n",
    "        pos_embedding[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 홀수 인덱스: cos 함수 적용\n",
    "        pos_embedding[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 배치 차원 추가 (1, maxlen, emb_size)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 학습되지 않는 파라미터로 등록\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_embedding: (batch_size, seq_len, emb_size)\n",
    "        Returns:\n",
    "            token_embedding + pos_embedding: 위치 정보가 더해진 임베딩\n",
    "        \"\"\"\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    다중 헤드 어텐션 메커니즘을 구현한 클래스.\n",
    "    쿼리, 키, 값 행렬을 여러 헤드로 분할하여 병렬로 어텐션을 계산하고, 결과를 결합합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads  # 각 헤드의 차원\n",
    "        # 쿼리, 키, 값 행렬을 위한 선형 변환 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        # 최종 출력 선형 변환 레이어\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        스케일드 닷-프로덕트 어텐션 계산.\n",
    "        Args:\n",
    "            Q: 쿼리 행렬\n",
    "            K: 키 행렬\n",
    "            V: 값 행렬\n",
    "            mask: 어텐션 마스크 (선택적)\n",
    "        Returns:\n",
    "            out: 어텐션 가중치 적용된 값 행렬\n",
    "            attentions: 어텐션 가중치 행렬\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        QK = torch.matmul(Q, K.transpose(-1, -2))  # QK^T 계산\n",
    "        scaled_qk = QK / math.sqrt(d_k)  # 스케일링\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)  # 마스크 적용 (매우 작은 값 더하기)\n",
    "        attentions = nn.Softmax(dim=-1)(scaled_qk)  # 소프트맥스 적용\n",
    "        out = torch.matmul(attentions, V)  # 가중치 적용\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        입력 텐서를 여러 헤드로 분할.\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)  # 차원 재배치\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 헤드를 다시 결합.\n",
    "        Args:\n",
    "            x: (batch_size, num_heads, seq_len, depth)\n",
    "        Returns:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        bsz, _, seq_len, _ = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.view(bsz, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: 쿼리 입력 (batch_size, seq_len, d_model)\n",
    "            K: 키 입력\n",
    "            V: 값 입력\n",
    "            mask: 어텐션 마스크\n",
    "        Returns:\n",
    "            out: 어텐션 적용된 출력\n",
    "            attention_weights: 어텐션 가중치\n",
    "        \"\"\"\n",
    "        # 헤드 분할 후 어텐션 계산\n",
    "        WQ = self.split_heads(self.W_q(Q))\n",
    "        WK = self.split_heads(self.W_k(K))\n",
    "        WV = self.split_heads(self.W_v(V))\n",
    "        out, attention_weights = self.scaled_dot_product_attention(WQ, WK, WV, mask)\n",
    "        # 헤드 결합 후 선형 변환\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        return out, attention_weights\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    포지션 와이즈 피드포워드 네트워크.\n",
    "    각 위치별로 독립적으로 적용되는 2층 완전 연결 네트워크 (ReLU 활성화 함수 사용).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # 첫 번째 레이어 (차원 확장)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)  # 두 번째 레이어 (원래 차원으로 복원)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    인코더의 단일 레이어.\n",
    "    셀프 어텐션과 피드포워드 네트워크를 포함하며, 레이어 정규화와 드롭아웃을 적용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)  # 첫 번째 정규화\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)  # 두 번째 정규화\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 텐서\n",
    "            mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 출력 텐서\n",
    "            enc_attn: 셀프 어텐션 가중치\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # 셀프 어텐션 + 드롭아웃 + 잔차 연결\n",
    "        out, enc_attn = self.enc_self_attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 피드포워드 네트워크 + 드롭아웃 + 잔차 연결\n",
    "        out = self.ffn(self.norm_2(out))\n",
    "        out = self.do(out) + residual\n",
    "        return out, enc_attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    디코더의 단일 레이어.\n",
    "    셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 네트워크를 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)  # 셀프 어텐션\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # 인코더-디코더 어텐션\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력\n",
    "            enc_out: 인코더 출력\n",
    "            dec_enc_mask: 디코더-인코더 어텐션 마스크\n",
    "            padding_mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 출력 텐서\n",
    "            dec_attn: 셀프 어텐션 가중치\n",
    "            dec_enc_attn: 인코더-디코더 어텐션 가중치\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # 셀프 어텐션 (look-ahead 마스크 적용)\n",
    "        out, dec_attn = self.dec_self_attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask=padding_mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 인코더-디코더 어텐션\n",
    "        out, dec_enc_attn = self.enc_dec_attn(self.norm_2(out), enc_out, enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 피드포워드 네트워크\n",
    "        out = self.ffn(self.norm_3(out))\n",
    "        out = self.do(out) + residual\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    인코더 전체 구조.\n",
    "    임베딩 레이어, 위치 인코딩, 여러 개의 인코더 레이어로 구성됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout, vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # 토큰 임베딩\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)  # 위치 인코딩\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 시퀀스 (batch_size, seq_len)\n",
    "            mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 인코더 출력\n",
    "            enc_attns: 각 레이어의 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        out = self.embedding(x) * math.sqrt(self.d_model)  # 임베딩 스케일링\n",
    "        out = self.pos_encoding(out)  # 위치 인코딩 추가\n",
    "        enc_attns = []\n",
    "        for layer in self.enc_layers:\n",
    "            out, enc_attn = layer(out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    디코더 전체 구조.\n",
    "    임베딩 레이어, 위치 인코딩, 여러 개의 디코더 레이어로 구성됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력 시퀀스\n",
    "            enc_out: 인코더 출력\n",
    "            dec_enc_mask: 디코더-인코더 어텐션 마스크\n",
    "            padding_mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 디코더 출력\n",
    "            dec_attns: 셀프 어텐션 가중치 리스트\n",
    "            dec_enc_attns: 인코더-디코더 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        out = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        out = self.pos_encoding(out)\n",
    "        dec_attns, dec_enc_attns = [], []\n",
    "        for layer in self.dec_layers:\n",
    "            out, dec_attn, dec_enc_attn = layer(out, enc_out, dec_enc_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    전체 Transformer 모델.\n",
    "    인코더와 디코더를 연결하고, 최종 출력 레이어를 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout, src_vocab_size)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout, tgt_vocab_size)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)  # 최종 출력 레이어\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: 소스 시퀀스 (batch_size, src_seq_len)\n",
    "            tgt: 타겟 시퀀스 (batch_size, tgt_seq_len)\n",
    "        Returns:\n",
    "            logits: 최종 예측 로짓 (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "            enc_attns: 인코더 어텐션 가중치 리스트\n",
    "            dec_attns: 디코더 셀프 어텐션 가중치 리스트\n",
    "            dec_enc_attns: 디코더-인코더 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        # 마스크 생성\n",
    "        src_mask = (src == ko_tokenizer.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt == en_tokenizer.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "        lookahead_mask = torch.triu(torch.ones(tgt.shape[1], tgt.shape[1]), diagonal=1).bool().to(device)\n",
    "        tgt_mask = tgt_mask | lookahead_mask\n",
    "        # 인코더/디코더 순전파\n",
    "        enc_out, enc_attns = self.encoder(src, src_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```nn.CrossEntropyLoss``` : 다중 분류(classification)에서 가장 많이 쓰는 손실 함수\n",
    "- <pad> 토큰은 학습에 불필요하므로 손실 계산에서 제외\n",
    "    - 예를 들어 패딩된 부분이 많아도 loss가 그 부분 때문에 왜곡되지 않음\n",
    "<br/>\n",
    "\n",
    "- 옵티마이저 : Adam\n",
    "- lr=0.0001 : 학습률(learning rate)\n",
    "- betas=(0.9, 0.98) : 모멘텀 계수 (논문에서 제안된 설정)\n",
    "- 0.9 : 1차 모멘텀(gradient 평균)\n",
    "- 0.98 : 2차 모멘텀(gradient 제곱 평균)\n",
    "- eps=1e-9 : 아주 작은 값 → 분모가 0 되는 걸 방지\n",
    "- 이 설정은 논문 “Attention is All You Need”의 Transformer 원 논문에 나온 Adam 설정과 거의 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(N_LAYERS, D_MODEL, N_HEADS, D_FF, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, DROPOUT).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ko_tokenizer.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip, log_interval=100):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].to(device)\n",
    "        tgt = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _, _, _ = model(src, tgt[:,:-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # log_interval 마다 로그 출력\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(f\"  - Step {i+1}/{len(iterator)} | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0].to(device)\n",
    "            tgt = batch[1].to(device)\n",
    "\n",
    "            output, _, _, _ = model(src, tgt[:,:-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02} / {EPOCHS:02}\")\n",
    "    print(\"Training...\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, 1)\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer-baseline.pt')\n",
    "        print(\"best model saved.\")\n",
    "\n",
    "    print(f'Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 번역 및 성능 평가 (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    src_tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "    tgt_tokens = [tgt_tokenizer.bos_id()]\n",
    "    for i in range(max_len):\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, _, _, dec_enc_attns = model(src_tensor, tgt_tensor)\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        tgt_tokens.append(pred_token)\n",
    "        if pred_token == tgt_tokenizer.eos_id():\n",
    "            break\n",
    "    tgt_sentence = tgt_tokenizer.decode_ids(tgt_tokens)\n",
    "    return tgt_sentence, dec_enc_attns\n",
    "\n",
    "example_idx = 0\n",
    "src = test_kor_raw[example_idx]\n",
    "trg = test_eng_raw[example_idx]\n",
    "translation, attention = translate_sentence(src, ko_tokenizer, en_tokenizer, model, device)\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
    "    \"\"\"어텐션 맵을 시각화합니다.\"\"\"\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    font_path = 'NanumBarunGothic.ttf'\n",
    "    font_prop = fm.FontProperties(fname=font_path, size=8)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 28))  # x축을 조금 넓혀서 압축 줄임 (10->12)\n",
    "\n",
    "    # 번역된 문장과 원본 문장을 토큰 단위로 분리\n",
    "    sentence_tokens = sentence.split()\n",
    "    translation_tokens = translation.split()\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # attention shape: (head_idx, tgt_len, src_len)\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        # extent 명시: (-0.5, src_len-0.5, tgt_len-0.5, -0.5)로 ticks와 맞춤\n",
    "        src_len = len(sentence_tokens)\n",
    "        tgt_len = len(translation_tokens)\n",
    "        cax = ax.matshow(_attention, cmap='viridis', extent=[-0.5, src_len - 0.5, tgt_len - 0.5, -0.5])\n",
    "\n",
    "        # 눈금 위치 설정\n",
    "        ax.set_xticks(range(src_len))\n",
    "        ax.set_yticks(range(tgt_len))\n",
    "\n",
    "        # 라벨 설정: ha/va로 중앙 정렬\n",
    "        ax.set_xticklabels(sentence_tokens, rotation=90, fontproperties=font_prop, ha='center', va='center')\n",
    "        ax.set_yticklabels(translation_tokens, fontproperties=font_prop, ha='right', va='center')\n",
    "\n",
    "        ax.tick_params(labelsize=8, pad=15)  # pad로 텍스트와 tick 간격 미세 조정\n",
    "\n",
    "    plt.tight_layout()  # subplot 간 여백 자동 조정 (밀림 방지)\n",
    "    plt.show()\n",
    "\n",
    "display_attention(src, translation, attention[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention_sp(src_sentence, tgt_sentence, attention, \n",
    "                         src_tokenizer, tgt_tokenizer,\n",
    "                         n_heads=8, n_rows=4, n_cols=2):\n",
    "    \"\"\"SentencePiece 토큰 기준 어텐션 맵 시각화\"\"\"\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    font_path = 'NanumBarunGothic.ttf'\n",
    "    font_prop = fm.FontProperties(fname=font_path, size=8)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 28))\n",
    "\n",
    "    # 1️SentencePiece 토큰화 (ID → piece)\n",
    "    src_ids = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_ids = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    src_tokens = [src_tokenizer.id_to_piece(idx) for idx in src_ids]\n",
    "    tgt_tokens = [tgt_tokenizer.id_to_piece(idx) for idx in tgt_ids]\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # attention shape: (n_heads, tgt_len, src_len)\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        src_len = len(src_tokens)\n",
    "        tgt_len = len(tgt_tokens)\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='viridis',\n",
    "                         extent=[-0.5, src_len - 0.5, tgt_len - 0.5, -0.5])\n",
    "\n",
    "        # SentencePiece 토큰을 tick label로 붙임\n",
    "        ax.set_xticks(range(src_len))\n",
    "        ax.set_yticks(range(tgt_len))\n",
    "\n",
    "        ax.set_xticklabels(src_tokens, rotation=90, fontproperties=font_prop, ha='center', va='center')\n",
    "        ax.set_yticklabels(tgt_tokens, fontproperties=font_prop, ha='right', va='center')\n",
    "\n",
    "        ax.tick_params(labelsize=8, pad=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_attention_sp(\n",
    "    src_sentence=src,              # 한국어 원문 문자열\n",
    "    tgt_sentence=translation,      # 영어 번역 문자열\n",
    "    attention=attention[-1],       # 마지막 layer attention\n",
    "    src_tokenizer=ko_tokenizer, \n",
    "    tgt_tokenizer=en_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
