{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4ed961-758d-4a76-9775-444345adbf66",
   "metadata": {},
   "source": [
    "## 뉴스 요약봇 만들기 🤖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f171ac-76aa-4ffb-9600-3c58460b53a0",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 및 패키지 준비\n",
    "- NLTK : 영어 기호, 통계, 자연어 처리를 위한 라이브러리입니다.\n",
    "- NLTK에는 I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어가 미리 정리되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbe8a7-3ad9-4f53-8465-fb88f39c548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7de5-3a43-4253-805a-300291324aa1",
   "metadata": {},
   "source": [
    "- BeautifulSoup : 문서를 파싱 하는데 사용하는 패키지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27ecf9-7839-45f7-95de-91213be8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396f52c-669e-4c6e-b7e7-dea0e4d13c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK에서 불용어 사전 다운로드\n",
    "# 데이터 전처리에 필요한 패키지 다운로드\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6d88b-8c1a-4095-bcdf-ea2e329e13f9",
   "metadata": {},
   "source": [
    "### 2. 데이터 수집하기\n",
    "- 이미 데이터를 한 번 불러왔었기 때문에 주석 처리했습니다.\n",
    "- 추상적 요약을 할 때는 text를 본문, headlines를 이미 요약된 데이터로 간주하고 학습할 수 있습니다.\n",
    "- 추출적 요약을 할 때에는 text 열만 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044915fa-5968-44d3-9379-18cf0f3ce8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib.request\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177feaf4-bc59-4828-85f1-713b20450e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35590dc-eb5e-4b1c-ae57-5786b7d4e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55392ac-226e-4a84-8633-226207a661c4",
   "metadata": {},
   "source": [
    "- 데이터 열 중에서 Summary와 Text 열만 별도로 저장해서 사용합니다.\n",
    "- 참고로 Text 열의 내용을 요약한 것이 Summary 열입니다.\n",
    "- Text 시퀀스를 입력 받으면 Summary 시퀀스를 예측할 수 있도록 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218749d5-4204-41fe-be16-df0fe3b6a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'headlines']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcc4cd-9e69-4686-9ec1-13647127e179",
   "metadata": {},
   "source": [
    "### 3. 데이터 전처리하기 (추상적 요약)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd67fc-035b-40f5-b98b-8873a08788ad",
   "metadata": {},
   "source": [
    "- 중복 샘플과 NULL 값이 존재하는 샘플을 제거합니다.\n",
    "- ```drop_duplicates()```를 사용해서 중복 샘플을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0dbca-0866-4374-a9dd-946c02450f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
    "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103f226-f75f-4ef3-9e0a-82e2a23622f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다.\n",
    "\n",
    "data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd412b37-deae-4558-95dd-e2c1101a681a",
   "metadata": {},
   "source": [
    "- Null 값 한 개가 남아있을 수 있으므로 ```.isnull().sum()```을 통해 Null 값이 있는지 확인하고 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b3973-f1ad-4c7d-9a9c-3ad5fe49c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1ffc7-3c9f-4916-90df-55ca17f878a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ddc62-8bae-428a-bd86-a782b9ac914e",
   "metadata": {},
   "source": [
    "- 텍스트 정규화 : it'll = it will, must'n = must not 등 이러한 표현을 같은 표현으로 통일을 시켜 기계의 연산량을 줄이는 방법입니다.\n",
    "- 텍스트 정규화를 위해 사전을 구성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf216d-155b-431e-b065-e1502621fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578e363-b079-446d-8cb7-c8d6df63d5b7",
   "metadata": {},
   "source": [
    "- NLTK에서 제공하는 불용어 리스트를 참조해서, 샘플의 불용어를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a57c8c-0cf0-4c7a-839f-1c8055559bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302f33e-c429-4f53-aad6-fa226ed4d209",
   "metadata": {},
   "source": [
    "- 불용어 제거 + 모든 영어 문자 소문자화 + HTML 태그 제거 + 특수 문자 제거를 목표로 합니다.\n",
    "- 다만 text 전처리 시에만 호출하고 headlines는 자연스러운 문장 요약 결과를 위해 불용어를 삭제하지 않습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8258d5e-8db1-4bad-bc95-52033c0ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "\n",
    "    # 불용어 제거 (text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Headlines)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3735d-b82e-4399-ba93-28372aeb80d1",
   "metadata": {},
   "source": [
    "- lxml 설치 후에 꼭 커널만 Restart 해야 오류가 나지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37f5bd-4d28-428e-859b-74103ebbd303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ec347-7878-43a6-92a7-1d4c4be09eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_headlines = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"headlines:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ee12a-7337-4670-b236-185704136ade",
   "metadata": {},
   "source": [
    "- 전체 text 데이터에 대한 전처리를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f4dcd-44cd-4aac-aa92-d0c614c57bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다.\n",
    "clean_text = []\n",
    "\n",
    "for sentence in data['text']:\n",
    "    clean_text.append(preprocess_sentence(sentence))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1b094-3d65-4c23-ae95-503e0c719547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 headlines 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다.\n",
    "clean_headlines = []\n",
    "\n",
    "for sentence in data['headlines']:\n",
    "    clean_summary.append(preprocess_sentence(sentence, remove_stopwords=False))\n",
    "\n",
    "print(\"headlines 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd435d2f-c40b-4974-9533-3194ee82eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ab40f-afc8-402c-b91b-fa041222faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f5bdb-71a0-4aeb-bc13-b77ad9c8c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6050ec-d260-43f5-9ce6-b9edb3f8dcb6",
   "metadata": {},
   "source": [
    "- text와 headlines의 최소, 최대, 평균 길이를 구하고 길이 분포를 시각화해서 본 다음, 샘플의 최대 길이를 정하기로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82be66-4926-4141-bc54-8ed227017526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Headlines')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9354e-573b-450f-90d0-3a30a4663ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "headlines_max_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5542b5-b750-41d5-b24c-f72d93e84dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ca237-3087-46ff-a526-c7140b774e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(summary_max_len, data['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ec01f-29e8-447f-93aa-3db445801d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01134c9e-fc46-409f-8027-0280f66a7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865e93f-e5e3-4cb2-8066-d005c431ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2336d-634b-4ce5-b110-a8ef92ddb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993d0ad-e4fc-49ee-83b4-c529915d9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9273f-9521-4c8a-a593-aa555ddb16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca248f6-225e-41b0-83d3-5edfd051b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916001f7-5af6-4abf-b6ba-de7419b1f51b",
   "metadata": {},
   "source": [
    "- 단어 집합(Vocabulary) : 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꿔야 하는데, 이 때 각 단어에 고유한 정수를 맵핑하는 것을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cbcb9-6f9a-4402-b90b-f80b6c66d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_tokenizer(text): # 토크나이저 정의\n",
    "    text = text.lower()  # 소문자로 변환\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # 특수문자 제거\n",
    "    return text.split()  # 공백 기준 토큰화\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩과 UNK 토큰 추가\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        word_counter.update(src_tokenizer(text))  # 단어 빈도수 계산\n",
    "\n",
    "    # 단어 집합 생성 (빈도가 높은 순서대로)\n",
    "    for word, _ in word_counter.most_common():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3b808-c4af-4253-a35c-8c65f0944b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도 수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인합니다.\n",
    "\n",
    "threshold = 7\n",
    "\n",
    "# 전처리된 데이터 사용\n",
    "text_data = data['text'].tolist()\n",
    "summary_data = data['headlines'].tolist()\n",
    "# 단어 빈도수 계산\n",
    "word_counter = Counter()\n",
    "for text in text_data:\n",
    "    word_counter.update(text.split())\n",
    "\n",
    "total_cnt = len(word_counter)  # 전체 단어 개수\n",
    "total_freq = sum(word_counter.values())  # 전체 단어 등장 횟수\n",
    "rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # 희귀 단어 개수\n",
    "rare_freq = sum(count for count in word_counter.values() if count < threshold)  # 희귀 단어 등장 횟수\n",
    "\n",
    "# 희귀 단어를 제외한 단어 사전 구축\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩 및 미등록 단어 추가\n",
    "word_index = {word: idx + 2 for idx, (word, count) in enumerate(word_counter.items()) if count >= threshold}\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58c6c8-309d-49ae-976d-cd5c807ba19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 8000\n",
    "\n",
    "def build_limited_vocab(texts, vocab_size):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩과 UNK 토큰 추가\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        word_counter.update(src_tokenizer(text))  # 단어 빈도수 계산\n",
    "\n",
    "    # 빈도가 높은 상위 vocab_size - 2개 단어만 선택 (PAD, UNK 포함)\n",
    "    for word, _ in word_counter.most_common(vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_limited_vocab(encoder_input_train, src_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb247c6-c155-4225-83ed-f9211d102863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(texts, vocab):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in src_tokenizer(text)]\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "# 텍스트 데이터 정수 시퀀스로 변환\n",
    "encoder_input_train_seq = text_to_sequence(encoder_input_train, src_vocab)\n",
    "encoder_input_test_seq = text_to_sequence(encoder_input_test, src_vocab)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train_seq[:3])\n",
    "print(encoder_input_test_seq[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e0913-3811-4744-a694-0a8030d1852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_tokenizer(text):\n",
    "    text = text.lower()  # 소문자로 변환\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # 특수문자 제거\n",
    "    return text.split()  # 공백 기준 토큰화\n",
    "\n",
    "tar_vocab = build_vocab(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40774c-75aa-42aa-bf06-13412df9cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6\n",
    "\n",
    "word_counter = Counter()\n",
    "for text in decoder_input_train:\n",
    "    word_counter.update(tar_tokenizer(text))  # 각 문장의 단어 빈도 계산\n",
    "\n",
    "# 전체 단어 개수 및 등장 빈도 계산\n",
    "total_cnt = len(word_counter)  # 전체 단어 개수\n",
    "total_freq = sum(word_counter.values())  # 전체 단어 등장 횟수\n",
    "rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # 희귀 단어 개수\n",
    "rare_freq = sum(count for count in word_counter.values() if count < threshold)  # 희귀 단어 등장 횟수\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b1b0a-dd6b-443f-baae-737fc00ec351",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab_size = 2000\n",
    "tar_vocab = build_limited_vocab(decoder_input_train + decoder_target_train, tar_vocab_size)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train_seq = text_to_sequence(decoder_input_train, tar_vocab)\n",
    "decoder_target_train_seq = text_to_sequence(decoder_target_train, tar_vocab)\n",
    "decoder_input_test_seq = text_to_sequence(decoder_input_test, tar_vocab)\n",
    "decoder_target_test_seq = text_to_sequence(decoder_target_test, tar_vocab)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train_seq[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3ef1b-a76e-4c4b-b063-047285cacacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab_size = 2000\n",
    "tar_vocab = build_limited_vocab(decoder_input_train + decoder_target_train, tar_vocab_size)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train_seq = text_to_sequence(decoder_input_train, tar_vocab)\n",
    "decoder_target_train_seq = text_to_sequence(decoder_target_train, tar_vocab)\n",
    "decoder_input_test_seq = text_to_sequence(decoder_input_test, tar_vocab)\n",
    "decoder_target_test_seq = text_to_sequence(decoder_target_test, tar_vocab)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train_seq[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed9fc7-20cc-489a-a293-1ebdc29383f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7dfe1-92c9-4dbd-a570-f8ac745d6a47",
   "metadata": {},
   "source": [
    "- 텍스트 시퀀스를 정수 시퀀스로 변환했으므로, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주기 위해 패딩 작업을 해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e2b80-3563-409b-b0d6-ec0bfb188b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 텐서 변환 함수 (리스트 → PyTorch 텐서)\n",
    "def convert_to_tensor(sequences):\n",
    "    return [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "\n",
    "# 패딩 적용 함수 (PyTorch `pad_sequence()` 활용)\n",
    "def pad_sequences_pytorch(sequences, maxlen, padding_value=0):\n",
    "    sequences = convert_to_tensor(sequences)  # 리스트를 텐서로 변환\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=padding_value)  # 패딩 적용\n",
    "    return padded_seqs[:, :maxlen]  # maxlen 길이로 자르기 (최대 길이 초과 방지)\n",
    "\n",
    "# 패딩 적용\n",
    "encoder_input_train = pad_sequences_pytorch(encoder_input_train_seq, maxlen=text_max_len)\n",
    "encoder_input_test = pad_sequences_pytorch(encoder_input_test_seq, maxlen=text_max_len)\n",
    "decoder_input_train = pad_sequences_pytorch(decoder_input_train_seq, maxlen=summary_max_len)\n",
    "decoder_target_train = pad_sequences_pytorch(decoder_target_train_seq, maxlen=summary_max_len)\n",
    "decoder_input_test = pad_sequences_pytorch(decoder_input_test_seq, maxlen=summary_max_len)\n",
    "decoder_target_test = pad_sequences_pytorch(decoder_target_test_seq, maxlen=summary_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5052dd2-a522-4747-bcb1-e0b569190426",
   "metadata": {},
   "source": [
    "### 6. 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1708dc-d891-4788-8308-ff1545830986",
   "metadata": {},
   "source": [
    "- 인코더 : 입력 문장을 임베딩하고 LSTM을 거쳐 문맥 정보(hidden state, cell state)로 압축합니다. (Output : 각 시점별 LSTM 출력 / Hidden : 마지막 시점의 Hidden state (디코더 초기 상태로 사용) / Cell : 마지막 시점의 Cell state (디코더 초기 상태로 사용)\n",
    "\n",
    "- 디코더 : 이전 시점의 단어 또는 SOS 토큰을 입력 받아 LSTM으로 다음 단어를 예측할 준비를 합니다. 인코더의 Hidden, Cell을 초기 상태로 받아서 문장 생성을 시작합니다.\n",
    "\n",
    "- 시퀀스 투 시퀀스 : 인코더로 입력 시퀀스를 인코딩하고, 디코더로 출력 시퀀스를 한 시점식 디코딩합니다. 디코더의 각 시점 출력을 Linear Layer에 통과시켜 단어 분류 확률 벡터를 생성합니다.\n",
    "\n",
    "```\n",
    "[입력 문장] → Encoder → (hidden, cell) ─┐\n",
    "                                        ↓\n",
    "                              Decoder (반복) → Linear Layer → 단어 확률\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342810e-75f2-4a78-a7ca-7328f67300b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256 # LSTM에서 얼만큼의 수용력(Capacity)을 가질지 정하는 파라미터\n",
    "src_vocab_size = len(src_vocab)  # 단어 집합 크기\n",
    "\n",
    "# 인코더\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # 인코더의 임베딩 층\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # LSTM 실행\n",
    "        return output, hidden, cell\n",
    "\n",
    "# 인코더 모델 생성\n",
    "encoder = Encoder(src_vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f12ef-eb6f-4379-a8d6-970ce3b82b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.4, num_layers=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden, cell): # 디코더의 임베딩 층\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # 초기 상태를 인코더에서 전달받음\n",
    "        return output, hidden, cell\n",
    "\n",
    "# 디코더 모델 생성\n",
    "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a9d46-ddf5-4502-8c1d-1e813eccb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 출력층\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.softmax_layer = nn.Linear(hidden_size, vocab_size)  # 출력층 정의\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # 인코더 실행\n",
    "        encoder_output, hidden, cell = self.encoder(encoder_input)\n",
    "\n",
    "        # 디코더 실행\n",
    "        decoder_output, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # 출력층 적용 (Softmax는 Loss 내부에서 적용되므로 생략 가능)\n",
    "        output = self.softmax_layer(decoder_output)\n",
    "        return output\n",
    "\n",
    "# 모델 정의\n",
    "model = Seq2Seq(encoder, decoder, tar_vocab_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b8cda-f449-4a66-bd32-4ef0d7c8958b",
   "metadata": {},
   "source": [
    "- 아래는 기존 구조에 어텐션 메커니즘을 적용합니다.\n",
    "- 아래 코드는 인코더의 hidden state들과 디코더의 hidden state들을 어텐셥 함수의 입력으로 사용하고, 어텐션 함수가 리턴한 값을 디코더의 hidden state와 함께 활용하는 형태로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8df50-ab47-4b2b-8c09-9b2ec1173c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_dot(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_dot, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)  # 어텐션 가중치\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)  # 어텐션 가중치 벡터\n",
    "\n",
    "    def forward(self, decoder_output, encoder_outputs):\n",
    "        attn_weights = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  # 어텐션 가중치 정규화\n",
    "        attn_out = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size, hidden_size):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = Attention_dot(hidden_size)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)  # 어텐션 결합\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # 최종 출력층\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
    "        decoder_outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # 어텐션 적용\n",
    "        attn_out = self.attention(decoder_outputs, encoder_outputs)\n",
    "\n",
    "        # 어텐션 결과와 디코더 출력 연결\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "\n",
    "        # 어텐션 결합 후 최종 출력\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "        output = self.output_layer(decoder_concat_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# 모델 생성\n",
    "model = Seq2SeqWithAttention(encoder, decoder, tar_vocab_size, hidden_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474fadb-d736-4732-9034-3b7a9b08b0bd",
   "metadata": {},
   "source": [
    "### 7. 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08694a62-6674-4913-98d3-d4f7372d9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "patience = 2\n",
    "\n",
    "# 손실 함수 & 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 패딩 토큰 무시\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# PyTorch DataLoader 설정\n",
    "train_dataset = TensorDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TensorDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8c0fe-b7a7-4df8-82f8-85ad4ca428da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "patience = 2\n",
    "\n",
    "# 손실 함수 & 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 패딩 토큰 무시\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# PyTorch DataLoader 설정\n",
    "train_dataset = TensorDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TensorDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e279a-5256-435e-b29a-357286213b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 학습 함수\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs, patience):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for encoder_input, decoder_input, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 정수형 변환\n",
    "            encoder_input = encoder_input.to(device).long()\n",
    "            decoder_input = decoder_input.to(device).long()\n",
    "            target = target.to(device).long()\n",
    "\n",
    "            # 모델 실행\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation loss 계산\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for encoder_input, decoder_input, target in test_loader:\n",
    "                encoder_input = encoder_input.to(device).long()\n",
    "                decoder_input = decoder_input.to(device).long()\n",
    "                target = target.to(device).long()\n",
    "\n",
    "                output = model(encoder_input, decoder_input)\n",
    "                output = output.view(-1, output.shape[-1])\n",
    "                target = target.view(-1)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping 조건\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8450db9-bad6-4039-bc50-32f149e5d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 30분 이상 시간이 걸릴 수 있습니다. GPU 환경에서 학습하는 것을 권장합니다.\n",
    "# 학습 실행\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c410a-4a8b-4d7d-a3fc-c9ad155f6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24896527-69aa-4bc2-a225-e48a4573e86b",
   "metadata": {},
   "source": [
    "### 8. 인퍼런스 모델 구현하기\n",
    "- 인퍼런스 모델 : 이미 학습이 끝난 모델을 이용해 새로운 입력에 대한 예측을 수행하는 모델을 말합니다.\n",
    "- 시퀀스 투 시퀀스는 훈련할 때와 실제 동작할 때의 방식이 달라서 그에 맞게 모델 설계를 따로 해줘야 한다고 합니다.\n",
    "- 훈련 때는 인코더가 입력 문장을 인코딩하고 디코더가 정답 시퀀스의 이전 단어를 받아서 다음 단어를 예측합니다.\n",
    "- 하지만 실제 상황에서는 정답이 없어서 디코더가 직전에 자신이 예측한 단어를 다시 입력으로 넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871868d-4136-4a5d-8fa4-0cb091cab8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = {idx: word for word, idx in src_vocab.items()} # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_vocab # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = {idx: word for word, idx in tar_vocab.items()} # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb44d5d-1b3f-4922-a0d1-0211f81c169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder.to(device)\n",
    "\n",
    "# 인코더 설계\n",
    "def encode_input(encoder, input_seq):\n",
    "    encoder_outputs, hidden, cell = encoder(input_seq)\n",
    "    return encoder_outputs, hidden, cell\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "num_layers = 3  # 디코더 LSTM 레이어 개수 (설정에 맞춰 조정)\n",
    "batch_size = 1\n",
    "\n",
    "decoder_state_input_h = torch.zeros((num_layers, batch_size, hidden_size), dtype=torch.float, device=device)\n",
    "decoder_state_input_c = torch.zeros((num_layers, batch_size, hidden_size), dtype=torch.float, device=device)\n",
    "decoder_input = torch.zeros((batch_size, 1), dtype=torch.long, device=device)\n",
    "\n",
    "dec_emb2 = decoder.embedding(decoder_input)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder(decoder_input, decoder_state_input_h, decoder_state_input_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2cc5d-53e0-45f5-891d-67d04e4c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, decoder, attention, hidden_size, vocab_size):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.decoder = decoder  # 기존 디코더\n",
    "        self.attention = attention  # 어텐션 레이어\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)  # 어텐션 결합 레이어\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # 최종 출력층\n",
    "        self.softmax = nn.Softmax(dim=-1)  # 소프트맥스\n",
    "\n",
    "    def forward(self, decoder_inputs, decoder_hidden_state, decoder_state_h, decoder_state_c):\n",
    "        # 디코더 실행\n",
    "        decoder_outputs, state_h, state_c = self.decoder(decoder_inputs, decoder_state_h, decoder_state_c)\n",
    "\n",
    "        # 어텐션 적용\n",
    "        attn_out = self.attention(decoder_outputs, decoder_hidden_state)\n",
    "\n",
    "        # 어텐션과 디코더 출력 결합\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "\n",
    "        # 최종 출력층 적용\n",
    "        decoder_outputs2 = self.softmax(self.output_layer(decoder_concat_output))\n",
    "\n",
    "        return decoder_outputs2, state_h, state_c\n",
    "\n",
    "# 기존 Attention 클래스 사용\n",
    "attention_layer = Attention_dot(hidden_size)\n",
    "\n",
    "# 디코더 모델 생성\n",
    "decoder_model = DecoderWithAttention(decoder, attention_layer, hidden_size, tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82388b-baaf-4c7a-ae78-1df7cac707b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder, decoder, tar_word_to_index, tar_index_to_word, text_max_len, summary_max_len, device):\n",
    "    # 입력을 PyTorch Tensor로 변환\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long, device=device)\n",
    "\n",
    "    # 인코더 실행하여 초기 상태(hidden, cell) 얻기\n",
    "    with torch.no_grad():\n",
    "        e_out, e_h, e_c = encoder(input_seq)\n",
    "\n",
    "    e_out = e_out.repeat(1, text_max_len, 1)  # 차원 조정 (np.tile 대신 repeat 사용)\n",
    "\n",
    "    # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        # 디코더 실행\n",
    "        with torch.no_grad():\n",
    "            output_tokens, h, c = decoder(target_seq, e_h, e_c)\n",
    "\n",
    "        # 가장 높은 확률을 가진 단어 선택\n",
    "        sampled_token_index = torch.argmax(output_tokens[0, -1, :]).item()\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostoken':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # 종료 조건: <eos>에 도달하거나 최대 길이를 초과하면 중단\n",
    "        if sampled_token == 'eostoken' or len(decoded_sentence.split()) >= (summary_max_len - 1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태 업데이트\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13ff04-26c2-4014-8750-af4581b72809",
   "metadata": {},
   "source": [
    "### 9. 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd5adc-c6e2-4911-b352-dc4ff9d10fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        key = int(i.item())  # PyTorch Tensor → int 변환\n",
    "        if key != 0:  # 패딩(0) 제외\n",
    "            temp = temp + src_index_to_word.get(key, \"<UNK>\") + ' '  # 안전한 조회\n",
    "    return temp.strip()\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        key = int(i.item())  # PyTorch Tensor → int 변환\n",
    "        if key != 0 and key != tar_word_to_index['sostoken'] and key != tar_word_to_index['eostoken']:\n",
    "            temp = temp + tar_index_to_word.get(key, \"<UNK>\") + ' '  # 안전한 조회\n",
    "    return temp.strip()  # 양쪽 공백 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efce7dd-4b20-47c0-935d-98187e11a5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    input_seq = torch.tensor(encoder_input_test[i], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(\"예측 요약 :\", decode_sequence(input_seq, encoder, decoder, tar_word_to_index, tar_index_to_word, text_max_len, summary_max_len, device))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f160be-bf5f-46aa-9059-f4b95fe0d481",
   "metadata": {},
   "source": [
    "### 10. 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39797ecb-a1a9-4561-9564-dda3369794f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80176fc-8b7f-4b90-b455-2d8a1d2dd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c0f8c-0aed-488a-b901-8bfb4bff490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일부만 출력해보기\n",
    "\n",
    "print(data[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c62f79-397c-468e-922d-25ad6506a677",
   "metadata": {},
   "source": [
    "#### Summa - summarize()\n",
    "\n",
    "- ```text (str)``` : 요약할 테스트.\n",
    "- ```ratio (float, optional)``` : 요약문에서 원본에서 선택되는 문장 비율. (0~1 사이 값)\n",
    "- ```words (int or None, optional)``` : 출력에 포함할 단어 수. 만약, ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시합니다.\n",
    "- ```split (bool, optional)``` : True면 문장 list / False는 조인(join)된 문자열을 반환합니다.\n",
    "\n",
    "- Summa의 summarize는 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행합니다. (원문을 입력으로 넣을 수 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bacee9-1d81-412c-ab50-2b9d56495e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd46393-c251-4169-81d2-8bc3dd878713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed1b58-8bb2-4054-9ff4-6faa6ed74038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.01, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb7fbd-0194-4af8-a365-a5461b43baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 수를 설정하여 요약문의 크기 조절 가능\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4922ef-94ec-4969-9d70-4fcda8b799af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
