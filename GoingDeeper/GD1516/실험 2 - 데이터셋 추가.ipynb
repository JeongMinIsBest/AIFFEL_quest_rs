{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c872522",
   "metadata": {},
   "source": [
    "1. ë°ì´í„°ì…‹ ì¶”ê°€ ë° í†µí•© ğŸ“š\n",
    "\n",
    "- KoAlpaca ë°ì´í„°ì…‹ ë¡œë“œ : ë¨¼ì € datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ê³µê°œëœ í•œêµ­ì–´ ë°ì´í„°ì…‹ì¸ beomi/KoAlpaca-v1.1aë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "- ë°ì´í„° í˜•ì‹ ë§ì¶”ê¸° : KoAlpaca ë°ì´í„°ì…‹ì˜ ì»¬ëŸ¼ ì´ë¦„('instruction', 'output')ì„ ê¸°ì¡´ KoChatGPT ë°ì´í„°ì…‹ê³¼ ë™ì¼í•˜ê²Œ('prompt', 'completion') ë³€ê²½í•©ë‹ˆë‹¤.\n",
    "- ë°ì´í„°ì…‹ í†µí•© : ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ kochatgpt_1_SFT.jsonl ë°ì´í„°ì™€ ìƒˆë¡œ ë¶ˆëŸ¬ì˜¨ KoAlpaca ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì³ combined_dataë¼ëŠ” í†µí•© ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë” ë‹¤ì–‘í•˜ê³  ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7453d-eb09-4862-a6e2-b26d5eaff7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1. Ko-Alpaca ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "new_data = load_dataset(\"beomi/KoAlpaca-v1.1a\", split=\"train\")\n",
    "\n",
    "# 2. ë°ì´í„° í˜•ì‹ ë³€í™˜ í•¨ìˆ˜\n",
    "# Ko-AlpacaëŠ” 'instruction'ê³¼ 'output'ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,\n",
    "# ì´ë¥¼ 'prompt'ì™€ 'completion'ìœ¼ë¡œ ì´ë¦„ì„ ë°”ê¿”ì¤ë‹ˆë‹¤.\n",
    "def transform_ko_alpaca(example):\n",
    "    return {\n",
    "        'prompt': example['instruction'],\n",
    "        'completion': example['output']\n",
    "    }\n",
    "\n",
    "# í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
    "new_data_transformed = new_data.map(transform_ko_alpaca, remove_columns=new_data.column_names)\n",
    "\n",
    "# 3. ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ (ë§Œì•½ ë¡œì»¬ì— ì €ì¥ëœ jsonlì„ ì‚¬ìš©í•œë‹¤ë©´)\n",
    "# ì´ì „ì— data ë³€ìˆ˜ì— ë¡œë“œí–ˆë‹¤ë©´ ê·¸ ë³€ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤.\n",
    "# ì—¬ê¸°ì„œëŠ” ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” ì½”ë“œë¥¼ ì˜ˆì‹œë¡œ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.\n",
    "original_data = load_dataset(\"json\", data_files=\"KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl\", split=\"train\")\n",
    "\n",
    "\n",
    "# 4. ë‘ ë°ì´í„°ì…‹ í†µí•©\n",
    "# concatenate_datasets í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë°ì´í„°ë¥¼ í•©ì¹©ë‹ˆë‹¤.\n",
    "combined_data = concatenate_datasets([original_data, new_data_transformed])\n",
    "\n",
    "print(f\"--- ë°ì´í„°ì…‹ í†µí•© ì™„ë£Œ ---\")\n",
    "print(f\"ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(original_data)}\")\n",
    "print(f\"ì¶”ê°€ëœ ë°ì´í„°ì…‹ í¬ê¸°: {len(new_data_transformed)}\")\n",
    "print(f\"í†µí•© í›„ ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {len(combined_data)}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ëŠ” ì‚­ì œ\n",
    "del new_data, new_data_transformed, original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e398bfe",
   "metadata": {},
   "source": [
    "2. ë°ì´í„° ì •ì œ\n",
    "- í•„í„°ë§ : í†µí•©ëœ ë°ì´í„°ì…‹(33,155ê°œ)ì—ì„œ 'prompt'ë‚˜ 'completion'ì˜ ê¸¸ì´ê°€ 5ê¸€ì ì´í•˜ì¸ ì§§ì€ ë°ì´í„°ë“¤ì„ ì œê±°í•˜ì—¬, ìµœì¢…ì ìœ¼ë¡œ 32,906ê°œì˜ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ë„ë¡ ì •ì œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16ca3ff-47b0-42ac-b437-c957e744a453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5db15c546614fa5ab4390c4bdfd672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/33155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ì œ ì „ ë°ì´í„°ì…‹ í¬ê¸°: 33155\n",
      "ì •ì œ í›„ ë°ì´í„°ì…‹ í¬ê¸°: 32906\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ì„ ì •ì œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "def refine_dataset(example):\n",
    "    if example['prompt'] is None or example['completion'] is None:\n",
    "        return False\n",
    "    return len(example['prompt']) > 5 and len(example['completion']) > 5\n",
    "\n",
    "# 'combined_data'ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "refined_data = combined_data.filter(refine_dataset, num_proc=4)\n",
    "\n",
    "print(f\"ì •ì œ ì „ ë°ì´í„°ì…‹ í¬ê¸°: {len(combined_data)}\")\n",
    "print(f\"ì •ì œ í›„ ë°ì´í„°ì…‹ í¬ê¸°: {len(refined_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ddb04",
   "metadata": {},
   "source": [
    "3. ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ë° ì‹¤í–‰ âš™ï¸\n",
    "\n",
    "- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ : ì²« ë²ˆì§¸ ì‹¤í—˜ê³¼ ë™ì¼í•˜ê²Œ skt/kogpt2-base-v2 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "- ë°ì´í„° ì „ì²˜ë¦¬ : í†µí•©ë˜ê³  ì •ì œëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì í˜•íƒœ(í† í°)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ë•Œ ëª¨ë¸ì´ 'completion'(ì‘ë‹µ) ë¶€ë¶„ë§Œ í•™ìŠµí•˜ë„ë¡ 'prompt'(ì§€ì‹œ) ë¶€ë¶„ì€ ë ˆì´ë¸”ì—ì„œ ì œì™¸(-100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹)í•©ë‹ˆë‹¤.\n",
    "- ëª¨ë¸ í•™ìŠµ : Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Trainerë¥¼ ì‚¬ìš©í•´ ì¶”ê°€ëœ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •(SFT) í•™ìŠµì„ ì§„í–‰í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f9de1-67ef-4955-a5ac-b92bb69fd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoGPT2 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='</s>', eos_token='</s>', pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515689d-33e3-43d5-81f7-899c30d633b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í† í¬ë‚˜ì´ì§•í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "def preprocess(example):\n",
    "    # BOS(Beginning of Sentence)ì™€ EOS(End of Sentence) í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    prompt = base_tokenizer.bos_token + example['prompt']\n",
    "    completion = example['completion'] + base_tokenizer.eos_token\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ì™€ ì •ë‹µì„ í•©ì³ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    full_text = prompt + completion\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    encodings = base_tokenizer(full_text, truncation=True, padding='max_length', max_length=256)\n",
    "    \n",
    "    # ë ˆì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ loss ê³„ì‚°ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•´ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.\n",
    "    labels = encodings['input_ids'][:]\n",
    "    prompt_tokens = len(base_tokenizer(prompt, truncation=True)['input_ids'])\n",
    "    labels[:prompt_tokens] = [-100] * prompt_tokens\n",
    "    \n",
    "    encodings['labels'] = labels\n",
    "    return encodings\n",
    "\n",
    "# ì •ì œëœ ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "processed_data = refined_data.map(preprocess, remove_columns=refined_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ff9e6-5856-448a-88a0-94877f09f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµì„ ìœ„í•œ ì¸ìë“¤ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft-output\",\n",
    "    num_train_epochs=3, # ì‹œê°„ ê´€ê³„ìƒ 1 ì—í­ë§Œ ì„¤ì •, ì‹¤ì œë¡œëŠ” ë” ë§ì€ í•™ìŠµ í•„ìš”\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# SFTë¥¼ ìˆ˜í–‰í•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
    "\n",
    "# Trainer ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "trainer = Trainer(\n",
    "    model=sft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_data,\n",
    ")\n",
    "\n",
    "# í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb561be",
   "metadata": {},
   "source": [
    "4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ ğŸ”\n",
    "\n",
    "- ì •ì„±ì  í‰ê°€ (ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜) : \n",
    "    - ì‚¬ìš©ìê°€ ì§ì ‘ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ë©´, ì›ë³¸ KoGPT-2 ëª¨ë¸ê³¼ ìƒˆë¡­ê²Œ ë¯¸ì„¸ ì¡°ì •í•œ SFT ëª¨ë¸ì´ ê°ê° ìƒì„±í•œ ë‹µë³€ì„ ë‚˜ë€íˆ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "    - íŠ¹íˆ SFT ëª¨ë¸ì€ Greedy, Beam Search, Top-k Sampling ë“± ë‹¤ì–‘í•œ ìƒì„± ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ë§Œë“¤ì–´ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ ë¹„êµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì •ëŸ‰ì  í‰ê°€ (ROUGE ì ìˆ˜) :\n",
    "    - ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ê³¼ ì‹¤ì œ ì •ë‹µì´ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ROUGE ì ìˆ˜ë¼ëŠ” ì§€í‘œë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "    - ROUGE-1, ROUGE-2, ROUGE-L ì ìˆ˜ë¥¼ í†µí•´ ë‹¨ì–´, ì–´êµ¬, ë¬¸ì¥ êµ¬ì¡°ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° ìˆ˜ì¹˜ë¡œ í‰ê°€í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3862791-1b2d-462b-8707-827155d78a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def generate_answer(model, tokenizer, prompt, generation_type, max_length=128):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    output_sequences = None\n",
    "    if generation_type == \"greedy\":\n",
    "        output_sequences = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "    elif generation_type == \"beam_search\":\n",
    "        output_sequences = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    elif generation_type == \"top_k_sampling\":\n",
    "        output_sequences = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)\n",
    "\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "# ëª¨ë¸ë“¤ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "base_model.eval()\n",
    "sft_model.eval()\n",
    "\n",
    "# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ë©´ ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "sft_model.to(device)\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ë‹µë³€ ìƒì„± ë° ë¹„êµ\n",
    "while True:\n",
    "    prompt = input(\"í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): \")\n",
    "    if prompt.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"[ì…ë ¥ í”„ë¡¬í”„íŠ¸]: {prompt}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # ì›ë³¸ KoGPT-2 ë‹µë³€\n",
    "    base_answer = generate_answer(base_model, base_tokenizer, prompt, \"greedy\")\n",
    "    print(f\"[KoGPT-2 ì›ë³¸]: {base_answer}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    # SFT ëª¨ë¸ ë‹µë³€\n",
    "    sft_greedy = generate_answer(sft_model, sft_tokenizer, prompt, \"greedy\")\n",
    "    sft_beam = generate_answer(sft_model, sft_tokenizer, prompt, \"beam_search\")\n",
    "    sft_top_k = generate_answer(sft_model, sft_tokenizer, prompt, \"top_k_sampling\")\n",
    "    print(f\"[SFT - Greedy]: {sft_greedy}\")\n",
    "    print(f\"[SFT - Beam Search]: {sft_beam}\")\n",
    "    print(f\"[SFT - Top-k Sampling]: {sft_top_k}\")\n",
    "    print(\"--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ecb2f-31b5-4837-abce-ef2f734db732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE í‰ê°€ ì§€í‘œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì¼ë¶€ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. (ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ 100ê°œë§Œ ì‚¬ìš©)\n",
    "eval_dataset = refined_data.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# SFT ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì •ë‹µì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for item in eval_dataset:\n",
    "    prompt = item['prompt']\n",
    "    reference = item['completion']\n",
    "    \n",
    "    # SFT ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±\n",
    "    prediction = generate_answer(sft_model, sft_tokenizer, prompt, \"greedy\")\n",
    "    # ìƒì„±ëœ ë‹µë³€ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤ (ìˆœìˆ˜ ìƒì„± ë¶€ë¶„ë§Œ ë‚¨ê¸°ê¸° ìœ„í•¨).\n",
    "    if prediction.startswith(prompt):\n",
    "        prediction = prediction[len(prompt):]\n",
    "        \n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "    \n",
    "# ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "# nltk.sent_tokenizeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ í›„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references, tokenizer=lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"--- ì •ëŸ‰ì  í‰ê°€ (ROUGE Score) ---\")\n",
    "print(f\"ROUGE-1: {rouge_results['rouge1'] * 100:.2f}\") # Unigrams (ë‹¨ì¼ ë‹¨ì–´) ê¸°ë°˜ ìœ ì‚¬ë„\n",
    "print(f\"ROUGE-2: {rouge_results['rouge2'] * 100:.2f}\") # Bigrams (ì—°ì†ëœ ë‘ ë‹¨ì–´) ê¸°ë°˜ ìœ ì‚¬ë„\n",
    "print(f\"ROUGE-L: {rouge_results['rougeL'] * 100:.2f}\") # Longest Common Subsequence (ê°€ì¥ ê¸´ ê³µí†µ ë¶€ë¶„ ë¬¸ìì—´) ê¸°ë°˜ ìœ ì‚¬ë„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
