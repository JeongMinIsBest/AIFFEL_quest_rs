{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLwv1vpCqTAx"
   },
   "source": [
    "# Transformer (Ko-En) 번역기 Baseline\n",
    "---\n",
    "### 프로젝트 목표\n",
    "**주요 변경 사항 (baseline 대비):**\n",
    "\n",
    "1. **데이터 증강 (Back-translation)** 추가:\n",
    "      - Meta AI의 NLLB-200 모델을 이용하여 한국어 문장을 영어로 번역한 뒤, 다시 한국어로 번역합니다.\n",
    "      - 이렇게 얻은 역번역 문장은 원래 문장과는 표현이 다르지만 의미는 동일합니다.\n",
    "      - 원본 한국어-영어 쌍 데이터에 이 역번역된 한국어 문장을 추가해, 모델이 더 다양한 표현을 학습하도록 합니다.\n",
    "      - 데이터셋의 일부(1/10)를 증강하여, 학습 데이터의 크기를 약 1.1배로 확장했습니다.\n",
    "2. **형태소 분석기** 적용 (MeCab):\n",
    "      - 한국어 문장을 MeCab 형태소 분석기로 전처리하여, 어절 단위가 아닌 형태소 단위로 토큰화합니다.\n",
    "      - 이를 통해 한국어 문법적 특성을 더 잘 반영하고 희소성을 줄여 모델 학습 효율을 높입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y74p5QWDqTAz"
   },
   "source": [
    "## 1. 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17002,
     "status": "ok",
     "timestamp": 1757535981017,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "tzhW0KLmqTA0",
    "outputId": "7d4b6b86-496c-449a-fe25-4e47eb1d482d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 8761,
     "status": "ok",
     "timestamp": 1757535989795,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "HMte5yyKqTA1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLd8XbwlqTA1"
   },
   "source": [
    "## 2. 하이퍼파라미터 및 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQDg_LnIqTA1"
   },
   "source": [
    "- Transformer 논문(Attention is All You Need) 기본 구조와 유사한 설정\n",
    "    - d_model=512, n_heads=8, n_layers=6 → 표준 Transformer base 모델 크기\n",
    "    - d_ff=2048 → Position-wise FFN 내부 차원\n",
    "    - max_len=50 → 문장 최대 길이 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1757535989846,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "yGkdLKyGqTA1",
    "outputId": "34b72a4a-6d43-4b5f-bd93-bc68b00f8509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "SRC_VOCAB_SIZE = 20000\n",
    "TGT_VOCAB_SIZE = 20000\n",
    "D_MODEL = 512\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Training Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dWEfmBWqTA1"
   },
   "source": [
    "## 3. 데이터 준비 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1757535990013,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "P_8EMpzGqTA1",
    "outputId": "33d024ee-f7c0-45a7-84af-626312bc2822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.dev.en\n",
      "korean-english-park.dev.ko\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ~/work/0908/data/korean-english-park.dev.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1757535990093,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "FjNtFBx2qTA2",
    "outputId": "43dc821c-7ae7-4b71-ed74-2b407ba184ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.test.en\n",
      "korean-english-park.test.ko\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ~/work/0908/data/korean-english-park.test.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1757535990504,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "gGdz3gPQqTA2",
    "outputId": "7c36d09b-83b5-45a2-fe0e-fcf62243d1b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.train.en\n",
      "korean-english-park.train.ko\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ~/work/0908/data/korean-english-park.train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1757535990845,
     "user": {
      "displayName": "Jenny Lim",
      "userId": "17264280714358384713"
     },
     "user_tz": -540
    },
    "id": "XN5kR-aFqTA2",
    "outputId": "bd8541d1-1497-4678-9a46-06d4001baad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 94123, Dev: 1000, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 경로 설정\n",
    "data_dir = '.'\n",
    "train_kor_path = os.path.join(data_dir, 'korean-english-park.train.ko')\n",
    "train_eng_path = os.path.join(data_dir, 'korean-english-park.train.en')\n",
    "dev_kor_path = os.path.join(data_dir, 'korean-english-park.dev.ko')\n",
    "dev_eng_path = os.path.join(data_dir, 'korean-english-park.dev.en')\n",
    "test_kor_path = os.path.join(data_dir, 'korean-english-park.test.ko')\n",
    "test_eng_path = os.path.join(data_dir, 'korean-english-park.test.en')\n",
    "\n",
    "# 2. 원본 데이터 로딩\n",
    "with open(train_kor_path, \"r\", encoding='utf-8') as f: train_kor_raw = f.read().splitlines()\n",
    "with open(train_eng_path, \"r\", encoding='utf-8') as f: train_eng_raw = f.read().splitlines()\n",
    "with open(dev_kor_path, \"r\", encoding='utf-8') as f: dev_kor_raw = f.read().splitlines()\n",
    "with open(dev_eng_path, \"r\", encoding='utf-8') as f: dev_eng_raw = f.read().splitlines()\n",
    "with open(test_kor_path, \"r\", encoding='utf-8') as f: test_kor_raw = f.read().splitlines()\n",
    "with open(test_eng_path, \"r\", encoding='utf-8') as f: test_eng_raw = f.read().splitlines()\n",
    "\n",
    "print(f\"Train: {len(train_kor_raw)}, Dev: {len(dev_kor_raw)}, Test: {len(test_kor_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8X8O5qkqTA2",
    "outputId": "f9afc71a-8d20-4b65-a56f-f6f25657be56",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "데이터셋의 1/10인 9412개 문장에 대해 역번역을 시작합니다...\n",
      "총 9412개의 문장을 역번역합니다...\n",
      "[1000/9412] 문장 처리 완료 | 경과 시간: 328.37초\n",
      "[2000/9412] 문장 처리 완료 | 경과 시간: 679.11초\n",
      "[3000/9412] 문장 처리 완료 | 경과 시간: 1102.99초\n",
      "[4000/9412] 문장 처리 완료 | 경과 시간: 1488.22초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/9412] 문장 처리 완료 | 경과 시간: 1982.35초\n",
      "[6000/9412] 문장 처리 완료 | 경과 시간: 2396.60초\n",
      "[7000/9412] 문장 처리 완료 | 경과 시간: 2831.76초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000/9412] 문장 처리 완료 | 경과 시간: 3337.60초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 200 is bigger than 0.9 * max_length: 200. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9000/9412] 문장 처리 완료 | 경과 시간: 3767.35초\n",
      "[9412/9412] 문장 처리 완료 | 경과 시간: 3905.09초\n",
      "데이터 증강 완료\n",
      "최종 학습 데이터 크기: 한국어 103535, 영어 103535\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "# (이전에 정의한 back_translate_nllb_optimized 함수를 그대로 사용)\n",
    "def back_translate_nllb_optimized(sentences, chunk_size=1000, batch_size=32):\n",
    "    \"\"\"\n",
    "    역번역을 진행하면서 진행 상황을 출력하고, 효율성을 높이는 함수.\n",
    "    \"\"\"\n",
    "    augmented_sentences = []\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    print(f\"총 {total_sentences}개의 문장을 역번역합니다...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, total_sentences, chunk_size):\n",
    "        chunk_start = i\n",
    "        chunk_end = min(i + chunk_size, total_sentences)\n",
    "\n",
    "        translated_to_en = [\n",
    "            result['translation_text'] for result in translator(sentences[chunk_start:chunk_end],\n",
    "                                                               src_lang='kor_Hang',\n",
    "                                                               tgt_lang='eng_Latn',\n",
    "                                                               batch_size=batch_size)\n",
    "        ]\n",
    "\n",
    "        back_translated_to_ko = [\n",
    "            result['translation_text'] for result in translator(translated_to_en,\n",
    "                                                               src_lang='eng_Latn',\n",
    "                                                               tgt_lang='kor_Hang',\n",
    "                                                               batch_size=batch_size)\n",
    "        ]\n",
    "\n",
    "        augmented_sentences.extend(back_translated_to_ko)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"[{chunk_end}/{total_sentences}] 문장 처리 완료 | 경과 시간: {elapsed_time:.2f}초\")\n",
    "\n",
    "    return augmented_sentences\n",
    "\n",
    "# NLLB 모델 로드\n",
    "translator = pipeline(\n",
    "    'translation',\n",
    "    model='facebook/nllb-200-distilled-600M',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# 데이터 증강을 적용할 데이터의 양을 결정\n",
    "# 전체 학습 데이터의 1/10만 사용\n",
    "num_to_augment = len(train_kor_raw) // 10\n",
    "\n",
    "train_kor_subset = train_kor_raw[:num_to_augment]\n",
    "train_eng_subset = train_eng_raw[:num_to_augment]\n",
    "\n",
    "print(f\"\\n데이터셋의 1/10인 {num_to_augment}개 문장에 대해 역번역을 시작합니다...\")\n",
    "augmented_train_kor_raw = back_translate_nllb_optimized(train_kor_subset)\n",
    "print(\"데이터 증강 완료\")\n",
    "\n",
    "# 원본 데이터와 증강된 데이터를 합치기\n",
    "# 증강된 데이터는 원본 문장의 1/10에 해당하므로, 총 데이터 크기는 1.1배가 됨\n",
    "combined_train_kor_raw = train_kor_raw + augmented_train_kor_raw\n",
    "combined_train_eng_raw = train_eng_raw + train_eng_subset\n",
    "\n",
    "print(f\"최종 학습 데이터 크기: 한국어 {len(combined_train_kor_raw)}, 영어 {len(combined_train_eng_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0wj87R9qTA2"
   },
   "source": [
    "- 중복된 문장 쌍 제거\n",
    "- 소문자화 + 구두점 분리 + 특수문자 제거(. ? ! , 만 남김)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vgHORaeIqTA3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정제 및 전처리 후 최종 데이터 크기: Train: 88369, Dev: 1000, Test: 1996\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"구두점, 특수문자 등 불필요한 부분을 제거하고 소문자로 변환합니다.\"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\\\" \\\"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def clean_and_preprocess_corpus(kor_raw, eng_raw):\n",
    "    \"\"\"문장 쌍의 중복을 제거하고 각 문장을 전처리합니다.\"\"\"\n",
    "    # 1. zip으로 문장 쌍 생성 후 set으로 중복 제거\n",
    "    cleaned_pairs = list(set(zip(kor_raw, eng_raw)))\n",
    "\n",
    "    # 2. 각 문장 전처리\n",
    "    kor_corpus, eng_corpus = [], []\n",
    "    for kor, eng in cleaned_pairs:\n",
    "        kor_corpus.append(preprocess_sentence(kor))\n",
    "        eng_corpus.append(preprocess_sentence(eng))\n",
    "\n",
    "    return kor_corpus, eng_corpus\n",
    "\n",
    "# 증강된 데이터를 포함한 최종 학습 데이터에 대해 정제 및 전처리 수행\n",
    "train_kor_corpus, train_eng_corpus = clean_and_preprocess_corpus(combined_train_kor_raw, combined_train_eng_raw)\n",
    "dev_kor_corpus, dev_eng_corpus = clean_and_preprocess_corpus(dev_kor_raw, dev_eng_raw)\n",
    "test_kor_corpus, test_eng_corpus = clean_and_preprocess_corpus(test_kor_raw, test_eng_raw)\n",
    "\n",
    "print(f\"\\n정제 및 전처리 후 최종 데이터 크기: Train: {len(train_kor_corpus)}, Dev: {len(dev_kor_corpus)}, Test: {len(test_kor_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Spg4ZwnruR9l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
      "Downloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, JPype1, konlpy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [konlpy]2m2/3\u001b[0m [konlpy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed JPype1-1.6.0 konlpy-0.6.0 lxml-6.0.1\n",
      "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
      "remote: Enumerating objects: 138, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91 (from 1)\u001b[K\n",
      "Receiving objects: 100% (138/138), 1.72 MiB | 7.38 MiB/s, done.\n",
      "Resolving deltas: 100% (65/65), done.\n",
      "/home/jovyan/work/0908/Mecab-ko-for-Google-Colab\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 4: cd: /content: No such file or directory\n",
      "Installing konlpy.....\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Done\n",
      "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
      "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "--2025-09-10 22:28:14--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "Resolving bitbucket.org (bitbucket.org)... 13.200.41.135, 13.200.41.134, 13.200.41.136, ...\n",
      "Connecting to bitbucket.org (bitbucket.org)|13.200.41.135|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNCIQ2ESHQ&Signature=KO597Wwz%2FY1QE3CvRbWEHW7MTuc%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEI%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICTRl%2BIgA2%2FuuNBPFZi8FATAPPfSs%2BbAhU%2Bc83SXAYXqAiEA3BGqez%2FF7pmDz9VF1%2BsYLWcosojfxytlZCJh0KzfgMAqsAII%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw5ODQ1MjUxMDExNDYiDBGkKnZ49dYQb3sdQSqEAgpj8MWcGIGPHQvhK39yXvXl%2Fnx8HaxIeksv%2B19NZ4JjpJuf%2BlVqlzWurqCxQLBZJbn595uchQecZLGiCe5Y8C%2FVcKvrQg8T4LAE9RtHMUIn%2BKY3AMDVp2YETMtd9tVlLyCnsm61YVgI%2FOS%2BK3h81Yn1Ol4XMHeaptFpdvlQ7c653%2BkVudklYFzgzmt3HcmyZDN5rqEp8wEsZgqWiJBYUWr5tFI%2FFdcCsmtw21MbyvgBGAk6acvmFKu94w1JeAyxIjIHqbZwOTGTyW4Bfp6hJDU1urstsKKQuweMAK5rMMi8ENMs5j6KWYyMn7k1WiX3a%2FW66%2BQXYzxo%2Fu3Z6%2FbmXUMksEPUMP%2F2h8YGOp0BBrfeY4AbP34ijzLnfL6QdQJex%2FZm3PVAILekt5hfHN9sTeUjA0jhtKwoG1KhGfMpsCqS5Em8NHTIA5hT5JFudIN%2BaYID6RosH5LLhc0rDAzSkZl5NPu5JhQm40aMi%2BbTcUr8NOD4j6bRyJ3YMkzpHLK2Qi5fT6A39vJatbBXVuHy%2FW%2BXdhOC8Glt25qmI958yp4Usf8%2FgY3VWZhPKQ%3D%3D&Expires=1757545096 [following]\n",
      "--2025-09-10 22:28:16--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNCIQ2ESHQ&Signature=KO597Wwz%2FY1QE3CvRbWEHW7MTuc%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEI%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICTRl%2BIgA2%2FuuNBPFZi8FATAPPfSs%2BbAhU%2Bc83SXAYXqAiEA3BGqez%2FF7pmDz9VF1%2BsYLWcosojfxytlZCJh0KzfgMAqsAII%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw5ODQ1MjUxMDExNDYiDBGkKnZ49dYQb3sdQSqEAgpj8MWcGIGPHQvhK39yXvXl%2Fnx8HaxIeksv%2B19NZ4JjpJuf%2BlVqlzWurqCxQLBZJbn595uchQecZLGiCe5Y8C%2FVcKvrQg8T4LAE9RtHMUIn%2BKY3AMDVp2YETMtd9tVlLyCnsm61YVgI%2FOS%2BK3h81Yn1Ol4XMHeaptFpdvlQ7c653%2BkVudklYFzgzmt3HcmyZDN5rqEp8wEsZgqWiJBYUWr5tFI%2FFdcCsmtw21MbyvgBGAk6acvmFKu94w1JeAyxIjIHqbZwOTGTyW4Bfp6hJDU1urstsKKQuweMAK5rMMi8ENMs5j6KWYyMn7k1WiX3a%2FW66%2BQXYzxo%2Fu3Z6%2FbmXUMksEPUMP%2F2h8YGOp0BBrfeY4AbP34ijzLnfL6QdQJex%2FZm3PVAILekt5hfHN9sTeUjA0jhtKwoG1KhGfMpsCqS5Em8NHTIA5hT5JFudIN%2BaYID6RosH5LLhc0rDAzSkZl5NPu5JhQm40aMi%2BbTcUr8NOD4j6bRyJ3YMkzpHLK2Qi5fT6A39vJatbBXVuHy%2FW%2BXdhOC8Glt25qmI958yp4Usf8%2FgY3VWZhPKQ%3D%3D&Expires=1757545096\n",
      "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 54.231.235.209, 3.5.13.144, 16.15.184.186, ...\n",
      "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|54.231.235.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1414979 (1.3M) [application/x-tar]\n",
      "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
      "\n",
      "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.24MB/s    in 1.1s    \n",
      "\n",
      "2025-09-10 22:28:18 (1.24 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
      "\n",
      "Done\n",
      "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-0.996-ko-0.9.2.......\n",
      "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
      "configure\n",
      "make\n",
      "make check\n",
      "make install\n",
      "ldconfig\n",
      "Done\n",
      "Change Directory to /content\n",
      "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "--2025-09-10 22:30:08--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "Resolving bitbucket.org (bitbucket.org)... 104.192.143.23, 104.192.143.22, 104.192.143.21, ...\n",
      "Connecting to bitbucket.org (bitbucket.org)|104.192.143.23|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNII6JI3TD&Signature=6DN0uPhgsbo4eMGzhbN2n068S8M%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEI%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGaYvkGGE2jWFODZAC6KebBVFZKiaN%2BPA8mQRL%2BZVYP3AiEA4H%2BIMamxeqAWHysWfSGnBh1qSyYYOw5w6u8ARpxeFpAqsAII%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw5ODQ1MjUxMDExNDYiDA69ygbwAylhGNtCuSqEAvau47kFEpLTdMzlx9jwX4v%2BRN7vHjs25prUpXiUa8%2FM1DmBby%2F2noxZypFs8HRFvYRZ0PBpnoRcX%2Fpsno9V51jQ8O9PoDjE8Mt5I3QTvOSobf0VPQWiHTwD8zdTBQyhq7FuGvxzE%2F%2FTEX4z3OTwxLx7gAHe0RVvFp5wZS%2BSKSozeJf7wPEzJunSMIT%2B6ZGQ8x5Wy6u1k6tslpd6k%2FdXaoTHa2KLW8oLu%2BtqgNPmLzWj2n7MtnVHI0N%2FsaqVsJcLS2VXIL3yOFmmKVZEnr6nII5U9aBYywfyivRS4lx3BHg7kQqINnH5ed%2FLp5iQrTnyBzHj3r51rktEtt1jE7YRRjACGaMQMPH3h8YGOp0BIdLumbFiTaAdQeB2G%2FEi%2Ff83chYVGAPchP5C6fI2Y06RElgVFXTm5MDzYOBdLG2nmqGNQC43ZXihB05VWnU45YchJP45jHfme1ewa8k%2FA2Ad%2B9v2UwPexeTTTUBh7Aorql%2Fvw%2F2rVtIy6Uq%2BoVNoMob2yIQhcZsC%2Fj%2BeV%2F7qf3si4dSHhuMs0bYHddpqiX4E%2BflXqs5IZOwwvA6U%2Fg%3D%3D&Expires=1757545209 [following]\n",
      "--2025-09-10 22:30:09--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNII6JI3TD&Signature=6DN0uPhgsbo4eMGzhbN2n068S8M%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEI%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGaYvkGGE2jWFODZAC6KebBVFZKiaN%2BPA8mQRL%2BZVYP3AiEA4H%2BIMamxeqAWHysWfSGnBh1qSyYYOw5w6u8ARpxeFpAqsAII%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw5ODQ1MjUxMDExNDYiDA69ygbwAylhGNtCuSqEAvau47kFEpLTdMzlx9jwX4v%2BRN7vHjs25prUpXiUa8%2FM1DmBby%2F2noxZypFs8HRFvYRZ0PBpnoRcX%2Fpsno9V51jQ8O9PoDjE8Mt5I3QTvOSobf0VPQWiHTwD8zdTBQyhq7FuGvxzE%2F%2FTEX4z3OTwxLx7gAHe0RVvFp5wZS%2BSKSozeJf7wPEzJunSMIT%2B6ZGQ8x5Wy6u1k6tslpd6k%2FdXaoTHa2KLW8oLu%2BtqgNPmLzWj2n7MtnVHI0N%2FsaqVsJcLS2VXIL3yOFmmKVZEnr6nII5U9aBYywfyivRS4lx3BHg7kQqINnH5ed%2FLp5iQrTnyBzHj3r51rktEtt1jE7YRRjACGaMQMPH3h8YGOp0BIdLumbFiTaAdQeB2G%2FEi%2Ff83chYVGAPchP5C6fI2Y06RElgVFXTm5MDzYOBdLG2nmqGNQC43ZXihB05VWnU45YchJP45jHfme1ewa8k%2FA2Ad%2B9v2UwPexeTTTUBh7Aorql%2Fvw%2F2rVtIy6Uq%2BoVNoMob2yIQhcZsC%2Fj%2BeV%2F7qf3si4dSHhuMs0bYHddpqiX4E%2BflXqs5IZOwwvA6U%2Fg%3D%3D&Expires=1757545209\n",
      "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.76.132, 16.15.217.147, 54.231.234.241, ...\n",
      "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.76.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49775061 (47M) [application/x-tar]\n",
      "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
      "\n",
      "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  13.7MB/s    in 3.5s    \n",
      "\n",
      "2025-09-10 22:30:14 (13.7 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
      "\n",
      "Done\n",
      "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
      "Done\n",
      "installing........\n",
      "configure\n",
      "make\n",
      "make install\n",
      "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)\n",
      "https://github.com/konlpy/konlpy/issues/395#issue-1099168405 - 2022.01.11\n",
      "Done\n",
      "Install mecab-python\n",
      "Successfully Installed\n",
      "Now you can use Mecab\n",
      "from konlpy.tag import Mecab\n",
      "mecab = Mecab()\n",
      "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
      "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
      "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n",
      "light 버전 작성 : Dogdriip님 ( https://github.com/Dogdriip )\n",
      "문제를 해결해주신 combacsa님 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "#mecab 설치\n",
    "\n",
    "!pip install konlpy\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab/\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "HgDbeWR-uTrL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 야사미 리처슨 소방방재청 대변인은 일 태풍으로 엔터프라이즈와 윌콕스 카운티에 각각 한 명씩 명의 사망자가 더 있다고 밝혔다 .\n",
      "After : 야 사미 리처 슨 소방방 재청 대변인 은 일 태풍 으로 엔터 프라이즈 와 윌콕스 카운티 에 각각 한 명 씩 명 의 사망자 가 더 있 다고 밝혔 다 .\n"
     ]
    }
   ],
   "source": [
    "#=========================================================\n",
    "#================추가된 Mecab 단계=========================\n",
    "#=========================================================\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 1. Mecab 초기화\n",
    "mecab = Mecab()\n",
    "def mecab_tokenize_corpus(corpus):\n",
    "    mecab_corpus = []\n",
    "    for sentence in corpus:\n",
    "        # 형태소 분리 후 공백으로 join\n",
    "        morphs = mecab.morphs(sentence)\n",
    "        mecab_corpus.append(\" \".join(morphs))\n",
    "    return mecab_corpus\n",
    "\n",
    "# 2. 한국어 데이터셋 Mecab 처리\n",
    "train_kor_mecab = mecab_tokenize_corpus(train_kor_corpus)\n",
    "dev_kor_mecab   = mecab_tokenize_corpus(dev_kor_corpus)\n",
    "test_kor_mecab  = mecab_tokenize_corpus(test_kor_corpus)\n",
    "\n",
    "print(\"Before:\", train_kor_corpus[0])\n",
    "print(\"After :\", train_kor_mecab[0])\n",
    "#=========================================================\n",
    "#=========================================================\n",
    "#========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_FDF7r-uWjt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAxb5AhNqTA3"
   },
   "source": [
    "- SentencePiece 토크나이저 학습 + 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gb5EqPd6qTA3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./ko_corpus.txt --model_prefix=ko_spm --vocab_size=20000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./ko_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./ko_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 88368 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=6700186\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1148\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 88368 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=2540512\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 38702 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 88368\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 51918\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 51918 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=27188 obj=8.72225 num_tokens=100662 num_tokens/piece=3.70244\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21635 obj=8.1988 num_tokens=101075 num_tokens/piece=4.67183\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: ko_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: ko_spm.vocab\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en_corpus.txt --model_prefix=en_spm --vocab_size=20000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: en_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./en_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 88357 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=11846606\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9905% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999905\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 88357 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=6739900\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 82992 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 88357\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=35366 obj=9.8665 num_tokens=83334 num_tokens/piece=2.35633\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=26269 obj=8.00996 num_tokens=83712 num_tokens/piece=3.18672\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21988 obj=7.9272 num_tokens=84758 num_tokens/piece=3.85474\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21887 obj=7.90799 num_tokens=84935 num_tokens/piece=3.88061\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: en_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: en_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "\n",
    "def generate_tokenizer(corpus, vocab_size, lang, pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    file = f'./{lang}_corpus.txt'\n",
    "    model_prefix = f'{lang}_spm'\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={file} --model_prefix={model_prefix} --vocab_size={vocab_size}' +\n",
    "        f' --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'{model_prefix}.model')\n",
    "    return tokenizer\n",
    "\n",
    "# 증강된 데이터로 SentencePiece 토크나이저 재학습\n",
    "SRC_VOCAB_SIZE = 20000\n",
    "TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "# 한국어만 mecab으로 영어는 corpus로만 수정함\n",
    "ko_tokenizer = generate_tokenizer(train_kor_mecab, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(train_eng_corpus, TGT_VOCAB_SIZE, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Yq9cHBbqTA3"
   },
   "source": [
    "- 입력 파일 : ko_corpus.txt\n",
    "- 결과 파일 prefix: ko_spm → ko_spm.model, ko_spm.vocab 생성\n",
    "- vocab 크기 : 20,000\n",
    "- 특수 토큰 ID : pad=0, bos=1, eos=2, unk=3\n",
    "- 영어도 동일하게 en_corpus.txt → en_spm.model, en_spm.vocab 으로 학습\n",
    "<br/>\n",
    "\n",
    "- 모델 유형 : UNIGRAM → BPE 말고 유니그램 기반 서브워드 모델\n",
    "- 문자 커버리지 : 99.95% 문자를 커버 → 드물게 쓰이는 문자는 버림\n",
    "- 최대 문장 길이: 4192자\n",
    "- 최대 서브워드 길이: 16자\n",
    "- 스레드 수 : 병렬 학습을 위해 16개 스레드 사용\n",
    "<br/>\n",
    "\n",
    "- 학습에 사용된 문장 수 : 94,123개 (한/영 동일)\n",
    "- ```<pad>, <s>, </s>, <unk>``` 토큰을 vocab에 고정 추가\n",
    "<br/>\n",
    "\n",
    "- 한국어\n",
    "    - 전체 문자 개수 : 약 581만\n",
    "    - 고유 문자 수 : 1,324종 (한글 + 특수기호 포함)\n",
    "    - 커버율 : 99.95% → corpus 대부분의 문자가 반영됨\n",
    "- 영어\n",
    "    - 전체 문자 개수: 약 1,196만\n",
    "    - 고유 문자 수: 82종 (영문 알파벳 + 기호)\n",
    "<br/>\n",
    "\n",
    "- EM (Expectation-Maximization) 반복을 통해 서브워드를 점점 줄여감\n",
    "- size : 남은 서브워드 후보 개수\n",
    "- obj : likelihood 값 (클수록 모델이 데이터를 잘 설명함)\n",
    "- num_tokens : 전체 토큰화 결과 개수\n",
    "- num_tokens/piece : 평균적으로 하나의 서브워드가 몇 번 등장했는지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABs8qUMeqTA3"
   },
   "source": [
    "## 5. 데이터셋 및 DataLoader 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpgSB1l4qTA3"
   },
   "source": [
    "- 번역 데이터셋을 PyTorch에서 학습 가능한 형태로 구성하는 과정\n",
    "- 문장 쌍을 토큰화하여 텐서로 변환\n",
    "- 배치마다 패딩 처리\n",
    "- DataLoader로 학습용 배치 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z4evU9wjqTA3"
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset 및 DataLoader 인스턴스 생성\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# TranslationDataset 클래스 정의\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_corpus, tgt_corpus, src_tokenizer, tgt_tokenizer):\n",
    "        self.src_corpus = src_corpus\n",
    "        self.tgt_corpus = tgt_corpus\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_tokenizer.encode_as_ids(self.src_corpus[idx])\n",
    "        tgt = self.tgt_tokenizer.encode_as_ids(self.tgt_corpus[idx])\n",
    "\n",
    "        # 텐서의 데이터 타입을 torch.long으로 명시적으로 지정\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "# collate_fn 함수\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # 여기에 SOS/EOS 토큰을 추가하거나, 이미 토크나이저에서 처리했다면 그대로 사용\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=ko_tokenizer.pad_id())\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=en_tokenizer.pad_id())\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# (Mecab으로 전처리된) 증강된 데이터셋으로 TranslationDataset 인스턴스 생성\n",
    "train_dataset = TranslationDataset(train_kor_mecab, train_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "valid_dataset = TranslationDataset(dev_kor_mecab, dev_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "test_dataset  = TranslationDataset(test_kor_mecab, test_eng_corpus, ko_tokenizer, en_tokenizer)\n",
    "\n",
    "# DataLoader 인스턴스 생성 (변경 없음)\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO18FPjQqTA4"
   },
   "source": [
    "## 6. 트랜스포머 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "i9D2szXtqTA4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    입력 임베딩에 위치 정보를 추가하는 클래스입니다.\n",
    "    Transformer 모델은 순서 정보가 없으므로, 토큰의 위치를 알려주기 위해 sin/cos 함수를 사용합니다.\n",
    "    이 방식은 고정 위치 인코딩으로, 학습되지 않는 파라미터(buffer)로 등록됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # sin/cos 함수에 사용할 div_term 계산: 주파수 조절을 위한 값\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(10000.0) / emb_size))\n",
    "        # 각 위치(0~maxlen)에 대한 인덱스 생성\n",
    "        position = torch.arange(maxlen).unsqueeze(1)\n",
    "        # 위치 임베딩 행렬 초기화 (maxlen, emb_size)\n",
    "        pos_embedding = torch.zeros(maxlen, emb_size)\n",
    "        # 짝수 인덱스: sin 함수 적용\n",
    "        pos_embedding[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 홀수 인덱스: cos 함수 적용\n",
    "        pos_embedding[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 배치 차원 추가 (1, maxlen, emb_size)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 학습되지 않는 파라미터로 등록\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_embedding: (batch_size, seq_len, emb_size)\n",
    "        Returns:\n",
    "            token_embedding + pos_embedding: 위치 정보가 더해진 임베딩\n",
    "        \"\"\"\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    다중 헤드 어텐션 메커니즘을 구현한 클래스.\n",
    "    쿼리, 키, 값 행렬을 여러 헤드로 분할하여 병렬로 어텐션을 계산하고, 결과를 결합합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads  # 각 헤드의 차원\n",
    "        # 쿼리, 키, 값 행렬을 위한 선형 변환 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        # 최종 출력 선형 변환 레이어\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        스케일드 닷-프로덕트 어텐션 계산.\n",
    "        Args:\n",
    "            Q: 쿼리 행렬\n",
    "            K: 키 행렬\n",
    "            V: 값 행렬\n",
    "            mask: 어텐션 마스크 (선택적)\n",
    "        Returns:\n",
    "            out: 어텐션 가중치 적용된 값 행렬\n",
    "            attentions: 어텐션 가중치 행렬\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        QK = torch.matmul(Q, K.transpose(-1, -2))  # QK^T 계산\n",
    "        scaled_qk = QK / math.sqrt(d_k)  # 스케일링\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)  # 마스크 적용 (매우 작은 값 더하기)\n",
    "        attentions = nn.Softmax(dim=-1)(scaled_qk)  # 소프트맥스 적용\n",
    "        out = torch.matmul(attentions, V)  # 가중치 적용\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        입력 텐서를 여러 헤드로 분할.\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)  # 차원 재배치\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 헤드를 다시 결합.\n",
    "        Args:\n",
    "            x: (batch_size, num_heads, seq_len, depth)\n",
    "        Returns:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        bsz, _, seq_len, _ = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.view(bsz, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: 쿼리 입력 (batch_size, seq_len, d_model)\n",
    "            K: 키 입력\n",
    "            V: 값 입력\n",
    "            mask: 어텐션 마스크\n",
    "        Returns:\n",
    "            out: 어텐션 적용된 출력\n",
    "            attention_weights: 어텐션 가중치\n",
    "        \"\"\"\n",
    "        # 헤드 분할 후 어텐션 계산\n",
    "        WQ = self.split_heads(self.W_q(Q))\n",
    "        WK = self.split_heads(self.W_k(K))\n",
    "        WV = self.split_heads(self.W_v(V))\n",
    "        out, attention_weights = self.scaled_dot_product_attention(WQ, WK, WV, mask)\n",
    "        # 헤드 결합 후 선형 변환\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        return out, attention_weights\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    포지션 와이즈 피드포워드 네트워크.\n",
    "    각 위치별로 독립적으로 적용되는 2층 완전 연결 네트워크 (ReLU 활성화 함수 사용).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # 첫 번째 레이어 (차원 확장)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)  # 두 번째 레이어 (원래 차원으로 복원)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    인코더의 단일 레이어.\n",
    "    셀프 어텐션과 피드포워드 네트워크를 포함하며, 레이어 정규화와 드롭아웃을 적용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)  # 첫 번째 정규화\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)  # 두 번째 정규화\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 텐서\n",
    "            mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 출력 텐서\n",
    "            enc_attn: 셀프 어텐션 가중치\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # 셀프 어텐션 + 드롭아웃 + 잔차 연결\n",
    "        out, enc_attn = self.enc_self_attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 피드포워드 네트워크 + 드롭아웃 + 잔차 연결\n",
    "        out = self.ffn(self.norm_2(out))\n",
    "        out = self.do(out) + residual\n",
    "        return out, enc_attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    디코더의 단일 레이어.\n",
    "    셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 네트워크를 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)  # 셀프 어텐션\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # 인코더-디코더 어텐션\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력\n",
    "            enc_out: 인코더 출력\n",
    "            dec_enc_mask: 디코더-인코더 어텐션 마스크\n",
    "            padding_mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 출력 텐서\n",
    "            dec_attn: 셀프 어텐션 가중치\n",
    "            dec_enc_attn: 인코더-디코더 어텐션 가중치\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # 셀프 어텐션 (look-ahead 마스크 적용)\n",
    "        out, dec_attn = self.dec_self_attn(self.norm_1(x), self.norm_1(x), self.norm_1(x), mask=padding_mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 인코더-디코더 어텐션\n",
    "        out, dec_enc_attn = self.enc_dec_attn(self.norm_2(out), enc_out, enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out) + residual\n",
    "        residual = out\n",
    "        # 피드포워드 네트워크\n",
    "        out = self.ffn(self.norm_3(out))\n",
    "        out = self.do(out) + residual\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    인코더 전체 구조.\n",
    "    임베딩 레이어, 위치 인코딩, 여러 개의 인코더 레이어로 구성됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout, vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # 토큰 임베딩\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)  # 위치 인코딩\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 입력 시퀀스 (batch_size, seq_len)\n",
    "            mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 인코더 출력\n",
    "            enc_attns: 각 레이어의 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        out = self.embedding(x) * math.sqrt(self.d_model)  # 임베딩 스케일링\n",
    "        out = self.pos_encoding(out)  # 위치 인코딩 추가\n",
    "        enc_attns = []\n",
    "        for layer in self.enc_layers:\n",
    "            out, enc_attn = layer(out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    디코더 전체 구조.\n",
    "    임베딩 레이어, 위치 인코딩, 여러 개의 디코더 레이어로 구성됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 디코더 입력 시퀀스\n",
    "            enc_out: 인코더 출력\n",
    "            dec_enc_mask: 디코더-인코더 어텐션 마스크\n",
    "            padding_mask: 패딩 마스크\n",
    "        Returns:\n",
    "            out: 디코더 출력\n",
    "            dec_attns: 셀프 어텐션 가중치 리스트\n",
    "            dec_enc_attns: 인코더-디코더 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        out = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        out = self.pos_encoding(out)\n",
    "        dec_attns, dec_enc_attns = [], []\n",
    "        for layer in self.dec_layers:\n",
    "            out, dec_attn, dec_enc_attn = layer(out, enc_out, dec_enc_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    전체 Transformer 모델.\n",
    "    인코더와 디코더를 연결하고, 최종 출력 레이어를 포함합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout, src_vocab_size)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout, tgt_vocab_size)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)  # 최종 출력 레이어\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: 소스 시퀀스 (batch_size, src_seq_len)\n",
    "            tgt: 타겟 시퀀스 (batch_size, tgt_seq_len)\n",
    "        Returns:\n",
    "            logits: 최종 예측 로짓 (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "            enc_attns: 인코더 어텐션 가중치 리스트\n",
    "            dec_attns: 디코더 셀프 어텐션 가중치 리스트\n",
    "            dec_enc_attns: 디코더-인코더 어텐션 가중치 리스트\n",
    "        \"\"\"\n",
    "        # 마스크 생성\n",
    "        src_mask = (src == ko_tokenizer.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt == en_tokenizer.pad_id()).unsqueeze(1).unsqueeze(2)\n",
    "        lookahead_mask = torch.triu(torch.ones(tgt.shape[1], tgt.shape[1]), diagonal=1).bool().to(device)\n",
    "        tgt_mask = tgt_mask | lookahead_mask\n",
    "        # 인코더/디코더 순전파\n",
    "        enc_out, enc_attns = self.encoder(src, src_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CainSWyRqTA4"
   },
   "source": [
    "## 7. 학습 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nWICQXWqTA4"
   },
   "source": [
    "- ```nn.CrossEntropyLoss``` : 다중 분류(classification)에서 가장 많이 쓰는 손실 함수\n",
    "- <pad> 토큰은 학습에 불필요하므로 손실 계산에서 제외\n",
    "    - 예를 들어 패딩된 부분이 많아도 loss가 그 부분 때문에 왜곡되지 않음\n",
    "<br/>\n",
    "\n",
    "- 옵티마이저 : Adam\n",
    "- lr=0.0001 : 학습률(learning rate)\n",
    "- betas=(0.9, 0.98) : 모멘텀 계수 (논문에서 제안된 설정)\n",
    "- 0.9 : 1차 모멘텀(gradient 평균)\n",
    "- 0.98 : 2차 모멘텀(gradient 제곱 평균)\n",
    "- eps=1e-9 : 아주 작은 값 → 분모가 0 되는 걸 방지\n",
    "- 이 설정은 논문 “Attention is All You Need”의 Transformer 원 논문에 나온 Adam 설정과 거의 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0rli69nJqTA4"
   },
   "outputs": [],
   "source": [
    "model = Transformer(N_LAYERS, D_MODEL, N_HEADS, D_FF, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, DROPOUT).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ko_tokenizer.pad_id())\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QwOobpvqTA5"
   },
   "source": [
    "## 8. 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tImbyQd_qTA5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 40.8787\n",
      "  - Step 200/1381 | Batch Loss: 30.4555\n",
      "  - Step 300/1381 | Batch Loss: 25.3776\n",
      "  - Step 400/1381 | Batch Loss: 20.4698\n",
      "  - Step 500/1381 | Batch Loss: 18.6647\n",
      "  - Step 600/1381 | Batch Loss: 16.5328\n",
      "  - Step 700/1381 | Batch Loss: 15.7577\n",
      "  - Step 800/1381 | Batch Loss: 15.1378\n",
      "  - Step 900/1381 | Batch Loss: 14.1569\n",
      "  - Step 1000/1381 | Batch Loss: 13.0376\n",
      "  - Step 1100/1381 | Batch Loss: 12.4241\n",
      "  - Step 1200/1381 | Batch Loss: 11.1943\n",
      "  - Step 1300/1381 | Batch Loss: 11.4174\n",
      "Evaluating...\n",
      "best model saved.\n",
      "Time: 12m 4s\n",
      "\tTrain Loss: 20.071 | Train PPL: 520918047.034\n",
      "\t Val. Loss: 8.983 |  Val. PPL: 7963.884\n",
      "------------------------------\n",
      "Epoch 02 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 9.4466\n",
      "  - Step 200/1381 | Batch Loss: 9.0671\n",
      "  - Step 300/1381 | Batch Loss: 8.5702\n",
      "  - Step 400/1381 | Batch Loss: 8.2968\n",
      "  - Step 500/1381 | Batch Loss: 7.9864\n",
      "  - Step 800/1381 | Batch Loss: 7.2481\n",
      "  - Step 900/1381 | Batch Loss: 7.4459\n",
      "  - Step 1000/1381 | Batch Loss: 7.2127\n",
      "  - Step 1100/1381 | Batch Loss: 6.8451\n",
      "  - Step 1200/1381 | Batch Loss: 6.8694\n",
      "  - Step 1300/1381 | Batch Loss: 6.5571\n",
      "Evaluating...\n",
      "best model saved.\n",
      "Time: 12m 3s\n",
      "\tTrain Loss: 7.973 | Train PPL: 2901.101\n",
      "\t Val. Loss: 7.273 |  Val. PPL: 1440.709\n",
      "------------------------------\n",
      "Epoch 03 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 6.2454\n",
      "  - Step 200/1381 | Batch Loss: 6.5843\n",
      "  - Step 300/1381 | Batch Loss: 6.0753\n",
      "  - Step 400/1381 | Batch Loss: 6.1017\n",
      "  - Step 500/1381 | Batch Loss: 5.9982\n",
      "  - Step 600/1381 | Batch Loss: 5.7485\n",
      "  - Step 700/1381 | Batch Loss: 5.8437\n",
      "  - Step 800/1381 | Batch Loss: 5.8292\n",
      "  - Step 900/1381 | Batch Loss: 5.5145\n",
      "  - Step 1000/1381 | Batch Loss: 5.9789\n",
      "  - Step 1100/1381 | Batch Loss: 5.7498\n",
      "  - Step 1200/1381 | Batch Loss: 5.3101\n",
      "  - Step 1300/1381 | Batch Loss: 5.3223\n",
      "Evaluating...\n",
      "best model saved.\n",
      "Time: 12m 3s\n",
      "\tTrain Loss: 5.828 | Train PPL: 339.740\n",
      "\t Val. Loss: 5.654 |  Val. PPL: 285.530\n",
      "------------------------------\n",
      "Epoch 04 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 5.0793\n",
      "  - Step 200/1381 | Batch Loss: 5.0604\n",
      "  - Step 300/1381 | Batch Loss: 5.3136\n",
      "  - Step 400/1381 | Batch Loss: 5.3693\n",
      "  - Step 500/1381 | Batch Loss: 5.1711\n",
      "  - Step 600/1381 | Batch Loss: 5.0308\n",
      "  - Step 700/1381 | Batch Loss: 5.2351\n",
      "  - Step 800/1381 | Batch Loss: 5.0609\n",
      "  - Step 900/1381 | Batch Loss: 5.1835\n",
      "  - Step 1000/1381 | Batch Loss: 4.9235\n",
      "  - Step 1100/1381 | Batch Loss: 4.9732\n",
      "  - Step 1200/1381 | Batch Loss: 5.2388\n",
      "  - Step 1300/1381 | Batch Loss: 5.0813\n",
      "Evaluating...\n",
      "best model saved.\n",
      "Time: 12m 2s\n",
      "\tTrain Loss: 5.066 | Train PPL: 158.568\n",
      "\t Val. Loss: 5.171 |  Val. PPL: 176.159\n",
      "------------------------------\n",
      "Epoch 05 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 4.6927\n",
      "  - Step 200/1381 | Batch Loss: 4.9669\n",
      "  - Step 300/1381 | Batch Loss: 4.6609\n",
      "  - Step 400/1381 | Batch Loss: 4.7191\n",
      "  - Step 500/1381 | Batch Loss: 4.7752\n",
      "  - Step 600/1381 | Batch Loss: 4.7471\n",
      "  - Step 700/1381 | Batch Loss: 4.8590\n",
      "  - Step 800/1381 | Batch Loss: 4.7666\n",
      "  - Step 900/1381 | Batch Loss: 4.6985\n",
      "  - Step 1000/1381 | Batch Loss: 4.4795\n",
      "  - Step 1100/1381 | Batch Loss: 4.6051\n",
      "  - Step 1200/1381 | Batch Loss: 4.8180\n",
      "  - Step 1300/1381 | Batch Loss: 4.6395\n",
      "Evaluating...\n",
      "best model saved.\n",
      "Time: 12m 4s\n",
      "\tTrain Loss: 4.719 | Train PPL: 112.074\n",
      "\t Val. Loss: 4.982 |  Val. PPL: 145.708\n",
      "------------------------------\n",
      "Epoch 06 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 4.5423\n",
      "  - Step 200/1381 | Batch Loss: 4.6163\n",
      "  - Step 300/1381 | Batch Loss: 4.5024\n",
      "  - Step 400/1381 | Batch Loss: 4.3648\n",
      "  - Step 500/1381 | Batch Loss: 4.6955\n",
      "  - Step 600/1381 | Batch Loss: 4.6366\n",
      "  - Step 700/1381 | Batch Loss: 4.4438\n",
      "  - Step 800/1381 | Batch Loss: 4.5139\n",
      "  - Step 900/1381 | Batch Loss: 4.6933\n",
      "  - Step 1000/1381 | Batch Loss: 4.5790\n",
      "  - Step 1100/1381 | Batch Loss: 4.4323\n",
      "  - Step 1200/1381 | Batch Loss: 4.2864\n",
      "  - Step 1300/1381 | Batch Loss: 4.5974\n",
      "Evaluating...\n",
      "best model saved.\n",
      "Time: 12m 1s\n",
      "\tTrain Loss: 4.505 | Train PPL:  90.497\n",
      "\t Val. Loss: 4.830 |  Val. PPL: 125.193\n",
      "------------------------------\n",
      "Epoch 07 / 10\n",
      "Training...\n",
      "  - Step 100/1381 | Batch Loss: 4.2550\n",
      "  - Step 200/1381 | Batch Loss: 4.3958\n",
      "  - Step 300/1381 | Batch Loss: 4.3508\n",
      "  - Step 400/1381 | Batch Loss: 4.3483\n",
      "  - Step 500/1381 | Batch Loss: 4.5292\n",
      "  - Step 600/1381 | Batch Loss: 4.2898\n",
      "  - Step 700/1381 | Batch Loss: 4.3961\n",
      "  - Step 800/1381 | Batch Loss: 4.4459\n",
      "  - Step 900/1381 | Batch Loss: 4.1324\n",
      "  - Step 1000/1381 | Batch Loss: 4.3153\n",
      "  - Step 1100/1381 | Batch Loss: 4.5642\n",
      "  - Step 1200/1381 | Batch Loss: 4.3883\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip, log_interval=100):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].to(device)\n",
    "        tgt = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _, _, _ = model(src, tgt[:,:-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # log_interval 마다 로그 출력\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(f\"  - Step {i+1}/{len(iterator)} | Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0].to(device)\n",
    "            tgt = batch[1].to(device)\n",
    "\n",
    "            output, _, _, _ = model(src, tgt[:,:-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- 학습 루프 ---\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02} / {EPOCHS:02}\")\n",
    "    print(\"Training...\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, 1)\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer-baseline.pt')\n",
    "        print(\"best model saved.\")\n",
    "\n",
    "    print(f'Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_V7rHI2qTA5"
   },
   "source": [
    "## 9. 번역 및 성능 평가 (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCbxh9b5qTA5"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    src_tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "    tgt_tokens = [tgt_tokenizer.bos_id()]\n",
    "    for i in range(max_len):\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, _, _, dec_enc_attns = model(src_tensor, tgt_tensor)\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        tgt_tokens.append(pred_token)\n",
    "        if pred_token == tgt_tokenizer.eos_id():\n",
    "            break\n",
    "    tgt_sentence = tgt_tokenizer.decode_ids(tgt_tokens)\n",
    "    return tgt_sentence, dec_enc_attns\n",
    "\n",
    "example_idx = 0\n",
    "src = test_kor_raw[example_idx]\n",
    "trg = test_eng_raw[example_idx]\n",
    "translation, attention = translate_sentence(src, ko_tokenizer, en_tokenizer, model, device)\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCe0qHH6qTA5"
   },
   "source": [
    "## 10. 어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSdNfGpxqTA5"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlLPnsmwqTA6"
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
    "    \"\"\"어텐션 맵을 시각화합니다.\"\"\"\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    font_path = 'NanumBarunGothic.ttf'\n",
    "    font_prop = fm.FontProperties(fname=font_path, size=8)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 28))  # x축을 조금 넓혀서 압축 줄임 (10->12)\n",
    "\n",
    "    # 번역된 문장과 원본 문장을 토큰 단위로 분리\n",
    "    sentence_tokens = sentence.split()\n",
    "    translation_tokens = translation.split()\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # attention shape: (head_idx, tgt_len, src_len)\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        # extent 명시: (-0.5, src_len-0.5, tgt_len-0.5, -0.5)로 ticks와 맞춤\n",
    "        src_len = len(sentence_tokens)\n",
    "        tgt_len = len(translation_tokens)\n",
    "        cax = ax.matshow(_attention, cmap='viridis', extent=[-0.5, src_len - 0.5, tgt_len - 0.5, -0.5])\n",
    "\n",
    "        # 눈금 위치 설정\n",
    "        ax.set_xticks(range(src_len))\n",
    "        ax.set_yticks(range(tgt_len))\n",
    "\n",
    "        # 라벨 설정: ha/va로 중앙 정렬\n",
    "        ax.set_xticklabels(sentence_tokens, rotation=90, fontproperties=font_prop, ha='center', va='center')\n",
    "        ax.set_yticklabels(translation_tokens, fontproperties=font_prop, ha='right', va='center')\n",
    "\n",
    "        ax.tick_params(labelsize=8, pad=15)  # pad로 텍스트와 tick 간격 미세 조정\n",
    "\n",
    "    plt.tight_layout()  # subplot 간 여백 자동 조정 (밀림 방지)\n",
    "    plt.show()\n",
    "\n",
    "display_attention(src, translation, attention[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WILaaRwSqTA6"
   },
   "outputs": [],
   "source": [
    "def display_attention_sp(src_sentence, tgt_sentence, attention,\n",
    "                         src_tokenizer, tgt_tokenizer,\n",
    "                         n_heads=8, n_rows=4, n_cols=2):\n",
    "    \"\"\"SentencePiece 토큰 기준 어텐션 맵 시각화\"\"\"\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    font_path = 'NanumBarunGothic.ttf'\n",
    "    font_prop = fm.FontProperties(fname=font_path, size=8)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 28))\n",
    "\n",
    "    # 1️SentencePiece 토큰화 (ID → piece)\n",
    "    src_ids = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_ids = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    src_tokens = [src_tokenizer.id_to_piece(idx) for idx in src_ids]\n",
    "    tgt_tokens = [tgt_tokenizer.id_to_piece(idx) for idx in tgt_ids]\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # attention shape: (n_heads, tgt_len, src_len)\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        src_len = len(src_tokens)\n",
    "        tgt_len = len(tgt_tokens)\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='viridis',\n",
    "                         extent=[-0.5, src_len - 0.5, tgt_len - 0.5, -0.5])\n",
    "\n",
    "        # SentencePiece 토큰을 tick label로 붙임\n",
    "        ax.set_xticks(range(src_len))\n",
    "        ax.set_yticks(range(tgt_len))\n",
    "\n",
    "        ax.set_xticklabels(src_tokens, rotation=90, fontproperties=font_prop, ha='center', va='center')\n",
    "        ax.set_yticklabels(tgt_tokens, fontproperties=font_prop, ha='right', va='center')\n",
    "\n",
    "        ax.tick_params(labelsize=8, pad=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_attention_sp(\n",
    "    src_sentence=src,              # 한국어 원문 문자열\n",
    "    tgt_sentence=translation,      # 영어 번역 문자열\n",
    "    attention=attention[-1],       # 마지막 layer attention\n",
    "    src_tokenizer=ko_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGIEwu5ZqTA6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
